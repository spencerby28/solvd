Directory structure:
└── graphql-python-gql/
    ├── README.md
    ├── CODEOWNERS
    ├── CONTRIBUTING.md
    ├── LICENSE
    ├── MANIFEST.in
    ├── Makefile
    ├── pyproject.toml
    ├── setup.cfg
    ├── setup.py
    ├── tox.ini
    ├── .readthedocs.yaml
    ├── docs/
    │   ├── Makefile
    │   ├── conf.py
    │   ├── index.rst
    │   ├── intro.rst
    │   ├── make.bat
    │   ├── requirements.txt
    │   ├── advanced/
    │   │   ├── async_advanced_usage.rst
    │   │   ├── async_permanent_session.rst
    │   │   ├── dsl_module.rst
    │   │   ├── error_handling.rst
    │   │   ├── index.rst
    │   │   ├── local_schema.rst
    │   │   └── logging.rst
    │   ├── async/
    │   │   ├── async_intro.rst
    │   │   ├── async_usage.rst
    │   │   └── index.rst
    │   ├── code_examples/
    │   │   ├── aiohttp_async.py
    │   │   ├── aiohttp_async_dsl.py
    │   │   ├── aiohttp_sync.py
    │   │   ├── aiohttp_websockets_async.py
    │   │   ├── console_async.py
    │   │   ├── fastapi_async.py
    │   │   ├── httpx_async.py
    │   │   ├── httpx_async_trio.py
    │   │   ├── httpx_sync.py
    │   │   ├── phoenix_channel_async.py
    │   │   ├── reconnecting_mutation_http.py
    │   │   ├── reconnecting_mutation_ws.py
    │   │   ├── reconnecting_subscription.py
    │   │   ├── requests_sync.py
    │   │   ├── requests_sync_dsl.py
    │   │   ├── websockets_async.py
    │   │   └── appsync/
    │   │       ├── mutation_api_key.py
    │   │       ├── mutation_iam.py
    │   │       ├── subscription_api_key.py
    │   │       └── subscription_iam.py
    │   ├── gql-cli/
    │   │   └── intro.rst
    │   ├── modules/
    │   │   ├── client.rst
    │   │   ├── dsl.rst
    │   │   ├── gql.rst
    │   │   ├── transport.rst
    │   │   ├── transport_aiohttp.rst
    │   │   ├── transport_aiohttp_websockets.rst
    │   │   ├── transport_appsync_auth.rst
    │   │   ├── transport_appsync_websockets.rst
    │   │   ├── transport_exceptions.rst
    │   │   ├── transport_httpx.rst
    │   │   ├── transport_phoenix_channel_websockets.rst
    │   │   ├── transport_requests.rst
    │   │   ├── transport_websockets.rst
    │   │   ├── transport_websockets_base.rst
    │   │   └── utilities.rst
    │   ├── transports/
    │   │   ├── aiohttp.rst
    │   │   ├── aiohttp_websockets.rst
    │   │   ├── appsync.rst
    │   │   ├── async_transports.rst
    │   │   ├── httpx.rst
    │   │   ├── httpx_async.rst
    │   │   ├── index.rst
    │   │   ├── phoenix.rst
    │   │   ├── requests.rst
    │   │   ├── sync_transports.rst
    │   │   └── websockets.rst
    │   └── usage/
    │       ├── basic_usage.rst
    │       ├── custom_scalars_and_enums.rst
    │       ├── extensions.rst
    │       ├── file_upload.rst
    │       ├── headers.rst
    │       ├── index.rst
    │       ├── subscriptions.rst
    │       ├── validation.rst
    │       └── variables.rst
    ├── gql/
    │   ├── __init__.py
    │   ├── __version__.py
    │   ├── cli.py
    │   ├── client.py
    │   ├── dsl.py
    │   ├── gql.py
    │   ├── graphql_request.py
    │   ├── py.typed
    │   ├── utils.py
    │   ├── transport/
    │   │   ├── __init__.py
    │   │   ├── aiohttp.py
    │   │   ├── aiohttp_websockets.py
    │   │   ├── appsync_auth.py
    │   │   ├── appsync_websockets.py
    │   │   ├── async_transport.py
    │   │   ├── exceptions.py
    │   │   ├── httpx.py
    │   │   ├── local_schema.py
    │   │   ├── phoenix_channel_websockets.py
    │   │   ├── requests.py
    │   │   ├── transport.py
    │   │   ├── websockets.py
    │   │   └── websockets_base.py
    │   └── utilities/
    │       ├── __init__.py
    │       ├── build_client_schema.py
    │       ├── get_introspection_query_ast.py
    │       ├── node_tree.py
    │       ├── parse_result.py
    │       ├── serialize_variable_values.py
    │       ├── update_schema_enum.py
    │       └── update_schema_scalars.py
    ├── tests/
    │   ├── __init__.py
    │   ├── conftest.py
    │   ├── test_aiohttp.py
    │   ├── test_aiohttp_online.py
    │   ├── test_aiohttp_websocket_exceptions.py
    │   ├── test_aiohttp_websocket_graphqlws_exceptions.py
    │   ├── test_aiohttp_websocket_graphqlws_subscription.py
    │   ├── test_aiohttp_websocket_query.py
    │   ├── test_aiohttp_websocket_subscription.py
    │   ├── test_appsync_auth.py
    │   ├── test_appsync_http.py
    │   ├── test_appsync_websockets.py
    │   ├── test_async_client_validation.py
    │   ├── test_cli.py
    │   ├── test_client.py
    │   ├── test_graphql_request.py
    │   ├── test_graphqlws_exceptions.py
    │   ├── test_graphqlws_subscription.py
    │   ├── test_http_async_sync.py
    │   ├── test_httpx.py
    │   ├── test_httpx_async.py
    │   ├── test_httpx_online.py
    │   ├── test_localhost.cnf
    │   ├── test_localhost.pem
    │   ├── test_phoenix_channel_exceptions.py
    │   ├── test_phoenix_channel_query.py
    │   ├── test_phoenix_channel_subscription.py
    │   ├── test_requests.py
    │   ├── test_requests_batch.py
    │   ├── test_transport.py
    │   ├── test_transport_batch.py
    │   ├── test_websocket_exceptions.py
    │   ├── test_websocket_online.py
    │   ├── test_websocket_query.py
    │   ├── test_websocket_subscription.py
    │   ├── custom_scalars/
    │   │   ├── __init__.py
    │   │   ├── test_datetime.py
    │   │   ├── test_enum_colors.py
    │   │   ├── test_json.py
    │   │   ├── test_money.py
    │   │   └── test_parse_results.py
    │   ├── fixtures/
    │   │   ├── __init__.py
    │   │   ├── aws/
    │   │   │   ├── __init__.py
    │   │   │   ├── fake_credentials.py
    │   │   │   ├── fake_request.py
    │   │   │   ├── fake_session.py
    │   │   │   └── fake_signer.py
    │   │   ├── graphql/
    │   │   │   └── sample.graphql
    │   │   └── vcr_cassettes/
    │   │       ├── client.yaml
    │   │       ├── queries.yaml
    │   │       └── queries_batch.yaml
    │   ├── nested_input/
    │   │   ├── __init__.py
    │   │   ├── schema.py
    │   │   └── test_nested_input.py
    │   ├── regressions/
    │   │   └── issue_447_dsl_missing_directives/
    │   │       └── test_dsl_directives.py
    │   └── starwars/
    │       ├── __init__.py
    │       ├── fixtures.py
    │       ├── schema.py
    │       ├── test_dsl.py
    │       ├── test_introspection.py
    │       ├── test_parse_results.py
    │       ├── test_query.py
    │       ├── test_subscription.py
    │       └── test_validation.py
    └── .github/
        ├── ISSUE_TEMPLATE/
        │   ├── bug_report.md
        │   └── feature_request.md
        └── workflows/
            ├── deploy.yml
            ├── lint.yml
            └── tests.yml


Files Content:

(Files content cropped to 300k characters, download full ingest to see more)
================================================
File: README.md
================================================
# GQL

This is a GraphQL client for Python 3.8+.
Plays nicely with `graphene`, `graphql-core`, `graphql-js` and any other GraphQL implementation compatible with the spec.

GQL architecture is inspired by `React-Relay` and `Apollo-Client`.

[![GitHub-Actions][gh-image]][gh-url]
[![pyversion][pyversion-image]][pyversion-url]
[![pypi][pypi-image]][pypi-url]
[![Anaconda-Server Badge][conda-image]][conda-url]
[![codecov][codecov-image]][codecov-url]

[gh-image]: https://github.com/graphql-python/gql/workflows/Tests/badge.svg
[gh-url]: https://github.com/graphql-python/gql/actions?query=workflow%3ATests
[pyversion-image]: https://img.shields.io/pypi/pyversions/gql
[pyversion-url]: https://pypi.org/project/gql/
[pypi-image]: https://img.shields.io/pypi/v/gql.svg?style=flat
[pypi-url]: https://pypi.org/project/gql/
[conda-image]: https://img.shields.io/conda/vn/conda-forge/gql.svg
[conda-url]: https://anaconda.org/conda-forge/gql
[codecov-image]: https://codecov.io/gh/graphql-python/gql/branch/master/graph/badge.svg
[codecov-url]: https://codecov.io/gh/graphql-python/gql

## Documentation

The complete documentation for GQL can be found at
[gql.readthedocs.io](https://gql.readthedocs.io).

## Features

* Execute GraphQL queries using [different protocols](https://gql.readthedocs.io/en/latest/transports/index.html):
  * http
  * websockets:
    * apollo or graphql-ws protocol
    * Phoenix channels
    * AWS AppSync realtime protocol (experimental)
* Possibility to [validate the queries locally](https://gql.readthedocs.io/en/latest/usage/validation.html) using a GraphQL schema provided locally or fetched from the backend using an instrospection query
* Supports GraphQL queries, mutations and [subscriptions](https://gql.readthedocs.io/en/latest/usage/subscriptions.html)
* Supports [sync or async usage](https://gql.readthedocs.io/en/latest/async/index.html), [allowing concurrent requests](https://gql.readthedocs.io/en/latest/advanced/async_advanced_usage.html#async-advanced-usage)
* Supports [File uploads](https://gql.readthedocs.io/en/latest/usage/file_upload.html)
* Supports [Custom scalars / Enums](https://gql.readthedocs.io/en/latest/usage/custom_scalars_and_enums.html)
* [gql-cli script](https://gql.readthedocs.io/en/latest/gql-cli/intro.html) to execute GraphQL queries or download schemas from the command line
* [DSL module](https://gql.readthedocs.io/en/latest/advanced/dsl_module.html) to compose GraphQL queries dynamically

## Installation

You can install GQL with all the optional dependencies using pip:

```bash
# Quotes may be required on certain shells such as zsh.
pip install "gql[all]"
```

> **NOTE**: See also [the documentation](https://gql.readthedocs.io/en/latest/intro.html#less-dependencies) to install GQL with less extra dependencies depending on the transports you would like to use or for alternative installation methods.

## Usage

### Basic usage

```python
from gql import gql, Client
from gql.transport.aiohttp import AIOHTTPTransport

# Select your transport with a defined url endpoint
transport = AIOHTTPTransport(url="https://countries.trevorblades.com/")

# Create a GraphQL client using the defined transport
client = Client(transport=transport, fetch_schema_from_transport=True)

# Provide a GraphQL query
query = gql(
    """
    query getContinents {
      continents {
        code
        name
      }
    }
"""
)

# Execute the query on the transport
result = client.execute(query)
print(result)
```

Executing the above code should output the following result:

```
$ python basic_example.py
{'continents': [{'code': 'AF', 'name': 'Africa'}, {'code': 'AN', 'name': 'Antarctica'}, {'code': 'AS', 'name': 'Asia'}, {'code': 'EU', 'name': 'Europe'}, {'code': 'NA', 'name': 'North America'}, {'code': 'OC', 'name': 'Oceania'}, {'code': 'SA', 'name': 'South America'}]}
```

> **WARNING**: Please note that this basic example won't work if you have an asyncio event loop running. In some
> python environments (as with Jupyter which uses IPython) an asyncio event loop is created for you. In that case you
> should use instead the [async usage example](https://gql.readthedocs.io/en/latest/async/async_usage.html#async-usage).

## Contributing
See [CONTRIBUTING.md](CONTRIBUTING.md)

## License

[MIT License](https://github.com/graphql-python/gql/blob/master/LICENSE)


================================================
File: CODEOWNERS
================================================
/ @syrusakbary @ekampf @cito @leszekhanusz @KingDarBoja


================================================
File: CONTRIBUTING.md
================================================
# Contributing

Thanks for helping to make gql awesome!

We welcome all kinds of contributions:

- Bug fixes
- Documentation improvements
- New features
- Refactoring & tidying


## Getting started

If you have a specific contribution in mind, be sure to check the
[issues](https://github.com/graphql-python/gql/issues)
and [pull requests](https://github.com/graphql-python/gql/pulls)
in progress - someone could already be working on something similar
and you can help out.

## Project setup

### Development with virtualenv (recommended)

After cloning this repo, create a virtualenv:

```console
virtualenv gql-dev
```

Activate the virtualenv and install dependencies by running:

```console
python -m pip install -e.[dev]
```

If you are using Linux or MacOS, you can make use of Makefile command
`make dev-setup`, which is a shortcut for the above python command.

### Development on Conda

You must create a new env (e.g. `gql-dev`) with the following command:

```sh
conda create -n gql-dev python=3.8
```

Then activate the environment with `conda activate gql-dev`.

Proceed to install all dependencies by running:

```console
pip install -e.[dev]
```

And you ready to start development!

<!-- TODO: Provide environment.yml file for conda env -->

## Coding guidelines

Several tools are used to ensure a coherent coding style.
You need to make sure that your code satisfy those requirements
or the automated tests will fail.

- [black code formatter](https://github.com/psf/black)
- [flake8 style enforcement](https://flake8.pycqa.org/en/latest/index.html)
- [mypy static type checker](http://mypy-lang.org/)
- [isort to sort imports alphabetically](https://isort.readthedocs.io/en/stable/)

On Linux or MacOS, you can fix and check your code style by running
the Makefile command `make check` (this is also checked by running
the automated tests with tox but it is much faster with make)

In addition to the above checks, it is asked that:

- [type hints are used](https://docs.python.org/3/library/typing.html)
- tests are added to ensure complete code coverage

## Running tests

After developing, the full test suite can be evaluated by running:

```sh
pytest tests --cov=gql --cov-report=term-missing -vv
```

Please note that some tests which require external online resources are not
done in the automated tests. You can run those tests by running:

```sh
pytest tests --cov=gql --cov-report=term-missing --run-online -vv
```

If you are using Linux or MacOS, you can make use of Makefile commands
`make tests` and `make all_tests`, which are shortcuts for the above
python commands.

You can also test on several python environments by using tox.

### Running tox on virtualenv

Install tox:
```console
pip install tox
```

Run `tox` on your virtualenv (do not forget to activate it!)
and that's it!

### Running tox on Conda

In order to run `tox` command on conda, install
[tox-conda](https://github.com/tox-dev/tox-conda):

```sh
conda install -c conda-forge tox-conda
```

This install tox underneath so no need to install it before.

Then add the line `requires = tox-conda` in the `tox.ini` file under `[tox]`.

Run `tox` and you will see all the environments being created
and all passing tests. :rocket:

## How to create a good Pull Request

1. Make a fork of the master branch on github
2. Clone your forked repo on your computer
3. Create a feature branch `git checkout -b feature_my_awesome_feature`
4. Modify the code
5. Verify that the [Coding guidelines](#coding-guidelines) are respected
6. Verify that the [automated tests](#running-tests) are passing
7. Make a commit and push it to your fork
8. From github, create the pull request. Automated tests from GitHub actions
and codecov will then automatically run the tests and check the code coverage
9. If other modifications are needed, you are free to create more commits and
push them on your branch. They'll get added to the PR automatically.

Once the Pull Request is accepted and merged, you can safely
delete the branch (and the forked repo if no more development is needed).


================================================
File: LICENSE
================================================
The MIT License (MIT)

Copyright (c) 2016 GraphQL Python

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================
File: MANIFEST.in
================================================
include MANIFEST.in

include CODEOWNERS
include LICENSE
include README.md
include CONTRIBUTING.md
include .readthedocs.yaml

include Makefile

include tox.ini

include gql/py.typed

recursive-include tests *.py *.graphql *.cnf *.yaml *.pem
recursive-include docs *.txt *.rst conf.py Makefile make.bat
recursive-include docs/code_examples *.py

prune docs/_build

global-exclude *.py[co] __pycache__


================================================
File: Makefile
================================================
.PHONY: clean tests docs

SRC_PYTHON := gql tests docs/code_examples

dev-setup:
	python -m pip install -e ".[test]"

tests:
	pytest tests --cov=gql --cov-report=term-missing -vv

all_tests:
	pytest tests --cov=gql --cov-report=term-missing --run-online -vv

tests_aiohttp:
	pytest tests --aiohttp-only

tests_requests:
	pytest tests --requests-only

tests_httpx:
	pytest tests --httpx-only

tests_websockets:
	pytest tests --websockets-only

check:
	isort --recursive $(SRC_PYTHON)
	black $(SRC_PYTHON)
	flake8 $(SRC_PYTHON)
	mypy $(SRC_PYTHON)
	check-manifest

docs:
	rm -rf ./docs/_build
	cd docs; make html

clean:
	find . -name "*.pyc" -delete
	find . -name "__pycache__" | xargs -I {} rm -rf {}
	rm -rf ./htmlcov
	rm -rf ./.mypy_cache
	rm -rf ./.pytest_cache
	rm -rf ./.tox
	rm -rf ./gql.egg-info
	rm -rf ./dist
	rm -rf ./build
	rm -rf ./docs/_build
	rm -f ./.coverage


================================================
File: pyproject.toml
================================================
[project]
name = "gql"
readme = "README.md"
requires-python = ">=3.8.1"
dynamic = ["authors", "classifiers", "dependencies", "description", "entry-points", "keywords", "license", "optional-dependencies", "scripts", "version"]

[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"


================================================
File: setup.cfg
================================================
[flake8]
max-line-length = 88

[isort]
known_standard_library = ssl
known_first_party = gql
multi_line_output = 3
include_trailing_comma = True
line_length = 88
not_skip = __init__.py

[mypy]
ignore_missing_imports = true

[tool:pytest]
norecursedirs = venv .venv .tox .git .cache .mypy_cache .pytest_cache


================================================
File: setup.py
================================================
import os

from setuptools import setup, find_packages

install_requires = [
    "graphql-core>=3.3.0a3,<3.4",
    "yarl>=1.6,<2.0",
    "backoff>=1.11.1,<3.0",
    "anyio>=3.0,<5",
]

console_scripts = [
    "gql-cli=gql.cli:gql_cli",
]

tests_requires = [
    "parse==1.15.0",
    "pytest==7.4.2",
    "pytest-asyncio==0.21.1",
    "pytest-console-scripts==1.4.1",
    "pytest-cov==5.0.0",
    "vcrpy==7.0.0",
    "aiofiles",
]

dev_requires = [
    "black==22.3.0",
    "check-manifest>=0.42,<1",
    "flake8==7.1.1",
    "isort==4.3.21",
    "mypy==1.10",
    "sphinx>=7.0.0,<8;python_version<='3.9'",
    "sphinx>=8.1.0,<9;python_version>'3.9'",
    "sphinx_rtd_theme>=3.0.2,<4",
    "sphinx-argparse==0.4.0",
    "types-aiofiles",
    "types-requests",
] + tests_requires

install_aiohttp_requires = [
    "aiohttp>=3.8.0,<4;python_version<='3.11'",
    "aiohttp>=3.9.0b0,<4;python_version>'3.11'",
]

install_requests_requires = [
    "requests>=2.26,<3",
    "requests_toolbelt>=1.0.0,<2",
]

install_httpx_requires = [
    "httpx>=0.23.1,<1",
]

install_websockets_requires = [
    "websockets>=10,<12",
]

install_botocore_requires = [
    "botocore>=1.21,<2",
]

install_all_requires = (
    install_aiohttp_requires + install_requests_requires + install_httpx_requires + install_websockets_requires + install_botocore_requires
)

# Get version from __version__.py file
current_folder = os.path.abspath(os.path.dirname(__file__))
about = {}
with open(os.path.join(current_folder, "gql", "__version__.py")) as f:
    exec(f.read(), about)

setup(
    name="gql",
    version=about["__version__"],
    description="GraphQL client for Python",
    long_description=open("README.md").read(),
    long_description_content_type="text/markdown",
    url="https://github.com/graphql-python/gql",
    author="Syrus Akbary",
    author_email="me@syrusakbary.com",
    license="MIT",
    classifiers=[
        "Development Status :: 5 - Production/Stable",
        "Intended Audience :: Developers",
        "Topic :: Software Development :: Libraries",
        "License :: OSI Approved :: MIT License",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3 :: Only",
        "Programming Language :: Python :: 3.9",
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Programming Language :: Python :: 3.12",
        "Programming Language :: Python :: 3.13",
        "Programming Language :: Python :: Implementation :: PyPy",
    ],
    keywords="api graphql protocol rest relay gql client",
    packages=find_packages(include=["gql*"]),
    # PEP-561: https://www.python.org/dev/peps/pep-0561/
    package_data={"gql": ["py.typed"]},
    install_requires=install_requires,
    extras_require={
        "all": install_all_requires,
        "test": install_all_requires + tests_requires,
        "test_no_transport": tests_requires,
        "dev": install_all_requires + dev_requires,
        "aiohttp": install_aiohttp_requires,
        "requests": install_requests_requires,
        "httpx": install_httpx_requires,
        "websockets": install_websockets_requires,
        "botocore": install_botocore_requires,
    },
    include_package_data=True,
    zip_safe=False,
    platforms="any",
    entry_points={"console_scripts": console_scripts},
)


================================================
File: tox.ini
================================================
[tox]
envlist =
    black,flake8,import-order,mypy,manifest,
    py{39,310,311,312,313,py3}

[gh-actions]
python =
    3.9: py39
    3.10: py310
    3.11: py311
    3.12: py312
    3.13: py313
    pypy-3: pypy3

[testenv]
conda_channels = conda-forge
passenv = *
setenv =
    PYTHONPATH = {toxinidir}
    MULTIDICT_NO_EXTENSIONS = 1 ; Related to https://github.com/aio-libs/multidict
    YARL_NO_EXTENSIONS = 1      ; Related to https://github.com/aio-libs/yarl
    GQL_TESTS_TIMEOUT_FACTOR = 10
install_command = python -m pip install --ignore-installed {opts} {packages}
whitelist_externals =
    python
deps = -e.[test]
; Prevent installing issues: https://github.com/ContinuumIO/anaconda-issues/issues/542
commands =
    pip install -U setuptools
    ; run "tox -- tests -s" to show output for debugging
    py{39,310,311,312,313,py3}: pytest {posargs:tests}
    py{312}: pytest {posargs:tests --cov-report=term-missing --cov=gql}

[testenv:black]
basepython = python
deps = -e.[dev]
commands =
    black --check gql tests

[testenv:flake8]
basepython = python
deps = -e.[dev]
commands =
    flake8 gql tests

[testenv:import-order]
basepython = python
deps = -e.[dev]
commands =
    isort --recursive --check-only --diff gql tests

[testenv:mypy]
basepython = python
deps = -e.[dev]
commands =
    mypy gql tests

[testenv:docs]
basepython = python
deps = -e.[dev]
commands =
    sphinx-build -b html -nEW docs docs/_build/html

[testenv:manifest]
basepython = python
deps = -e.[dev]
commands =
    check-manifest -v


================================================
File: .readthedocs.yaml
================================================
# .readthedocs.yaml
# Read the Docs configuration file
# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details

# Required
version: 2

# Set the version of Python and other tools you might need
build:
  os: ubuntu-20.04
  tools:
    python: "3.9"

# Build documentation in the docs/ directory with Sphinx
sphinx:
   configuration: docs/conf.py

# Optionally build your docs in additional formats such as PDF
formats:
  - pdf

python:
  install:
    - requirements: docs/requirements.txt
    - method: pip
      path: .
      extra_requirements:
        - all


================================================
File: docs/Makefile
================================================
# Minimal makefile for Sphinx documentation
#

# You can set these variables from the command line, and also
# from the environment for the first two.
SPHINXOPTS    ?= -n
SPHINXBUILD   ?= sphinx-build
SOURCEDIR     = .
BUILDDIR      = _build

# Put it first so that "make" without argument is like "make help".
help:
	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)

.PHONY: help Makefile

# Catch-all target: route all unknown targets to Sphinx using the new
# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
%: Makefile
	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)


================================================
File: docs/conf.py
================================================
# Configuration file for the Sphinx documentation builder.
#
# This file only contains a selection of the most common options. For a full
# list see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os
import sys
sys.path.insert(0, os.path.abspath('./..'))


# -- Project information -----------------------------------------------------

project = 'gql 3'
copyright = '2020, graphql-python.org'
author = 'graphql-python.org'

# The full version, including alpha/beta/rc tags
from gql import __version__
release = __version__


# -- General configuration ---------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    'sphinxarg.ext',
    'sphinx.ext.autodoc',
    'sphinx.ext.intersphinx',
    'sphinx_rtd_theme'
]

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path.
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# If true, `todo` and `todoList` produce output, else they produce nothing.
todo_include_todos = False


# -- Options for HTML output -------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
html_theme = 'sphinx_rtd_theme'

# Output file base name for HTML help builder.
htmlhelp_basename = 'gql-3-doc'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
# html_static_path = ['_static']

# -- AutoDoc configuration -------------------------------------------------
# autoclass_content = "both"
autodoc_default_options = {
    'members': True,
    'inherited-members': True,
    'special-members': '__init__',
    'undoc-members': True,
    'show-inheritance': True
}
autosummary_generate = True

# -- Intersphinx configuration ---------------------------------------------
intersphinx_mapping = {
    'aiohttp': ('https://docs.aiohttp.org/en/stable/', None),
    'graphql': ('https://graphql-core-3.readthedocs.io/en/latest/', None),
    'multidict': ('https://multidict.readthedocs.io/en/stable/', None),
    'python': ('https://docs.python.org/3/', None),
    'requests': ('https://requests.readthedocs.io/en/latest/', None),
    'websockets': ('https://websockets.readthedocs.io/en/11.0.3/', None),
    'yarl': ('https://yarl.readthedocs.io/en/stable/', None),
}

nitpick_ignore = [
    # graphql-core: should be fixed
    ('py:class', 'graphql.execution.execute.ExecutionResult'),
    ('py:class', 'Source'),
    ('py:class', 'GraphQLSchema'),

    # asyncio: should be fixed
    ('py:class', 'asyncio.locks.Event'),

    # aiohttp: should be fixed
    ('py:class', 'aiohttp.client_reqrep.Fingerprint'),
    ('py:class', 'aiohttp.helpers.BasicAuth'),

    # multidict: should be fixed
    ('py:class', 'multidict._multidict.CIMultiDictProxy'),
    ('py:class', 'multidict._multidict.CIMultiDict'),
    ('py:class', 'multidict._multidict.istr'),

    # websockets: first bump websockets version
    ('py:class', 'websockets.datastructures.SupportsKeysAndGetItem'),
    ('py:class', 'websockets.typing.Subprotocol'),

    # httpx: no sphinx docs yet https://github.com/encode/httpx/discussions/3091
    ('py:class', 'httpx.AsyncClient'),
    ('py:class', 'httpx.Client'),
    ('py:class', 'httpx.Headers'),

    # botocore: no sphinx docs
    ('py:class', 'botocore.auth.BaseSigner'),
    ('py:class', 'botocore.awsrequest.AWSRequest'),
    ('py:class', 'botocore.credentials.Credentials'),
    ('py:class', 'botocore.session.Session'),

    # gql: ignore private classes
    ('py:class', 'gql.transport.httpx._HTTPXTransport'),
    ('py:class', 'gql.client._CallableT'),
]


================================================
File: docs/index.rst
================================================
Welcome to GQL 3 documentation!
===============================

Contents
--------

.. toctree::
   :maxdepth: 2

   intro
   usage/index
   async/index
   transports/index
   advanced/index
   gql-cli/intro
   modules/gql


Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`


================================================
File: docs/intro.rst
================================================
Introduction
============

`GQL 3`_ is a `GraphQL`_ Client for Python 3.8+ which plays nicely with other
graphql implementations compatible with the spec.

Under the hood, it uses `GraphQL-core`_ which is a Python port of `GraphQL.js`_,
the JavaScript reference implementation for GraphQL.

Installation
------------

You can install GQL 3 and all the extra dependencies using pip_::

    pip install "gql[all]"

To have the latest pre-releases versions of gql, you can use::

    pip install --pre "gql[all]"

After installation, you can start using GQL by importing from the top-level
:mod:`gql` package.

Less dependencies
^^^^^^^^^^^^^^^^^

GQL supports multiple :ref:`transports <transports>` to communicate with the backend.
Each transport can necessitate specific dependencies.
If you only need one transport you might want to install only the dependency needed for your transport,
instead of using the "`all`" extra dependency as described above, which installs everything.

If for example you only need the :ref:`AIOHTTPTransport <aiohttp_transport>`,
which needs the :code:`aiohttp` dependency, then you can install GQL with::

    pip install gql[aiohttp]

The corresponding between extra dependencies required and the GQL classes is:

+---------------------+------------------------------------------------------------------+
| Extra dependencies  | Classes                                                          |
+=====================+==================================================================+
| aiohttp             | :ref:`AIOHTTPTransport <aiohttp_transport>`                      |
|                     |                                                                  |
|                     | :ref:`AIOHTTPWebsocketsTransport <aiohttp_websockets_transport>` |
+---------------------+------------------------------------------------------------------+
| websockets          | :ref:`WebsocketsTransport <websockets_transport>`                |
|                     |                                                                  |
|                     | :ref:`PhoenixChannelWebsocketsTransport <phoenix_transport>`     |
|                     |                                                                  |
|                     | :ref:`AppSyncWebsocketsTransport <appsync_transport>`            |
+---------------------+------------------------------------------------------------------+
| requests            | :ref:`RequestsHTTPTransport <requests_transport>`                |
+---------------------+------------------------------------------------------------------+
| httpx               | :ref:`HTTPTXTransport <httpx_transport>`                         |
|                     |                                                                  |
|                     | :ref:`HTTPXAsyncTransport <httpx_async_transport>`               |
+---------------------+------------------------------------------------------------------+
| botocore            | :ref:`AppSyncIAMAuthentication <appsync_iam_auth>`               |
+---------------------+------------------------------------------------------------------+

.. note::

    It is also possible to install multiple extra dependencies if needed
    using commas: :code:`gql[aiohttp,websockets]`

Installation with conda
^^^^^^^^^^^^^^^^^^^^^^^

It is also possible to install gql using `conda`_.

To install gql with all extra dependencies::

    conda install gql-with-all

To install gql with less dependencies, you might want to instead install a combinaison of the
following packages: :code:`gql-with-aiohttp`, :code:`gql-with-websockets`, :code:`gql-with-requests`,
:code:`gql-with-botocore`

If you want to have the latest pre-releases version of gql and graphql-core, you can install
them with conda using::

    conda install -c conda-forge -c conda-forge/label/graphql_core_alpha -c conda-forge/label/gql_beta gql-with-all

Reporting Issues and Contributing
---------------------------------

Please visit the `GitHub repository for gql`_ if you're interested in the current development or
want to report issues or send pull requests.

We welcome all kinds of contributions if the coding guidelines are respected.
Please check the  `Contributing`_ file to learn how to make a good pull request.

.. _GraphQL: https://graphql.org/
.. _GraphQL-core: https://github.com/graphql-python/graphql-core
.. _GraphQL.js: https://github.com/graphql/graphql-js
.. _GQL 3: https://github.com/graphql-python/gql
.. _pip: https://pip.pypa.io/
.. _GitHub repository for gql: https://github.com/graphql-python/gql
.. _Contributing: https://github.com/graphql-python/gql/blob/master/CONTRIBUTING.md
.. _conda: https://docs.conda.io


================================================
File: docs/make.bat
================================================
@ECHO OFF

pushd %~dp0

REM Command file for Sphinx documentation

if "%SPHINXBUILD%" == "" (
	set SPHINXBUILD=sphinx-build
)
set SOURCEDIR=source
set BUILDDIR=build

if "%1" == "" goto help

%SPHINXBUILD% >NUL 2>NUL
if errorlevel 9009 (
	echo.
	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
	echo.installed, then set the SPHINXBUILD environment variable to point
	echo.to the full path of the 'sphinx-build' executable. Alternatively you
	echo.may add the Sphinx directory to PATH.
	echo.
	echo.If you don't have Sphinx installed, grab it from
	echo.http://sphinx-doc.org/
	exit /b 1
)

%SPHINXBUILD% -M %1 %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%
goto end

:help
%SPHINXBUILD% -M help %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%

:end
popd


================================================
File: docs/requirements.txt
================================================
sphinx>=5.3.0,<6
sphinx_rtd_theme>=0.4,<1
sphinx-argparse==0.2.5
multidict<5.0,>=4.5


================================================
File: docs/advanced/async_advanced_usage.rst
================================================
.. _async_advanced_usage:

Async advanced usage
====================

It is possible to send multiple GraphQL queries (query, mutation or subscription) in parallel,
on the same websocket connection, using asyncio tasks.

In order to retry in case of connection failure, we can use the great `backoff`_ module.

.. code-block:: python

    # First define all your queries using a session argument:

    async def execute_query1(session):
        result = await session.execute(query1)
        print(result)

    async def execute_query2(session):
        result = await session.execute(query2)
        print(result)

    async def execute_subscription1(session):
        async for result in session.subscribe(subscription1):
            print(result)

    async def execute_subscription2(session):
        async for result in session.subscribe(subscription2):
            print(result)

    # Then create a couroutine which will connect to your API and run all your queries as tasks.
    # We use a `backoff` decorator to reconnect using exponential backoff in case of connection failure.

    @backoff.on_exception(backoff.expo, Exception, max_time=300)
    async def graphql_connection():

        transport = WebsocketsTransport(url="wss://YOUR_URL")

        client = Client(transport=transport, fetch_schema_from_transport=True)

        async with client as session:
            task1 = asyncio.create_task(execute_query1(session))
            task2 = asyncio.create_task(execute_query2(session))
            task3 = asyncio.create_task(execute_subscription1(session))
            task4 = asyncio.create_task(execute_subscription2(session))

            await asyncio.gather(task1, task2, task3, task4)

    asyncio.run(graphql_connection())

Subscriptions tasks can be stopped at any time by running

.. code-block:: python

    task.cancel()

.. _backoff: https://github.com/litl/backoff


================================================
File: docs/advanced/async_permanent_session.rst
================================================
.. _async_permanent_session:

Async permanent session
=======================

Sometimes you want to have a single permanent reconnecting async session to a GraphQL backend,
and that can be `difficult to manage`_ manually with the :code:`async with client as session` syntax.

It is now possible to have a single reconnecting session using the
:meth:`connect_async <gql.Client.connect_async>` method of Client
with a :code:`reconnecting=True` argument.

.. code-block:: python

    # Create a session from the client which will reconnect automatically.
    # This session can be kept in a class for example to provide a way
    # to execute GraphQL queries from many different places
    session = await client.connect_async(reconnecting=True)

    # You can run execute or subscribe method on this session
    result = await session.execute(query)

    # When you want the connection to close (for cleanup),
    # you call close_async
    await client.close_async()


When you use :code:`reconnecting=True`, gql will watch the exceptions generated
during the execute and subscribe calls and, if it detects a TransportClosed exception
(indicating that the link to the underlying transport is broken),
it will try to reconnect to the backend again.

Retries
-------

Connection retries
^^^^^^^^^^^^^^^^^^

With :code:`reconnecting=True`, gql will use the `backoff`_ module to repeatedly try to connect with
exponential backoff and jitter with a maximum delay of 60 seconds by default.

You can change the default reconnecting profile by providing your own
backoff decorator to the :code:`retry_connect` argument.

.. code-block:: python

    # Here wait maximum 5 minutes between connection retries
    retry_connect = backoff.on_exception(
        backoff.expo,  # wait generator (here: exponential backoff)
        Exception,     # which exceptions should cause a retry (here: everything)
        max_value=300, # max wait time in seconds
    )
    session = await client.connect_async(
        reconnecting=True,
        retry_connect=retry_connect,
    )

Execution retries
^^^^^^^^^^^^^^^^^

With :code:`reconnecting=True`, by default we will also retry up to 5 times
when an exception happens during an execute call (to manage a possible loss in the connection
to the transport).

There is no retry in case of a :code:`TransportQueryError` exception as it indicates that
the connection to the backend is working correctly.

You can change the default execute retry profile by providing your own
backoff decorator to the :code:`retry_execute` argument.

.. code-block:: python

    # Here Only 3 tries for execute calls
    retry_execute = backoff.on_exception(
        backoff.expo,
        Exception,
        max_tries=3,
    )
    session = await client.connect_async(
        reconnecting=True,
        retry_execute=retry_execute,
    )

If you don't want any retry on the execute calls, you can disable the retries with :code:`retry_execute=False`

.. note::
    If you want to retry even with :code:`TransportQueryError` exceptions,
    then you need to make your own backoff decorator on your own method:

    .. code-block:: python

        @backoff.on_exception(backoff.expo,
                              Exception,
                              max_tries=3)
        async def execute_with_retry(session, query):
            return await session.execute(query)

Subscription retries
^^^^^^^^^^^^^^^^^^^^

There is no :code:`retry_subscribe` as it is not feasible with async generators.
If you want retries for your subscriptions, then you can do it yourself
with backoff decorators on your methods.

.. code-block:: python

    @backoff.on_exception(backoff.expo,
                          Exception,
                          max_tries=3,
                          giveup=lambda e: isinstance(e, TransportQueryError))
    async def execute_subscription1(session):
        async for result in session.subscribe(subscription1):
            print(result)

FastAPI example
---------------

.. literalinclude:: ../code_examples/fastapi_async.py

Console example
---------------

.. literalinclude:: ../code_examples/console_async.py

.. _difficult to manage: https://github.com/graphql-python/gql/issues/179
.. _backoff: https://github.com/litl/backoff


================================================
File: docs/advanced/dsl_module.rst
================================================
Compose queries dynamically
===========================

Instead of providing the GraphQL queries as a Python String, it is also possible to create GraphQL queries dynamically.
Using the :mod:`DSL module <gql.dsl>`, we can create a query using a Domain Specific Language which is created from the schema.

The following code:

.. code-block:: python

    ds = DSLSchema(StarWarsSchema)

    query = dsl_gql(
        DSLQuery(
            ds.Query.hero.select(
                ds.Character.id,
                ds.Character.name,
                ds.Character.friends.select(ds.Character.name),
            )
        )
    )

will generate a query equivalent to:

.. code-block:: python

    query = gql("""
        query {
          hero {
            id
            name
            friends {
              name
            }
          }
        }
    """)

How to use
----------

First generate the root using the :class:`DSLSchema <gql.dsl.DSLSchema>`::

    ds = DSLSchema(client.schema)

Then use auto-generated attributes of the :code:`ds` instance
to get a root type (Query, Mutation or Subscription).
This will generate a :class:`DSLType <gql.dsl.DSLType>` instance::

    ds.Query

From this root type, you use auto-generated attributes to get a field.
This will generate a :class:`DSLField <gql.dsl.DSLField>` instance::

    ds.Query.hero

hero is a GraphQL object type and needs children fields. By default,
there is no children fields selected. To select the fields that you want
in your query, you use the :meth:`select <gql.dsl.DSLField.select>` method.

To generate the children fields, we use the same method as above to auto-generate the fields
from the :code:`ds` instance
(ie :code:`ds.Character.name` is the field `name` of the type `Character`)::

    ds.Query.hero.select(ds.Character.name)

The select method return the same instance, so it is possible to chain the calls::

    ds.Query.hero.select(ds.Character.name).select(ds.Character.id)

Or do it sequencially::

    hero_query = ds.Query.hero

    hero_query.select(ds.Character.name)
    hero_query.select(ds.Character.id)

As you can select children fields of any object type, you can construct your complete query tree::

    ds.Query.hero.select(
        ds.Character.id,
        ds.Character.name,
        ds.Character.friends.select(ds.Character.name),
    )

Once your root query fields are defined, you can put them in an operation using
:class:`DSLQuery <gql.dsl.DSLQuery>`,
:class:`DSLMutation <gql.dsl.DSLMutation>` or
:class:`DSLSubscription <gql.dsl.DSLSubscription>`::

    DSLQuery(
        ds.Query.hero.select(
            ds.Character.id,
            ds.Character.name,
            ds.Character.friends.select(ds.Character.name),
        )
    )


Once your operations are defined,
use the :func:`dsl_gql <gql.dsl.dsl_gql>` function to convert your operations into
a document which will be able to get executed in the client or a session::

    query = dsl_gql(
        DSLQuery(
            ds.Query.hero.select(
                ds.Character.id,
                ds.Character.name,
                ds.Character.friends.select(ds.Character.name),
            )
        )
    )

    result = client.execute(query)

Arguments
^^^^^^^^^

It is possible to add arguments to any field simply by calling it
with the required arguments::

    ds.Query.human(id="1000").select(ds.Human.name)

It can also be done using the :meth:`args <gql.dsl.DSLField.args>` method::

    ds.Query.human.args(id="1000").select(ds.Human.name)

.. note::
    If your argument name is a Python keyword (for, in, from, ...), you will receive a
    SyntaxError (See `issue #308`_). To fix this, you can provide the arguments by unpacking a dictionary.

    For example, instead of using :code:`from=5`, you can use :code:`**{"from":5}`

Aliases
^^^^^^^

You can set an alias of a field using the :meth:`alias <gql.dsl.DSLField.alias>` method::

    ds.Query.human.args(id=1000).alias("luke").select(ds.Character.name)

It is also possible to set the alias directly using keyword arguments of an operation::

    DSLQuery(
        luke=ds.Query.human.args(id=1000).select(ds.Character.name)
    )

Or using keyword arguments in the :meth:`select <gql.dsl.DSLField.select>` method::

    ds.Query.hero.select(
        my_name=ds.Character.name
    )

Mutations
^^^^^^^^^

For the mutations, you need to start from root fields starting from :code:`ds.Mutation`
then you need to create the GraphQL operation using the class
:class:`DSLMutation <gql.dsl.DSLMutation>`. Example::

    query = dsl_gql(
        DSLMutation(
            ds.Mutation.createReview.args(
                episode=6, review={"stars": 5, "commentary": "This is a great movie!"}
            ).select(ds.Review.stars, ds.Review.commentary)
        )
    )

Variable arguments
^^^^^^^^^^^^^^^^^^

To provide variables instead of argument values directly for an operation, you have to:

* Instantiate a :class:`DSLVariableDefinitions <gql.dsl.DSLVariableDefinitions>`::

    var = DSLVariableDefinitions()

* From this instance you can generate :class:`DSLVariable <gql.dsl.DSLVariable>` instances
  and provide them as the value of the arguments::

    ds.Mutation.createReview.args(review=var.review, episode=var.episode)

* Once the operation has been defined, you have to save the variable definitions used
  in it::

    operation.variable_definitions = var

The following code:

.. code-block:: python

    var = DSLVariableDefinitions()
    op = DSLMutation(
        ds.Mutation.createReview.args(review=var.review, episode=var.episode).select(
            ds.Review.stars, ds.Review.commentary
        )
    )
    op.variable_definitions = var
    query = dsl_gql(op)

will generate a query equivalent to::

    mutation ($review: ReviewInput, $episode: Episode) {
      createReview(review: $review, episode: $episode) {
        stars
        commentary
      }
    }

Variable arguments with a default value
"""""""""""""""""""""""""""""""""""""""

If you want to provide a **default value** for your variable, you can use
the :code:`default` method on a variable.

The following code:

.. code-block:: python

    var = DSLVariableDefinitions()
    op = DSLMutation(
        ds.Mutation.createReview.args(
            review=var.review.default({"stars": 5, "commentary": "Wow!"}),
            episode=var.episode,
        ).select(ds.Review.stars, ds.Review.commentary)
    )
    op.variable_definitions = var
    query = dsl_gql(op)

will generate a query equivalent to::

    mutation ($review: ReviewInput = {stars: 5, commentary: "Wow!"}, $episode: Episode) {
      createReview(review: $review, episode: $episode) {
        stars
        commentary
      }
    }

Subscriptions
^^^^^^^^^^^^^

For the subscriptions, you need to start from root fields starting from :code:`ds.Subscription`
then you need to create the GraphQL operation using the class
:class:`DSLSubscription <gql.dsl.DSLSubscription>`. Example::

    query = dsl_gql(
        DSLSubscription(
            ds.Subscription.reviewAdded(episode=6).select(ds.Review.stars, ds.Review.commentary)
        )
    )

Multiple fields in an operation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

It is possible to create an operation with multiple fields::

    DSLQuery(
        ds.Query.hero.select(ds.Character.name),
        hero_of_episode_5=ds.Query.hero(episode=5).select(ds.Character.name),
    )

Operation name
^^^^^^^^^^^^^^

You can set the operation name of an operation using a keyword argument
to :func:`dsl_gql <gql.dsl.dsl_gql>`::

    query = dsl_gql(
        GetHeroName=DSLQuery(ds.Query.hero.select(ds.Character.name))
    )

will generate the request::

    query GetHeroName {
        hero {
            name
        }
    }

Multiple operations in a document
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

It is possible to create an Document with multiple operations::

    query = dsl_gql(
        operation_name_1=DSLQuery( ... ),
        operation_name_2=DSLQuery( ... ),
        operation_name_3=DSLMutation( ... ),
    )

Fragments
^^^^^^^^^

To define a `Fragment`_, you have to:

* Instantiate a :class:`DSLFragment <gql.dsl.DSLFragment>` with a name::

    name_and_appearances = DSLFragment("NameAndAppearances")

* Provide the GraphQL type of the fragment with the
  :meth:`on <gql.dsl.DSLFragment.on>` method::

    name_and_appearances.on(ds.Character)

* Add children fields using the :meth:`select <gql.dsl.DSLFragment.select>` method::

    name_and_appearances.select(ds.Character.name, ds.Character.appearsIn)

Once your fragment is defined, to use it you should:

* select it as a field somewhere in your query::

    query_with_fragment = DSLQuery(ds.Query.hero.select(name_and_appearances))

* add it as an argument of :func:`dsl_gql <gql.dsl.dsl_gql>` with your query::

    query = dsl_gql(name_and_appearances, query_with_fragment)

The above example will generate the following request::

    fragment NameAndAppearances on Character {
        name
        appearsIn
    }

    {
        hero {
            ...NameAndAppearances
        }
    }

Inline Fragments
^^^^^^^^^^^^^^^^

To define an `Inline Fragment`_, you have to:

* Instantiate a :class:`DSLInlineFragment <gql.dsl.DSLInlineFragment>`::

    human_fragment = DSLInlineFragment()

* Provide the GraphQL type of the fragment with the
  :meth:`on <gql.dsl.DSLInlineFragment.on>` method::

    human_fragment.on(ds.Human)

* Add children fields using the :meth:`select <gql.dsl.DSLInlineFragment.select>` method::

    human_fragment.select(ds.Human.homePlanet)

Once your inline fragment is defined, to use it you should:

* select it as a field somewhere in your query::

    query_with_inline_fragment = ds.Query.hero.args(episode=6).select(
        ds.Character.name,
        human_fragment
    )

The above example will generate the following request::

    hero(episode: JEDI) {
        name
        ... on Human {
          homePlanet
        }
    }

Note: because the :meth:`on <gql.dsl.DSLInlineFragment.on>` and
:meth:`select <gql.dsl.DSLInlineFragment.select>` methods return :code:`self`,
this can be written in a concise manner::

    query_with_inline_fragment = ds.Query.hero.args(episode=6).select(
        ds.Character.name,
        DSLInlineFragment().on(ds.Human).select(ds.Human.homePlanet)
    )

Meta-fields
^^^^^^^^^^^

To define meta-fields (:code:`__typename`, :code:`__schema` and :code:`__type`),
you can use the :class:`DSLMetaField <gql.dsl.DSLMetaField>` class::

    query = ds.Query.hero.select(
        ds.Character.name,
        DSLMetaField("__typename")
    )

Executable examples
-------------------

Async example
^^^^^^^^^^^^^

.. literalinclude:: ../code_examples/aiohttp_async_dsl.py

Sync example
^^^^^^^^^^^^^

.. literalinclude:: ../code_examples/requests_sync_dsl.py

.. _Fragment: https://graphql.org/learn/queries/#fragments
.. _Inline Fragment: https://graphql.org/learn/queries/#inline-fragments
.. _issue #308: https://github.com/graphql-python/gql/issues/308


================================================
File: docs/advanced/error_handling.rst
================================================
Error Handing
=============

Local errors
------------

If gql detects locally that something does not correspond to the GraphQL specification,
then gql may raise a **GraphQLError** from graphql-core.

This may happen for example:

- if your query is not valid
- if your query does not correspond to your schema
- if the result received from the backend does not correspond to the schema
  if :code:`parse_results` is set to True

Transport errors
----------------

If an error happens with the transport, then gql may raise a
:class:`TransportError <gql.transport.exceptions.TransportError>`

Here are the possible Transport Errors:

- :class:`TransportProtocolError <gql.transport.exceptions.TransportProtocolError>`:
  Should never happen if the backend is a correctly configured GraphQL server.
  It means that the answer received from the server does not correspond
  to the transport protocol.

- :class:`TransportServerError <gql.transport.exceptions.TransportServerError>`:
  There was an error communicating with the server. If this error is received,
  then the connection with the server will be closed. This may happen if the server
  returned a 404 http header for example.
  The http error code is available in the exception :code:`code` attribute.

- :class:`TransportQueryError <gql.transport.exceptions.TransportQueryError>`:
  There was a specific error returned from the server for your query.
  The message you receive in this error has been created by the backend, not gql!
  In that case, the connection to the server is still available and you are
  free to try to send other queries using the same connection.
  The message of the exception contains the first error returned by the backend.
  All the errors messages are available in the exception :code:`errors` attribute.

  If the error message begins with :code:`Error while fetching schema:`, it means
  that gql was not able to get the schema from the backend.
  If you don't need the schema, you can try to create the client with
  :code:`fetch_schema_from_transport=False`

- :class:`TransportClosed <gql.transport.exceptions.TransportClosed>`:
  This exception is generated when the client is trying to use the transport
  while the transport was previously closed.

- :class:`TransportAlreadyConnected <gql.transport.exceptions.TransportAlreadyConnected>`:
  Exception generated when the client is trying to connect to the transport
  while the transport is already connected.

HTTP
^^^^

For HTTP transports, we should get a json response which contain
:code:`data` or :code:`errors` fields.
If that is not the case, then the returned error depends whether the http return code
is below 400 or not.

- json response:
    - with data or errors keys:
        - no errors key -> no exception
        - errors key -> raise **TransportQueryError**
    - no data or errors keys:
        - http code < 400:
          raise **TransportProtocolError**
        - http code >= 400:
          raise **TransportServerError**
- not a json response:
    - http code < 400:
      raise **TransportProtocolError**
    - http code >= 400:
      raise **TransportServerError**


================================================
File: docs/advanced/index.rst
================================================
Advanced
========

.. toctree::
   :maxdepth: 2

   async_advanced_usage
   async_permanent_session
   logging
   error_handling
   local_schema
   dsl_module


================================================
File: docs/advanced/local_schema.rst
================================================
Execution on a local schema
===========================

It is also possible to execute queries against a local schema (so without a transport), even
if it is not really useful except maybe for testing.

.. code-block:: python

    from gql import gql, Client

    from .someSchema import SampleSchema

    client = Client(schema=SampleSchema)

    query = gql('''
        {
          hello
        }
    ''')

    result = client.execute(query)

See `tests/starwars/test_query.py`_ for an example

.. _tests/starwars/test_query.py: https://github.com/graphql-python/gql/blob/master/tests/starwars/test_query.py


================================================
File: docs/advanced/logging.rst
================================================
Logging
=======

GQL uses the python `logging`_ module.

In order to debug a problem, you can enable logging to see the messages exchanged between the client and the server.
To do that, set the loglevel at **INFO** at the beginning of your code:

.. code-block:: python

    import logging
    logging.basicConfig(level=logging.INFO)

For even more logs, you can set the loglevel at **DEBUG**:

.. code-block:: python

    import logging
    logging.basicConfig(level=logging.DEBUG)

Disabling logs
--------------

By default, the logs for the transports are quite verbose.

On the **INFO** level, all the messages between the frontend and the backend are logged which can
be difficult to read especially when it fetches the schema from the transport.

It is possible to disable the logs only for a specific gql transport by setting a higher
log level for this transport (**WARNING** for example) so that the other logs of your program are not affected.

For this, you should import the logger from the transport file and set the level on this logger.

For the RequestsHTTPTransport:

.. code-block:: python

    from gql.transport.requests import log as requests_logger
    requests_logger.setLevel(logging.WARNING)

For the WebsocketsTransport:

.. code-block:: python

    from gql.transport.websockets import log as websockets_logger
    websockets_logger.setLevel(logging.WARNING)

.. _logging: https://docs.python.org/3/howto/logging.html


================================================
File: docs/async/async_intro.rst
================================================
On previous versions of GQL, the code was `sync` only , it means that when you ran
`execute` on the Client, you could do nothing else in the current Thread and had to wait for
an answer or a timeout from the backend to continue. The only http library was `requests`, allowing only sync usage.

From the version 3 of GQL, we support `sync` and `async` :ref:`transports <transports>` using `asyncio`_.

With the :ref:`async transports <async_transports>`, there is now the possibility to execute GraphQL requests
asynchronously, :ref:`allowing to execute multiple requests in parallel if needed <async_advanced_usage>`.

If you don't care or need async functionality, it is still possible, with :ref:`async transports <async_transports>`,
to run the `execute` or `subscribe` methods directly from the Client
(as described in the :ref:`Basic Usage <basic_usage>` example) and GQL will execute the request
in a synchronous manner by running an asyncio event loop itself.

This won't work though if you already have an asyncio event loop running. In that case you should use
:ref:`Async Usage <async_usage>`

.. _asyncio: https://docs.python.org/3/library/asyncio.html


================================================
File: docs/async/async_usage.rst
================================================
.. _async_usage:

Async Usage
===========

If you use an :ref:`async transport <async_transports>`, you can use GQL asynchronously using `asyncio`_.

* put your code in an asyncio coroutine (method starting with :code:`async def`)
* use :code:`async with client as session:` to connect to the backend and provide a session instance
* use the :code:`await` keyword to execute requests: :code:`await session.execute(...)`
* then run your coroutine in an asyncio event loop by running :code:`asyncio.run`

Example:

.. literalinclude:: ../code_examples/aiohttp_async.py

IPython
-------

.. warning::

    On some Python environments, like :emphasis:`Jupyter` or :emphasis:`Spyder`,
    which are using :emphasis:`IPython`,
    an asyncio event loop is already created for you by the environment.

In this case, running the above code might generate the following error::

    RuntimeError: asyncio.run() cannot be called from a running event loop

If that happens, depending on the environment,
you should replace :code:`asyncio.run(main())` by either:

.. code-block:: python

    await main()

OR:

.. code-block:: python

    loop = asyncio.get_running_loop()
    loop.create_task(main())

.. _asyncio: https://docs.python.org/3/library/asyncio.html


================================================
File: docs/async/index.rst
================================================
Async vs Sync
=============

.. include:: async_intro.rst

.. toctree::
   :hidden:
   :maxdepth: 1

   async_usage


================================================
File: docs/code_examples/aiohttp_async.py
================================================
import asyncio

from gql import Client, gql
from gql.transport.aiohttp import AIOHTTPTransport


async def main():

    transport = AIOHTTPTransport(url="https://countries.trevorblades.com/graphql")

    # Using `async with` on the client will start a connection on the transport
    # and provide a `session` variable to execute queries on this connection
    async with Client(
        transport=transport,
        fetch_schema_from_transport=True,
    ) as session:

        # Execute single query
        query = gql(
            """
            query getContinents {
              continents {
                code
                name
              }
            }
        """
        )

        result = await session.execute(query)
        print(result)


asyncio.run(main())


================================================
File: docs/code_examples/aiohttp_async_dsl.py
================================================
import asyncio

from gql import Client
from gql.dsl import DSLQuery, DSLSchema, dsl_gql
from gql.transport.aiohttp import AIOHTTPTransport


async def main():

    transport = AIOHTTPTransport(url="https://countries.trevorblades.com/graphql")

    client = Client(transport=transport, fetch_schema_from_transport=True)

    # Using `async with` on the client will start a connection on the transport
    # and provide a `session` variable to execute queries on this connection.
    # Because we requested to fetch the schema from the transport,
    # GQL will fetch the schema just after the establishment of the first session
    async with client as session:

        # Instantiate the root of the DSL Schema as ds
        ds = DSLSchema(client.schema)

        # Create the query using dynamically generated attributes from ds
        query = dsl_gql(
            DSLQuery(
                ds.Query.continents(filter={"code": {"eq": "EU"}}).select(
                    ds.Continent.code, ds.Continent.name
                )
            )
        )

        result = await session.execute(query)
        print(result)

        # This can also be written as:

        # I want to query the continents
        query_continents = ds.Query.continents

        # I want to get only the continents with code equal to "EU"
        query_continents(filter={"code": {"eq": "EU"}})

        # I want this query to return the code and name fields
        query_continents.select(ds.Continent.code)
        query_continents.select(ds.Continent.name)

        # I generate a document from my query to be able to execute it
        query = dsl_gql(DSLQuery(query_continents))

        # Execute the query
        result = await session.execute(query)
        print(result)


asyncio.run(main())


================================================
File: docs/code_examples/aiohttp_sync.py
================================================
from gql import Client, gql
from gql.transport.aiohttp import AIOHTTPTransport

# Select your transport with a defined url endpoint
transport = AIOHTTPTransport(url="https://countries.trevorblades.com/")

# Create a GraphQL client using the defined transport
client = Client(transport=transport, fetch_schema_from_transport=True)

# Provide a GraphQL query
query = gql(
    """
    query getContinents {
      continents {
        code
        name
      }
    }
"""
)

# Execute the query on the transport
result = client.execute(query)
print(result)


================================================
File: docs/code_examples/aiohttp_websockets_async.py
================================================
import asyncio
import logging

from gql import Client, gql
from gql.transport.aiohttp_websockets import AIOHTTPWebsocketsTransport

logging.basicConfig(level=logging.INFO)


async def main():

    transport = AIOHTTPWebsocketsTransport(
        url="wss://countries.trevorblades.com/graphql"
    )

    # Using `async with` on the client will start a connection on the transport
    # and provide a `session` variable to execute queries on this connection
    async with Client(
        transport=transport,
    ) as session:

        # Execute single query
        query = gql(
            """
            query getContinents {
              continents {
                code
                name
              }
            }
        """
        )
        result = await session.execute(query)
        print(result)

        # Request subscription
        subscription = gql(
            """
            subscription {
                somethingChanged {
                    id
                }
            }
        """
        )
        async for result in session.subscribe(subscription):
            print(result)


asyncio.run(main())


================================================
File: docs/code_examples/console_async.py
================================================
import asyncio
import logging

from aioconsole import ainput
from gql import Client, gql
from gql.transport.aiohttp import AIOHTTPTransport

logging.basicConfig(level=logging.INFO)

GET_CONTINENT_NAME = """
    query getContinentName ($code: ID!) {
      continent (code: $code) {
        name
      }
    }
"""


class GraphQLContinentClient:
    def __init__(self):
        self._client = Client(
            transport=AIOHTTPTransport(url="https://countries.trevorblades.com/")
        )
        self._session = None

        self.get_continent_name_query = gql(GET_CONTINENT_NAME)

    async def connect(self):
        self._session = await self._client.connect_async(reconnecting=True)

    async def close(self):
        await self._client.close_async()

    async def get_continent_name(self, code):
        params = {"code": code}

        answer = await self._session.execute(
            self.get_continent_name_query, variable_values=params
        )

        return answer.get("continent").get("name")


async def main():
    continent_client = GraphQLContinentClient()

    continent_codes = ["AF", "AN", "AS", "EU", "NA", "OC", "SA"]

    await continent_client.connect()

    while True:

        answer = await ainput("\nPlease enter a continent code or 'exit':")
        answer = answer.strip()

        if answer == "exit":
            break
        elif answer in continent_codes:

            try:
                continent_name = await continent_client.get_continent_name(answer)
                print(f"The continent name is {continent_name}\n")
            except Exception as exc:
                print(f"Received exception {exc} while trying to get continent name")

        else:
            print(f"Please enter a valid continent code from {continent_codes}")

    await continent_client.close()


asyncio.run(main())


================================================
File: docs/code_examples/fastapi_async.py
================================================
# First install fastapi and uvicorn:
#
# pip install fastapi uvicorn
#
# then run:
#
# uvicorn fastapi_async:app --reload

import logging

from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from gql import Client, gql
from gql.transport.aiohttp import AIOHTTPTransport

logging.basicConfig(level=logging.DEBUG)
log = logging.getLogger(__name__)

transport = AIOHTTPTransport(url="https://countries.trevorblades.com/graphql")

client = Client(transport=transport)

query = gql(
    """
query getContinentInfo($code: ID!) {
  continent(code:$code) {
    name
    code
    countries  {
      name
      capital
    }
  }
}
"""
)

app = FastAPI()


@app.on_event("startup")
async def startup_event():
    print("Connecting to GraphQL backend")

    await client.connect_async(reconnecting=True)
    print("End of startup")


@app.on_event("shutdown")
async def shutdown_event():
    print("Shutting down GraphQL permanent connection...")
    await client.close_async()
    print("Shutting down GraphQL permanent connection... done")


continent_codes = [
    "AF",
    "AN",
    "AS",
    "EU",
    "NA",
    "OC",
    "SA",
]


@app.get("/", response_class=HTMLResponse)
def get_root():

    continent_links = ", ".join(
        [f'<a href="continent/{code}">{code}</a>' for code in continent_codes]
    )

    return f"""
    <html>
        <head>
            <title>Continents</title>
        </head>
        <body>
            Continents: {continent_links}
        </body>
    </html>
"""


@app.get("/continent/{continent_code}")
async def get_continent(continent_code):

    if continent_code not in continent_codes:
        raise HTTPException(status_code=404, detail="Continent not found")

    try:
        result = await client.session.execute(
            query, variable_values={"code": continent_code}
        )
    except Exception as e:
        log.debug(f"get_continent Error: {e}")
        raise HTTPException(status_code=503, detail="GraphQL backend unavailable")

    return result


================================================
File: docs/code_examples/httpx_async.py
================================================
import asyncio

from gql import Client, gql
from gql.transport.httpx import HTTPXAsyncTransport


async def main():

    transport = HTTPXAsyncTransport(url="https://countries.trevorblades.com/graphql")

    # Using `async with` on the client will start a connection on the transport
    # and provide a `session` variable to execute queries on this connection
    async with Client(
        transport=transport,
        fetch_schema_from_transport=True,
    ) as session:

        # Execute single query
        query = gql(
            """
            query getContinents {
              continents {
                code
                name
              }
            }
        """
        )

        result = await session.execute(query)
        print(result)


asyncio.run(main())


================================================
File: docs/code_examples/httpx_async_trio.py
================================================
import trio
from gql import Client, gql
from gql.transport.httpx import HTTPXAsyncTransport


async def main():

    transport = HTTPXAsyncTransport(url="https://countries.trevorblades.com/graphql")

    # Using `async with` on the client will start a connection on the transport
    # and provide a `session` variable to execute queries on this connection
    async with Client(
        transport=transport,
        fetch_schema_from_transport=True,
    ) as session:

        # Execute single query
        query = gql(
            """
            query getContinents {
              continents {
                code
                name
              }
            }
        """
        )

        result = await session.execute(query)
        print(result)


trio.run(main)


================================================
File: docs/code_examples/httpx_sync.py
================================================
from gql import Client, gql
from gql.transport.httpx import HTTPXTransport

transport = HTTPXTransport(url="https://countries.trevorblades.com/")

client = Client(transport=transport, fetch_schema_from_transport=True)

query = gql(
    """
    query getContinents {
      continents {
        code
        name
      }
    }
"""
)

result = client.execute(query)
print(result)


================================================
File: docs/code_examples/phoenix_channel_async.py
================================================
import asyncio

from gql import Client, gql
from gql.transport.phoenix_channel_websockets import PhoenixChannelWebsocketsTransport


async def main():

    transport = PhoenixChannelWebsocketsTransport(
        channel_name="YOUR_CHANNEL", url="wss://YOUR_URL/graphql"
    )

    # Using `async with` on the client will start a connection on the transport
    # and provide a `session` variable to execute queries on this connection
    async with Client(transport=transport) as session:

        # Execute single query
        query = gql(
            """
            query yourQuery {
                ...
            }
        """
        )

        result = await session.execute(query)
        print(result)


asyncio.run(main())


================================================
File: docs/code_examples/reconnecting_mutation_http.py
================================================
import asyncio
import logging

import backoff

from gql import Client, gql
from gql.transport.aiohttp import AIOHTTPTransport

logging.basicConfig(level=logging.INFO)


async def main():

    # Note: this example used the test backend from
    # https://github.com/slothmanxyz/typegraphql-ws-apollo
    transport = AIOHTTPTransport(url="ws://localhost:5000/graphql")

    client = Client(transport=transport)

    retry_connect = backoff.on_exception(
        backoff.expo,
        Exception,
        max_value=10,
        jitter=None,
    )
    session = await client.connect_async(reconnecting=True, retry_connect=retry_connect)

    num = 0

    while True:
        num += 1

        # Execute single query
        query = gql("mutation ($message: String!) {sendMessage(message: $message)}")

        params = {"message": f"test {num}"}

        try:
            result = await session.execute(query, variable_values=params)
            print(result)
        except Exception as e:
            print(f"Received exception {e}")

        await asyncio.sleep(1)


asyncio.run(main())


================================================
File: docs/code_examples/reconnecting_mutation_ws.py
================================================
import asyncio
import logging

import backoff

from gql import Client, gql
from gql.transport.websockets import WebsocketsTransport

logging.basicConfig(level=logging.INFO)


async def main():

    # Note: this example used the test backend from
    # https://github.com/slothmanxyz/typegraphql-ws-apollo
    transport = WebsocketsTransport(url="ws://localhost:5000/graphql")

    client = Client(transport=transport)

    retry_connect = backoff.on_exception(
        backoff.expo,
        Exception,
        max_value=10,
        jitter=None,
    )
    session = await client.connect_async(reconnecting=True, retry_connect=retry_connect)

    num = 0

    while True:
        num += 1

        # Execute single query
        query = gql("mutation ($message: String!) {sendMessage(message: $message)}")

        params = {"message": f"test {num}"}

        try:
            result = await session.execute(query, variable_values=params)
            print(result)
        except Exception as e:
            print(f"Received exception {e}")

        await asyncio.sleep(1)


asyncio.run(main())


================================================
File: docs/code_examples/reconnecting_subscription.py
================================================
import asyncio
import logging

from gql import Client, gql
from gql.transport.websockets import WebsocketsTransport

logging.basicConfig(level=logging.INFO)


async def main():

    # Note: this example used the test backend from
    # https://github.com/slothmanxyz/typegraphql-ws-apollo
    transport = WebsocketsTransport(url="ws://localhost:5000/graphql")

    client = Client(transport=transport)

    session = await client.connect_async(reconnecting=True)

    query = gql("subscription {receiveMessage {message}}")

    while True:
        try:
            async for result in session.subscribe(query):
                print(result)
        except Exception as e:
            print(f"Received exception {e}")

        await asyncio.sleep(1)


asyncio.run(main())


================================================
File: docs/code_examples/requests_sync.py
================================================
from gql import Client, gql
from gql.transport.requests import RequestsHTTPTransport

transport = RequestsHTTPTransport(
    url="https://countries.trevorblades.com/",
    verify=True,
    retries=3,
)

client = Client(transport=transport, fetch_schema_from_transport=True)

query = gql(
    """
    query getContinents {
      continents {
        code
        name
      }
    }
"""
)

result = client.execute(query)
print(result)


================================================
File: docs/code_examples/requests_sync_dsl.py
================================================
from gql import Client
from gql.dsl import DSLQuery, DSLSchema, dsl_gql
from gql.transport.requests import RequestsHTTPTransport

transport = RequestsHTTPTransport(
    url="https://countries.trevorblades.com/",
    verify=True,
    retries=3,
)

client = Client(transport=transport, fetch_schema_from_transport=True)

# Using `with` on the sync client will start a connection on the transport
# and provide a `session` variable to execute queries on this connection.
# Because we requested to fetch the schema from the transport,
# GQL will fetch the schema just after the establishment of the first session
with client as session:

    # We should have received the schema now that the session is established
    assert client.schema is not None

    # Instantiate the root of the DSL Schema as ds
    ds = DSLSchema(client.schema)

    # Create the query using dynamically generated attributes from ds
    query = dsl_gql(
        DSLQuery(ds.Query.continents.select(ds.Continent.code, ds.Continent.name))
    )

    result = session.execute(query)
    print(result)


================================================
File: docs/code_examples/websockets_async.py
================================================
import asyncio
import logging

from gql import Client, gql
from gql.transport.websockets import WebsocketsTransport

logging.basicConfig(level=logging.INFO)


async def main():

    transport = WebsocketsTransport(url="wss://countries.trevorblades.com/graphql")

    # Using `async with` on the client will start a connection on the transport
    # and provide a `session` variable to execute queries on this connection
    async with Client(
        transport=transport,
        fetch_schema_from_transport=True,
    ) as session:

        # Execute single query
        query = gql(
            """
            query getContinents {
              continents {
                code
                name
              }
            }
        """
        )
        result = await session.execute(query)
        print(result)

        # Request subscription
        subscription = gql(
            """
            subscription {
                somethingChanged {
                    id
                }
            }
        """
        )
        async for result in session.subscribe(subscription):
            print(result)


asyncio.run(main())


================================================
File: docs/code_examples/appsync/mutation_api_key.py
================================================
import asyncio
import os
import sys
from urllib.parse import urlparse

from gql import Client, gql
from gql.transport.aiohttp import AIOHTTPTransport
from gql.transport.appsync_auth import AppSyncApiKeyAuthentication

# Uncomment the following lines to enable debug output
# import logging
# logging.basicConfig(level=logging.DEBUG)


async def main():

    # Should look like:
    # https://XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com/graphql
    url = os.environ.get("AWS_GRAPHQL_API_ENDPOINT")
    api_key = os.environ.get("AWS_GRAPHQL_API_KEY")

    if url is None or api_key is None:
        print("Missing environment variables")
        sys.exit()

    # Extract host from url
    host = str(urlparse(url).netloc)

    auth = AppSyncApiKeyAuthentication(host=host, api_key=api_key)

    transport = AIOHTTPTransport(url=url, auth=auth)

    async with Client(
        transport=transport,
        fetch_schema_from_transport=False,
    ) as session:

        query = gql(
            """
mutation createMessage($message: String!) {
  createMessage(input: {message: $message}) {
    id
    message
    createdAt
  }
}"""
        )

        variable_values = {"message": "Hello world!"}

        result = await session.execute(query, variable_values=variable_values)
        print(result)


asyncio.run(main())


================================================
File: docs/code_examples/appsync/mutation_iam.py
================================================
import asyncio
import os
import sys
from urllib.parse import urlparse

from gql import Client, gql
from gql.transport.aiohttp import AIOHTTPTransport
from gql.transport.appsync_auth import AppSyncIAMAuthentication

# Uncomment the following lines to enable debug output
# import logging
# logging.basicConfig(level=logging.DEBUG)


async def main():

    # Should look like:
    # https://XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com/graphql
    url = os.environ.get("AWS_GRAPHQL_API_ENDPOINT")

    if url is None:
        print("Missing environment variables")
        sys.exit()

    # Extract host from url
    host = str(urlparse(url).netloc)

    auth = AppSyncIAMAuthentication(host=host)

    transport = AIOHTTPTransport(url=url, auth=auth)

    async with Client(
        transport=transport,
        fetch_schema_from_transport=False,
    ) as session:

        query = gql(
            """
mutation createMessage($message: String!) {
  createMessage(input: {message: $message}) {
    id
    message
    createdAt
  }
}"""
        )

        variable_values = {"message": "Hello world!"}

        result = await session.execute(query, variable_values=variable_values)
        print(result)


asyncio.run(main())


================================================
File: docs/code_examples/appsync/subscription_api_key.py
================================================
import asyncio
import os
import sys
from urllib.parse import urlparse

from gql import Client, gql
from gql.transport.appsync_auth import AppSyncApiKeyAuthentication
from gql.transport.appsync_websockets import AppSyncWebsocketsTransport

# Uncomment the following lines to enable debug output
# import logging
# logging.basicConfig(level=logging.DEBUG)


async def main():

    # Should look like:
    # https://XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com/graphql
    url = os.environ.get("AWS_GRAPHQL_API_ENDPOINT")
    api_key = os.environ.get("AWS_GRAPHQL_API_KEY")

    if url is None or api_key is None:
        print("Missing environment variables")
        sys.exit()

    # Extract host from url
    host = str(urlparse(url).netloc)

    print(f"Host: {host}")

    auth = AppSyncApiKeyAuthentication(host=host, api_key=api_key)

    transport = AppSyncWebsocketsTransport(url=url, auth=auth)

    async with Client(transport=transport) as session:

        subscription = gql(
            """
subscription onCreateMessage {
  onCreateMessage {
    message
  }
}
"""
        )

        print("Waiting for messages...")

        async for result in session.subscribe(subscription):
            print(result)


asyncio.run(main())


================================================
File: docs/code_examples/appsync/subscription_iam.py
================================================
import asyncio
import os
import sys

from gql import Client, gql
from gql.transport.appsync_websockets import AppSyncWebsocketsTransport

# Uncomment the following lines to enable debug output
# import logging
# logging.basicConfig(level=logging.DEBUG)


async def main():

    # Should look like:
    # https://XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com/graphql
    url = os.environ.get("AWS_GRAPHQL_API_ENDPOINT")

    if url is None:
        print("Missing environment variables")
        sys.exit()

    # Using implicit auth (IAM)
    transport = AppSyncWebsocketsTransport(url=url)

    async with Client(transport=transport) as session:

        subscription = gql(
            """
subscription onCreateMessage {
  onCreateMessage {
    message
  }
}
"""
        )

        print("Waiting for messages...")

        async for result in session.subscribe(subscription):
            print(result)


asyncio.run(main())


================================================
File: docs/gql-cli/intro.rst
================================================
.. _gql_cli:

gql-cli
=======

GQL provides a python script, called `gql-cli` which allows you to execute
GraphQL queries directly from the terminal.

This script supports http(s) or websockets protocols.

Usage
-----

.. argparse::
   :module: gql.cli
   :func: get_parser
   :prog: gql-cli

Examples
--------

Simple query using https
^^^^^^^^^^^^^^^^^^^^^^^^^

.. code-block:: shell

    $ echo 'query { continent(code:"AF") { name } }' | gql-cli https://countries.trevorblades.com
    {"continent": {"name": "Africa"}}

Simple query using websockets
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code-block:: shell

    $ echo 'query { continent(code:"AF") { name } }' | gql-cli wss://countries.trevorblades.com/graphql
    {"continent": {"name": "Africa"}}

Query with variable
^^^^^^^^^^^^^^^^^^^

.. code-block:: shell

    $ echo 'query getContinent($code:ID!) { continent(code:$code) { name } }' | gql-cli https://countries.trevorblades.com --variables code:AF
    {"continent": {"name": "Africa"}}

Interactive usage
^^^^^^^^^^^^^^^^^

Insert your query in the terminal, then press Ctrl-D to execute it.

.. code-block:: shell

    $ gql-cli wss://countries.trevorblades.com/graphql --variables code:AF

Execute query saved in a file
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Put the query in a file:

.. code-block:: shell

    $ echo 'query {
      continent(code:"AF") {
        name
      }
    }' > query.gql

Then execute query from the file:

.. code-block:: shell

    $ cat query.gql | gql-cli wss://countries.trevorblades.com/graphql
    {"continent": {"name": "Africa"}}

Print the GraphQL schema in a file
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code-block:: shell

    $ gql-cli https://countries.trevorblades.com/graphql --print-schema > schema.graphql

.. note::

    By default, deprecated input fields are not requested from the backend.
    You can add :code:`--schema-download input_value_deprecation:true` to request them.

.. note::

    You can add :code:`--schema-download descriptions:false` to request a compact schema
    without comments.


================================================
File: docs/modules/client.rst
================================================
gql.client
==========

.. currentmodule:: gql.client

.. automodule:: gql.client


================================================
File: docs/modules/dsl.rst
================================================
gql.dsl
=======

.. currentmodule:: gql.dsl

.. automodule:: gql.dsl
    :member-order: bysource


================================================
File: docs/modules/gql.rst
================================================
Reference
=========

.. currentmodule:: gql

.. _top-level-functions:

Top-Level Functions
-------------------

.. automodule:: gql

.. _sub-packages:

Sub-Packages
------------

.. toctree::
   :maxdepth: 1

   client
   transport
   transport_aiohttp
   transport_aiohttp_websockets
   transport_appsync_auth
   transport_appsync_websockets
   transport_exceptions
   transport_phoenix_channel_websockets
   transport_requests
   transport_httpx
   transport_websockets
   transport_websockets_base
   dsl
   utilities


================================================
File: docs/modules/transport.rst
================================================
gql.transport
=============

.. currentmodule:: gql.transport

.. autoclass:: gql.transport.transport.Transport

.. autoclass:: gql.transport.async_transport.AsyncTransport

.. autoclass:: gql.transport.local_schema.LocalSchemaTransport


================================================
File: docs/modules/transport_aiohttp.rst
================================================
gql.transport.aiohttp
=====================

.. currentmodule:: gql.transport.aiohttp

.. automodule:: gql.transport.aiohttp
    :member-order: bysource


================================================
File: docs/modules/transport_aiohttp_websockets.rst
================================================
gql.transport.aiohttp_websockets
================================

.. currentmodule:: gql.transport.aiohttp_websockets

.. automodule:: gql.transport.aiohttp_websockets
    :member-order: bysource


================================================
File: docs/modules/transport_appsync_auth.rst
================================================
gql.transport.appsync_auth
==========================

.. currentmodule:: gql.transport.appsync_auth

.. automodule:: gql.transport.appsync_auth
    :member-order: bysource


================================================
File: docs/modules/transport_appsync_websockets.rst
================================================
gql.transport.appsync_websockets
================================

.. currentmodule:: gql.transport.appsync_websockets

.. automodule:: gql.transport.appsync_websockets
    :member-order: bysource


================================================
File: docs/modules/transport_exceptions.rst
================================================
gql.transport.exceptions
========================

.. currentmodule:: gql.transport.exceptions

.. automodule:: gql.transport.exceptions
    :member-order: bysource


================================================
File: docs/modules/transport_httpx.rst
================================================
gql.transport.httpx
===================

.. currentmodule:: gql.transport.httpx

.. automodule:: gql.transport.httpx
    :member-order: bysource


================================================
File: docs/modules/transport_phoenix_channel_websockets.rst
================================================
gql.transport.phoenix_channel_websockets
========================================

.. currentmodule:: gql.transport.phoenix_channel_websockets

.. automodule:: gql.transport.phoenix_channel_websockets
    :member-order: bysource


================================================
File: docs/modules/transport_requests.rst
================================================
gql.transport.requests
======================

.. currentmodule:: gql.transport.requests

.. automodule:: gql.transport.requests
    :member-order: bysource


================================================
File: docs/modules/transport_websockets.rst
================================================
gql.transport.websockets
========================

.. currentmodule:: gql.transport.websockets

.. automodule:: gql.transport.websockets
    :member-order: bysource


================================================
File: docs/modules/transport_websockets_base.rst
================================================
gql.transport.websockets_base
=============================

.. currentmodule:: gql.transport.websockets_base

.. automodule:: gql.transport.websockets_base
    :member-order: bysource


================================================
File: docs/modules/utilities.rst
================================================
gql.utilities
=============

.. currentmodule:: gql.utilities

.. automodule:: gql.utilities


================================================
File: docs/transports/aiohttp.rst
================================================
.. _aiohttp_transport:

AIOHTTPTransport
================

This transport uses the `aiohttp`_ library and allows you to send GraphQL queries using the HTTP protocol.

Reference: :class:`gql.transport.aiohttp.AIOHTTPTransport`

.. note::

    GraphQL subscriptions are not supported on the HTTP transport.
    For subscriptions you should use a websockets transport:
    :ref:`WebsocketsTransport <websockets_transport>` or
    :ref:`AIOHTTPWebsocketsTransport <aiohttp_websockets_transport>`.

.. literalinclude:: ../code_examples/aiohttp_async.py

Authentication
--------------

There are multiple ways to authenticate depending on the server configuration.

1. Using HTTP Headers

.. code-block:: python

    transport = AIOHTTPTransport(
        url='https://SERVER_URL:SERVER_PORT/graphql',
        headers={'Authorization': 'token'}
    )

2. Using HTTP Cookies

You can manually set the cookies which will be sent with each connection:

.. code-block:: python

    transport = AIOHTTPTransport(url=url, cookies={"cookie1": "val1"})

Or you can use a cookie jar to save cookies set from the backend and reuse them later.

In some cases, the server will set some connection cookies after a successful login mutation
and you can save these cookies in a cookie jar to reuse them in a following connection
(See `issue 197`_):

.. code-block:: python

    jar = aiohttp.CookieJar()
    transport = AIOHTTPTransport(url=url, client_session_args={'cookie_jar': jar})


.. _aiohttp: https://docs.aiohttp.org
.. _issue 197: https://github.com/graphql-python/gql/issues/197


================================================
File: docs/transports/aiohttp_websockets.rst
================================================
.. _aiohttp_websockets_transport:

AIOHTTPWebsocketsTransport
==========================

The AIOHTTPWebsocketsTransport is an alternative to the :ref:`websockets_transport`,
using the `aiohttp` dependency instead of the `websockets` dependency.

It also supports both:

 - the `Apollo websockets transport protocol`_.
 - the `GraphQL-ws websockets transport protocol`_

It will propose both subprotocols to the backend and detect the supported protocol
from the response http headers returned by the backend.

.. note::
    For some backends (graphql-ws before `version 5.6.1`_ without backwards compatibility), it may be necessary to specify
    only one subprotocol to the backend. It can be done by using
    :code:`subprotocols=[AIOHTTPWebsocketsTransport.GRAPHQLWS_SUBPROTOCOL]`
    or :code:`subprotocols=[AIOHTTPWebsocketsTransport.APOLLO_SUBPROTOCOL]` in the transport arguments.

This transport allows to do multiple queries, mutations and subscriptions on the same websocket connection.

Reference: :class:`gql.transport.aiohttp_websockets.AIOHTTPWebsocketsTransport`

.. literalinclude:: ../code_examples/aiohttp_websockets_async.py

.. _version 5.6.1: https://github.com/enisdenjo/graphql-ws/releases/tag/v5.6.1
.. _Apollo websockets transport protocol:  https://github.com/apollographql/subscriptions-transport-ws/blob/master/PROTOCOL.md
.. _GraphQL-ws websockets transport protocol: https://github.com/enisdenjo/graphql-ws/blob/master/PROTOCOL.md


================================================
File: docs/transports/appsync.rst
================================================
.. _appsync_transport:

AppSyncWebsocketsTransport
==========================

AWS AppSync allows you to execute GraphQL subscriptions on its realtime GraphQL endpoint.

See `Building a real-time websocket client`_ for an explanation.

GQL provides the :code:`AppSyncWebsocketsTransport` transport which implements this
for you to allow you to execute subscriptions.

.. note::
    It is only possible to execute subscriptions with this transport.
    For queries or mutations, See :ref:`AppSync GraphQL Queries and mutations <appsync_http>`

How to use it:

 * choose one :ref:`authentication method <appsync_authentication_methods>` (API key, IAM, Cognito user pools or OIDC)
 * instantiate a :code:`AppSyncWebsocketsTransport` with your GraphQL endpoint as url and your auth method

.. note::
    It is also possible to instantiate the transport without an auth argument. In that case,
    gql will use by default the :class:`IAM auth <gql.transport.appsync_auth.AppSyncIAMAuthentication>`
    which will try to authenticate with environment variables or from your aws credentials file.

.. note::
    All the examples in this documentation are based on the sample app created
    by following `this AWS blog post`_

Full example with API key authentication from environment variables:

.. literalinclude:: ../code_examples/appsync/subscription_api_key.py

Reference: :class:`gql.transport.appsync_websockets.AppSyncWebsocketsTransport`

.. _Building a real-time websocket client: https://docs.aws.amazon.com/appsync/latest/devguide/real-time-websocket-client.html
.. _this AWS blog post: https://aws.amazon.com/fr/blogs/mobile/appsync-realtime/


.. _appsync_authentication_methods:

Authentication methods
----------------------

.. _appsync_api_key_auth:

API key
^^^^^^^

Use the :code:`AppSyncApiKeyAuthentication` class to provide your API key:

.. code-block:: python

    auth = AppSyncApiKeyAuthentication(
        host="XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com",
        api_key="YOUR_API_KEY",
    )

    transport = AppSyncWebsocketsTransport(
        url="https://XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com/graphql",
        auth=auth,
    )

Reference: :class:`gql.transport.appsync_auth.AppSyncApiKeyAuthentication`

.. _appsync_iam_auth:

IAM
^^^

For the IAM authentication, you can simply create your transport without
an auth argument.

The region name will be autodetected from the url or from your AWS configuration
(:code:`.aws/config`) or the environment variable:

- AWS_DEFAULT_REGION

The credentials will be detected from your AWS configuration file
(:code:`.aws/credentials`) or from the environment variables:

- AWS_ACCESS_KEY_ID
- AWS_SECRET_ACCESS_KEY
- AWS_SESSION_TOKEN (optional)

.. code-block:: python

    transport = AppSyncWebsocketsTransport(
        url="https://XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com/graphql",
    )

OR You can also provide the credentials manually by creating the
:code:`AppSyncIAMAuthentication` class yourself:

.. code-block:: python

    from botocore.credentials import Credentials

    credentials = Credentials(
        access_key = os.environ.get("AWS_ACCESS_KEY_ID"),
        secret_key= os.environ.get("AWS_SECRET_ACCESS_KEY"),
        token=os.environ.get("AWS_SESSION_TOKEN", None),   # Optional
    )

    auth = AppSyncIAMAuthentication(
        host="XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com",
        credentials=credentials,
        region_name="your region"
    )

    transport = AppSyncWebsocketsTransport(
        url="https://XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com/graphql",
        auth=auth,
    )

Reference: :class:`gql.transport.appsync_auth.AppSyncIAMAuthentication`

.. _appsync_jwt_auth:

Json Web Tokens (jwt)
^^^^^^^^^^^^^^^^^^^^^

AWS provides json web tokens (jwt) for the authentication methods:

- Amazon Cognito user pools
- OpenID Connect (OIDC)

For these authentication methods, you can use the :code:`AppSyncJWTAuthentication` class:

.. code-block:: python

    auth = AppSyncJWTAuthentication(
        host="XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com",
        jwt="YOUR_JWT_STRING",
    )

    transport = AppSyncWebsocketsTransport(
        url="https://XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com/graphql",
        auth=auth,
    )

Reference: :class:`gql.transport.appsync_auth.AppSyncJWTAuthentication`

.. _appsync_http:

AppSync GraphQL Queries and mutations
-------------------------------------

Queries and mutations are not allowed on the realtime websockets endpoint.
But you can use the :ref:`AIOHTTPTransport <aiohttp_transport>` to create
a normal http session and reuse the authentication classes to create the headers for you.

Full example with API key authentication from environment variables:

.. literalinclude:: ../code_examples/appsync/mutation_api_key.py

From the command line
---------------------

Using :ref:`gql-cli <gql_cli>`, it is possible to execute GraphQL queries and subscriptions
from the command line on an AppSync endpoint.

- For queries and mutations, use the :code:`--transport appsync_http` argument::

    # Put the request in a file
    $ echo 'mutation createMessage($message: String!) {
      createMessage(input: {message: $message}) {
        id
        message
        createdAt
      }
    }' > mutation.graphql

    # Execute the request using gql-cli with --transport appsync_http
    $ cat mutation.graphql | gql-cli $AWS_GRAPHQL_API_ENDPOINT --transport appsync_http -V message:"Hello world!"

- For subscriptions, use the :code:`--transport appsync_websockets` argument::

    echo "subscription{onCreateMessage{message}}" | gql-cli $AWS_GRAPHQL_API_ENDPOINT --transport appsync_websockets

- You can also get the full GraphQL schema from the backend from introspection::

    $ gql-cli $AWS_GRAPHQL_API_ENDPOINT --transport appsync_http --print-schema > schema.graphql


================================================
File: docs/transports/async_transports.rst
================================================
.. _async_transports:

Async Transports
================

Async transports are transports which are using an underlying async library. They allow us to
:ref:`run GraphQL queries asynchronously <async_usage>`

.. toctree::
   :maxdepth: 1

   aiohttp
   httpx_async
   websockets
   aiohttp_websockets
   phoenix
   appsync


================================================
File: docs/transports/httpx.rst
================================================
.. _httpx_transport:

HTTPXTransport
==============

The HTTPXTransport is a sync transport using the `httpx`_ library
and allows you to send GraphQL queries using the HTTP protocol.

Reference: :class:`gql.transport.httpx.HTTPXTransport`

.. literalinclude:: ../code_examples/httpx_sync.py

.. _httpx: https://www.python-httpx.org


================================================
File: docs/transports/httpx_async.rst
================================================
.. _httpx_async_transport:

HTTPXAsyncTransport
===================

This transport uses the `httpx`_ library and allows you to send GraphQL queries using the HTTP protocol.

Reference: :class:`gql.transport.httpx.HTTPXAsyncTransport`

.. note::

    GraphQL subscriptions are not supported on the HTTP transport.
    For subscriptions you should use the :ref:`websockets transport <websockets_transport>`.

.. literalinclude:: ../code_examples/httpx_async.py

Authentication
--------------

There are multiple ways to authenticate depending on the server configuration.

1. Using HTTP Headers

.. code-block:: python

    transport = HTTPXAsyncTransport(
        url='https://SERVER_URL:SERVER_PORT/graphql',
        headers={'Authorization': 'token'}
    )

2. Using HTTP Cookies

You can manually set the cookies which will be sent with each connection:

.. code-block:: python

    transport = HTTPXAsyncTransport(url=url, cookies={"cookie1": "val1"})

.. _httpx: https://www.python-httpx.org


================================================
File: docs/transports/index.rst
================================================
.. _transports:

Transports
==========

GQL Transports are used to define how the connection is made with the backend.
We have different transports for different underlying protocols (http, websockets, ...)

.. toctree::
   :maxdepth: 2

   async_transports
   sync_transports


================================================
File: docs/transports/phoenix.rst
================================================
.. _phoenix_transport:

PhoenixChannelWebsocketsTransport
=================================

The PhoenixChannelWebsocketsTransport is an async transport which allows you
to execute queries and subscriptions against an `Absinthe`_ backend using the `Phoenix`_
framework `channels`_.

Reference:
:class:`gql.transport.phoenix_channel_websockets.PhoenixChannelWebsocketsTransport`

.. literalinclude:: ../code_examples/phoenix_channel_async.py

.. _Absinthe: http://absinthe-graphql.org
.. _Phoenix: https://www.phoenixframework.org
.. _channels: https://hexdocs.pm/phoenix/Phoenix.Channel.html#content


================================================
File: docs/transports/requests.rst
================================================
.. _requests_transport:

RequestsHTTPTransport
=====================

The RequestsHTTPTransport is a sync transport using the `requests`_ library
and allows you to send GraphQL queries using the HTTP protocol.

Reference: :class:`gql.transport.requests.RequestsHTTPTransport`

.. literalinclude:: ../code_examples/requests_sync.py

.. _requests: https://requests.readthedocs.io


================================================
File: docs/transports/sync_transports.rst
================================================
.. _sync_transports:

Sync Transports
================

Sync transports are transports which are using an underlying sync library.
They cannot be used asynchronously.

.. toctree::
   :maxdepth: 1

   requests
   httpx


================================================
File: docs/transports/websockets.rst
================================================
.. _websockets_transport:

WebsocketsTransport
===================

The websockets transport supports both:

 - the `Apollo websockets transport protocol`_.
 - the `GraphQL-ws websockets transport protocol`_

It will propose both subprotocols to the backend and detect the supported protocol
from the response http headers returned by the backend.

.. note::
    For some backends (graphql-ws before `version 5.6.1`_ without backwards compatibility), it may be necessary to specify
    only one subprotocol to the backend. It can be done by using
    :code:`subprotocols=[WebsocketsTransport.GRAPHQLWS_SUBPROTOCOL]`
    or :code:`subprotocols=[WebsocketsTransport.APOLLO_SUBPROTOCOL]` in the transport arguments.

This transport allows to do multiple queries, mutations and subscriptions on the same websocket connection.

Reference: :class:`gql.transport.websockets.WebsocketsTransport`

.. literalinclude:: ../code_examples/websockets_async.py

Websockets SSL
--------------

If you need to connect to an ssl encrypted endpoint:

* use :code:`wss` instead of :code:`ws` in the url of the transport

.. code-block:: python

    transport = WebsocketsTransport(
        url='wss://SERVER_URL:SERVER_PORT/graphql',
        headers={'Authorization': 'token'}
    )

If you have a self-signed ssl certificate, you need to provide an ssl_context with the server public certificate:

.. code-block:: python

    import pathlib
    import ssl

    ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)
    localhost_pem = pathlib.Path(__file__).with_name("YOUR_SERVER_PUBLIC_CERTIFICATE.pem")
    ssl_context.load_verify_locations(localhost_pem)

    transport = WebsocketsTransport(
        url='wss://SERVER_URL:SERVER_PORT/graphql',
        ssl=ssl_context
    )

If you have also need to have a client ssl certificate, add:

.. code-block:: python

    ssl_context.load_cert_chain(certfile='YOUR_CLIENT_CERTIFICATE.pem', keyfile='YOUR_CLIENT_CERTIFICATE_KEY.key')

Websockets authentication
-------------------------

There are two ways to send authentication tokens with websockets depending on the server configuration.

1. Using HTTP Headers

.. code-block:: python

    transport = WebsocketsTransport(
        url='wss://SERVER_URL:SERVER_PORT/graphql',
        headers={'Authorization': 'token'}
    )

2. With a payload in the connection_init websocket message

.. code-block:: python

    transport = WebsocketsTransport(
        url='wss://SERVER_URL:SERVER_PORT/graphql',
        init_payload={'Authorization': 'token'}
    )

.. _websockets_transport_keepalives:

Keep-Alives
-----------

Apollo protocol
^^^^^^^^^^^^^^^

With the Apollo protocol, the backend can optionally send unidirectional keep-alive ("ka") messages
(only from the server to the client).

It is possible to configure the transport to close if we don't receive a "ka" message
within a specified time using the :code:`keep_alive_timeout` parameter.

Here is an example with 60 seconds::

    transport = WebsocketsTransport(
        url='wss://SERVER_URL:SERVER_PORT/graphql',
        keep_alive_timeout=60,
    )

One disadvantage of the Apollo protocol is that because the keep-alives are only sent from the server
to the client, it can be difficult to detect the loss of a connection quickly from the server side.

GraphQL-ws protocol
^^^^^^^^^^^^^^^^^^^

With the GraphQL-ws protocol, it is possible to send bidirectional ping/pong messages.
Pings can be sent either from the client or the server and the other party should answer with a pong.

As with the Apollo protocol, it is possible to configure the transport to close if we don't
receive any message from the backend within the specified time using the :code:`keep_alive_timeout` parameter.

But there is also the possibility for the client to send pings at a regular interval and verify
that the backend sends a pong within a specified delay.
This can be done using the :code:`ping_interval` and :code:`pong_timeout` parameters.

Here is an example with a ping sent every 60 seconds, expecting a pong within 10 seconds::

    transport = WebsocketsTransport(
        url='wss://SERVER_URL:SERVER_PORT/graphql',
        ping_interval=60,
        pong_timeout=10,
    )

Underlying websockets protocol
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In addition to the keep-alives described above for the apollo and graphql-ws protocols,
there are also `ping frames`_ sent by the underlying websocket connection itself for both of them.

These pings are enabled by default (every 20 seconds) and could be modified or disabled
by passing extra arguments to the :code:`connect` call of the websockets client using the
:code:`connect_args` argument of the transport.

.. code-block:: python

    # Disabling websocket protocol level pings
    transport = WebsocketsTransport(
        url='wss://SERVER_URL:SERVER_PORT/graphql',
        connect_args={"ping_interval": None},
    )

See the `websockets keepalive documentation`_ for details.

.. _version 5.6.1: https://github.com/enisdenjo/graphql-ws/releases/tag/v5.6.1
.. _Apollo websockets transport protocol:  https://github.com/apollographql/subscriptions-transport-ws/blob/master/PROTOCOL.md
.. _GraphQL-ws websockets transport protocol: https://github.com/enisdenjo/graphql-ws/blob/master/PROTOCOL.md
.. _ping frames: https://www.rfc-editor.org/rfc/rfc6455.html#section-5.5.2
.. _websockets keepalive documentation: https://websockets.readthedocs.io/en/stable/topics/timeouts.html#keepalive-in-websockets


================================================
File: docs/usage/basic_usage.rst
================================================
.. _basic_usage:

Basic usage
-----------

In order to execute a GraphQL request against a GraphQL API:

* create your gql :ref:`transport <transports>` in order to choose the destination url
  and the protocol used to communicate with it
* create a gql :class:`Client <gql.client.Client>` with the selected transport
* parse a query using :func:`gql <gql.gql>`
* execute the query on the client to get the result

.. literalinclude:: ../code_examples/aiohttp_sync.py

.. warning::

    Please note that this basic example won't work if you have an asyncio event loop running. In some
    python environments (as with Jupyter which uses IPython) an asyncio event loop is created for you.
    In that case you should use instead the :ref:`Async Usage example<async_usage>`.



================================================
File: docs/usage/custom_scalars_and_enums.rst
================================================
Custom scalars and enums
========================

.. _custom_scalars:

Custom scalars
--------------

Scalar types represent primitive values at the leaves of a query.

GraphQL provides a number of built-in scalars (Int, Float, String, Boolean and ID), but a GraphQL backend
can add additional custom scalars to its schema to better express values in their data model.

For example, a schema can define the Datetime scalar to represent an ISO-8601 encoded date.

The schema will then only contain::

    scalar Datetime

When custom scalars are sent to the backend (as inputs) or from the backend (as outputs),
their values need to be serialized to be composed
of only built-in scalars, then at the destination the serialized values will be parsed again to
be able to represent the scalar in its local internal representation.

Because this serialization/unserialization is dependent on the language used at both sides, it is not
described in the schema and needs to be defined independently at both sides (client, backend).

A custom scalar value can have two different representations during its transport:

 - as a serialized value (usually as json):

    * in the results sent by the backend
    * in the variables sent by the client alongside the query

 - as "literal" inside the query itself sent by the client

To define a custom scalar, you need 3 methods:

 - a :code:`serialize` method used:

    * by the backend to serialize a custom scalar output in the result
    * by the client to serialize a custom scalar input in the variables

 - a :code:`parse_value` method used:

    * by the backend to unserialize custom scalars inputs in the variables sent by the client
    * by the client to unserialize custom scalars outputs from the results

 - a :code:`parse_literal` method used:

    * by the backend to unserialize custom scalars inputs inside the query itself

To define a custom scalar object, graphql-core provides the :code:`GraphQLScalarType` class
which contains the implementation of the above methods.

Example for Datetime:

.. code-block:: python

    from datetime import datetime
    from typing import Any, Dict, Optional

    from graphql import GraphQLScalarType, ValueNode
    from graphql.utilities import value_from_ast_untyped


    def serialize_datetime(value: Any) -> str:
        return value.isoformat()


    def parse_datetime_value(value: Any) -> datetime:
        return datetime.fromisoformat(value)


    def parse_datetime_literal(
        value_node: ValueNode, variables: Optional[Dict[str, Any]] = None
    ) -> datetime:
        ast_value = value_from_ast_untyped(value_node, variables)
        return parse_datetime_value(ast_value)


    DatetimeScalar = GraphQLScalarType(
        name="Datetime",
        serialize=serialize_datetime,
        parse_value=parse_datetime_value,
        parse_literal=parse_datetime_literal,
    )

If you get your schema from a "schema.graphql" file or from introspection,
then the generated schema in the gql Client will contain default :code:`GraphQLScalarType` instances
where the serialize and parse_value methods simply return the serialized value without modification.

In that case, if you want gql to parse custom scalars to a more useful Python representation,
or to serialize custom scalars variables from a Python representation,
then you can use the :func:`update_schema_scalars <gql.utilities.update_schema_scalars>`
or :func:`update_schema_scalar <gql.utilities.update_schema_scalar>` methods
to modify the definition of a scalar in your schema so that gql could do the parsing/serialization.

.. code-block:: python

    from gql.utilities import update_schema_scalar

    with open('path/to/schema.graphql') as f:
        schema_str = f.read()

    client = Client(schema=schema_str, ...)

    update_schema_scalar(client.schema, "Datetime", DatetimeScalar)

    # or update_schema_scalars(client.schema, [DatetimeScalar])

.. _enums:

Enums
-----

GraphQL Enum types are a special kind of scalar that is restricted to a particular set of allowed values.

For example, the schema may have a Color enum and contain::

    enum Color {
        RED
        GREEN
        BLUE
    }

Graphql-core provides the :code:`GraphQLEnumType` class to define an enum in the schema
(See `graphql-core schema building docs`_).

This class defines how the enum is serialized and parsed.

If you get your schema from a "schema.graphql" file or from introspection,
then the generated schema in the gql Client will contain default :code:`GraphQLEnumType` instances
which should serialize/parse enums to/from its String representation (the :code:`RED` enum
will be serialized to :code:`'RED'`).

You may want to parse enums to convert them to Python Enum types.
In that case, you can use the :func:`update_schema_enum <gql.utilities.update_schema_enum>`
to modify the default :code:`GraphQLEnumType` to use your defined Enum.

Example:

.. code-block:: python

    from enum import Enum
    from gql.utilities import update_schema_enum

    class Color(Enum):
        RED = 0
        GREEN = 1
        BLUE = 2

    with open('path/to/schema.graphql') as f:
        schema_str = f.read()

    client = Client(schema=schema_str, ...)

    update_schema_enum(client.schema, 'Color', Color)

Serializing Inputs
------------------

To provide custom scalars and/or enums in inputs with gql, you can:

- serialize the inputs manually
- let gql serialize the inputs using the custom scalars and enums defined in the schema

Manually
^^^^^^^^

You can serialize inputs yourself:

 - in the query itself
 - in variables

This has the advantage that you don't need a schema...

In the query
""""""""""""

- custom scalar:

.. code-block:: python

    query = gql(
        """{
        shiftDays(time: "2021-11-12T11:58:13.461161", days: 5)
    }"""
    )

- enum:

.. code-block:: python

    query = gql("{opposite(color: RED)}")

In a variable
"""""""""""""

- custom scalar:

.. code-block:: python

    query = gql("query shift5days($time: Datetime) {shiftDays(time: $time, days: 5)}")

    variable_values = {
        "time": "2021-11-12T11:58:13.461161",
    }

    result = client.execute(query, variable_values=variable_values)

- enum:

.. code-block:: python

    query = gql(
        """
        query GetOppositeColor($color: Color) {
            opposite(color:$color)
        }"""
    )

    variable_values = {
        "color": 'RED',
    }

    result = client.execute(query, variable_values=variable_values)

Automatically
^^^^^^^^^^^^^

If you have custom scalar and/or enums defined in your schema
(See: :ref:`custom_scalars` and :ref:`enums`),
then you can request gql to serialize your variables automatically.

- use :code:`Client(..., serialize_variables=True)` to request serializing variables for all queries
- use :code:`execute(..., serialize_variables=True)` or :code:`subscribe(..., serialize_variables=True)` if
  you want gql to serialize the variables only for a single query.

Examples:

- custom scalars:

.. code-block:: python

    from gql.utilities import update_schema_scalars

    from .myscalars import DatetimeScalar

    async with Client(transport=transport, fetch_schema_from_transport=True) as session:

        # We update the schema we got from introspection with our custom scalar type
        update_schema_scalars(session.client.schema, [DatetimeScalar])

        # In the query, the custom scalar in the input is set to a variable
        query = gql("query shift5days($time: Datetime) {shiftDays(time: $time, days: 5)}")

        # the argument for time is a datetime instance
        variable_values = {"time": datetime.now()}

        # we execute the query with serialize_variables set to True
        result = await session.execute(
            query, variable_values=variable_values, serialize_variables=True
        )

- enums:

.. code-block:: python

    from gql.utilities import update_schema_enum

    from .myenums import Color

    async with Client(transport=transport, fetch_schema_from_transport=True) as session:

        # We update the schema we got from introspection with our custom enum
        update_schema_enum(session.client.schema, 'Color', Color)

        # In the query, the enum in the input is set to a variable
        query = gql(
            """
            query GetOppositeColor($color: Color) {
                opposite(color:$color)
            }"""
        )

        # the argument for time is an instance of our Enum type
        variable_values = {
            "color": Color.RED,
        }

        # we execute the query with serialize_variables set to True
        result = client.execute(
            query, variable_values=variable_values, serialize_variables=True
        )

Parsing output
--------------

By default, gql returns the serialized result from the backend without parsing
(except json unserialization to Python default types).

if you want to convert the result of custom scalars to custom objects,
you can request gql to parse the results.

- use :code:`Client(..., parse_results=True)` to request parsing for all queries
- use :code:`execute(..., parse_result=True)` or :code:`subscribe(..., parse_result=True)` if
  you want gql to parse only the result of a single query.

Same example as above, with result parsing enabled:

.. code-block:: python

    from gql.utilities import update_schema_scalars

    async with Client(transport=transport, fetch_schema_from_transport=True) as session:

        update_schema_scalars(session.client.schema, [DatetimeScalar])

        query = gql("query shift5days($time: Datetime) {shiftDays(time: $time, days: 5)}")

        variable_values = {"time": datetime.now()}

        result = await session.execute(
            query,
            variable_values=variable_values,
            serialize_variables=True,
            parse_result=True,
        )

        # now result["time"] type is a datetime instead of string

.. _graphql-core schema building docs: https://graphql-core-3.readthedocs.io/en/latest/usage/schema.html


================================================
File: docs/usage/extensions.rst
================================================
.. _extensions:

Extensions
----------

When you execute (or subscribe) GraphQL requests, the server will send
responses which may have 3 fields:

- data: the serialized response from the backend
- errors: a list of potential errors
- extensions: an optional field for additional data

If there are errors in the response, then the
:code:`execute` or :code:`subscribe` methods will
raise a :code:`TransportQueryError`.

If no errors are present, then only the data from the response is returned by default.

.. code-block:: python

    result = client.execute(query)
    # result is here the content of the data field

If you need to receive the extensions data too, then you can run the
:code:`execute` or :code:`subscribe` methods with :code:`get_execution_result=True`.

In that case, the full execution result is returned and you can have access
to the extensions field

.. code-block:: python

    result = client.execute(query, get_execution_result=True)
    # result is here an ExecutionResult instance

    # result.data is the content of the data field
    # result.extensions is the content of the extensions field


================================================
File: docs/usage/file_upload.rst
================================================
File uploads
============

GQL supports file uploads with the :ref:`aiohttp transport <aiohttp_transport>`, the
:ref:`requests transport <requests_transport>`, the :ref:`httpx transport <httpx_transport>`,
and the :ref:`httpx async transport <httpx_async_transport>`,
using the `GraphQL multipart request spec`_.

.. _GraphQL multipart request spec: https://github.com/jaydenseric/graphql-multipart-request-spec

Single File
-----------

In order to upload a single file, you need to:

* set the file as a variable value in the mutation
* provide the opened file to the `variable_values` argument of `execute`
* set the `upload_files` argument to True

.. code-block:: python

    transport = AIOHTTPTransport(url='YOUR_URL')
    # Or transport = RequestsHTTPTransport(url='YOUR_URL')
    # Or transport = HTTPXTransport(url='YOUR_URL')
    # Or transport = HTTPXAsyncTransport(url='YOUR_URL')

    client = Client(transport=transport)

    query = gql('''
      mutation($file: Upload!) {
        singleUpload(file: $file) {
          id
        }
      }
    ''')

    with open("YOUR_FILE_PATH", "rb") as f:

        params = {"file": f}

        result = client.execute(
            query, variable_values=params, upload_files=True
        )

Setting the content-type
^^^^^^^^^^^^^^^^^^^^^^^^

If you need to set a specific Content-Type attribute to a file,
you can set the :code:`content_type` attribute of the file like this:

.. code-block:: python

    with open("YOUR_FILE_PATH", "rb") as f:

        # Setting the content-type to a pdf file for example
        f.content_type = "application/pdf"

        params = {"file": f}

        result = client.execute(
            query, variable_values=params, upload_files=True
        )

File list
---------

It is also possible to upload multiple files using a list.

.. code-block:: python

    transport = AIOHTTPTransport(url='YOUR_URL')
    # Or transport = RequestsHTTPTransport(url='YOUR_URL')
    # Or transport = HTTPXTransport(url='YOUR_URL')
    # Or transport = HTTPXAsyncTransport(url='YOUR_URL')

    client = Client(transport=transport)

    query = gql('''
      mutation($files: [Upload!]!) {
        multipleUpload(files: $files) {
          id
        }
      }
    ''')

    f1 = open("YOUR_FILE_PATH_1", "rb")
    f2 = open("YOUR_FILE_PATH_2", "rb")

    params = {"files": [f1, f2]}

    result = client.execute(
        query, variable_values=params, upload_files=True
    )

    f1.close()
    f2.close()


Streaming
---------

If you use the above methods to send files, then the entire contents of the files
must be loaded in memory before the files are sent.
If the files are not too big and you have enough RAM, it is not a problem.
On another hand if you want to avoid using too much memory, then it is better
to read the files and send them in small chunks so that the entire file contents
don't have to be in memory at once.

We provide methods to do that for two different uses cases:

* Sending local files
* Streaming downloaded files from an external URL to the GraphQL API

.. note::
    Streaming is only supported with the :ref:`aiohttp transport <aiohttp_transport>`

Streaming local files
^^^^^^^^^^^^^^^^^^^^^

aiohttp allows to upload files using an asynchronous generator.
See `Streaming uploads on aiohttp docs`_.


In order to stream local files, instead of providing opened files to the
`variable_values` argument of `execute`, you need to provide an async generator
which will provide parts of the files.

You can use `aiofiles`_
to read the files in chunks and create this asynchronous generator.

.. _Streaming uploads on aiohttp docs: https://docs.aiohttp.org/en/stable/client_quickstart.html#streaming-uploads
.. _aiofiles: https://github.com/Tinche/aiofiles

Example:

.. code-block:: python

    transport = AIOHTTPTransport(url='YOUR_URL')

    client = Client(transport=transport)

    query = gql('''
      mutation($file: Upload!) {
        singleUpload(file: $file) {
          id
        }
      }
    ''')

    async def file_sender(file_name):
        async with aiofiles.open(file_name, 'rb') as f:
            chunk = await f.read(64*1024)
                while chunk:
                    yield chunk
                    chunk = await f.read(64*1024)

    params = {"file": file_sender(file_name='YOUR_FILE_PATH')}

    result = client.execute(
		query, variable_values=params, upload_files=True
	)

Streaming downloaded files
^^^^^^^^^^^^^^^^^^^^^^^^^^

If the file you want to upload to the GraphQL API is not present locally
and needs to be downloaded from elsewhere, then it is possible to chain the download
and the upload in order to limit the amout of memory used.

Because the `content` attribute of an aiohttp response is a `StreamReader`
(it provides an async iterator protocol), you can chain the download and the upload
together.

In order to do that, you need to:

* get the response from an aiohttp request and then get the StreamReader instance
  from `resp.content`
* provide the StreamReader instance to the `variable_values` argument of `execute`

Example:

.. code-block:: python

    # First request to download your file with aiohttp
    async with aiohttp.ClientSession() as http_client:
        async with http_client.get('YOUR_DOWNLOAD_URL') as resp:

            # We now have a StreamReader instance in resp.content
            # and we provide it to the variable_values argument of execute

            transport = AIOHTTPTransport(url='YOUR_GRAPHQL_URL')

            client = Client(transport=transport)

            query = gql('''
              mutation($file: Upload!) {
                singleUpload(file: $file) {
                  id
                }
              }
            ''')

            params = {"file": resp.content}

            result = client.execute(
                query, variable_values=params, upload_files=True
            )


================================================
File: docs/usage/headers.rst
================================================
HTTP Headers
============

If you want to add additional http headers for your connection, you can specify these in your transport:

.. code-block:: python

    transport = AIOHTTPTransport(url='YOUR_URL', headers={'Authorization': 'token'})

After the connection, the latest response headers can be found in :code:`transport.response_headers`


================================================
File: docs/usage/index.rst
================================================
Usage
=====

.. toctree::
   :maxdepth: 2

   basic_usage
   validation
   subscriptions
   variables
   headers
   file_upload
   custom_scalars_and_enums
   extensions


================================================
File: docs/usage/subscriptions.rst
================================================
Subscriptions
=============

Using the :ref:`websockets transport <websockets_transport>`, it is possible to execute GraphQL subscriptions:

.. code-block:: python

    from gql import gql, Client
    from gql.transport.websockets import WebsocketsTransport

    transport = WebsocketsTransport(url='wss://your_server/graphql')

    client = Client(
        transport=transport,
        fetch_schema_from_transport=True,
    )

    query = gql('''
        subscription yourSubscription {
            ...
        }
    ''')

    for result in client.subscribe(query):
        print (result)

.. note::

    The websockets transport can also execute queries or mutations, it is not restricted to subscriptions


================================================
File: docs/usage/validation.rst
================================================
.. _schema_validation:

Schema validation
=================

If a GraphQL schema is provided, gql will validate the queries locally before sending them to the backend.
If no schema is provided, gql will send the query to the backend without local validation.

You can either provide a schema yourself, or you can request gql to get the schema
from the backend using `introspection`_.

Using a provided schema
-----------------------

The schema can be provided as a String (which is usually stored in a .graphql file):

.. code-block:: python

    with open('path/to/schema.graphql') as f:
        schema_str = f.read()

    client = Client(schema=schema_str)

.. note::
    You can download a schema from a server by using :ref:`gql-cli <gql_cli>`

    :code:`$ gql-cli https://SERVER_URL/graphql --print-schema --schema-download input_value_deprecation:true > schema.graphql`

OR can be created using python classes:

.. code-block:: python

    from .someSchema import SampleSchema
    # SampleSchema is an instance of GraphQLSchema

    client = Client(schema=SampleSchema)

See `tests/starwars/schema.py`_ for an example of such a schema.

Using introspection
-------------------

In order to get the schema directly from the GraphQL Server API using the transport, you need
to set the `fetch_schema_from_transport` argument of Client to True, and the client will
fetch the schema directly after the first connection to the backend.

.. _introspection: https://graphql.org/learn/introspection
.. _tests/starwars/schema.py: https://github.com/graphql-python/gql/blob/master/tests/starwars/schema.py


================================================
File: docs/usage/variables.rst
================================================
Using variables
===============

It is possible to provide variable values with your query by providing a Dict to
the variable_values argument of the `execute` or the `subscribe` methods.

The variable values will be sent alongside the query in the transport message
(there is no local substitution).

.. code-block:: python

    query = gql(
        """
        query getContinentName ($code: ID!) {
          continent (code: $code) {
            name
          }
        }
    """
    )

    params = {"code": "EU"}

    # Get name of continent with code "EU"
    result = client.execute(query, variable_values=params)
    print(result)

    params = {"code": "AF"}

    # Get name of continent with code "AF"
    result = client.execute(query, variable_values=params)
    print(result)


================================================
File: gql/__init__.py
================================================
"""The primary :mod:`gql` package includes everything you need to
execute GraphQL requests, with the exception of the transports
which are optional:

 - the :func:`gql <gql.gql>` method to parse a GraphQL query
 - the :class:`Client <gql.Client>` class as the entrypoint to execute requests
   and create sessions
"""

from .__version__ import __version__
from .client import Client
from .gql import gql
from .graphql_request import GraphQLRequest

__all__ = [
    "__version__",
    "gql",
    "Client",
    "GraphQLRequest",
]


================================================
File: gql/__version__.py
================================================
__version__ = "3.6.0b3"


================================================
File: gql/cli.py
================================================
import asyncio
import json
import logging
import signal as signal_module
import sys
import textwrap
from argparse import ArgumentParser, Namespace, RawTextHelpFormatter
from typing import Any, Dict, Optional

from graphql import GraphQLError, print_schema
from yarl import URL

from gql import Client, __version__, gql
from gql.transport import AsyncTransport
from gql.transport.exceptions import TransportQueryError

description = """
Send GraphQL queries from the command line using http(s) or websockets.
If used interactively, write your query, then use Ctrl-D (EOF) to execute it.
"""

examples = """
EXAMPLES
========

# Simple query using https
echo 'query { continent(code:"AF") { name } }' | \
gql-cli https://countries.trevorblades.com

# Simple query using websockets
echo 'query { continent(code:"AF") { name } }' | \
gql-cli wss://countries.trevorblades.com/graphql

# Query with variable
echo 'query getContinent($code:ID!) { continent(code:$code) { name } }' | \
gql-cli https://countries.trevorblades.com --variables code:AF

# Interactive usage (insert your query in the terminal, then press Ctrl-D to execute it)
gql-cli wss://countries.trevorblades.com/graphql --variables code:AF

# Execute query saved in a file
cat query.gql | gql-cli wss://countries.trevorblades.com/graphql

# Print the schema of the backend
gql-cli https://countries.trevorblades.com/graphql --print-schema

"""


def positive_int_or_none(value_str: str) -> Optional[int]:
    """Convert a string argument value into either an int or None.

    Raise a ValueError if the argument is negative or a string which is not "none"
    """
    try:
        value_int = int(value_str)
    except ValueError:
        if value_str.lower() == "none":
            return None
        else:
            raise

    if value_int < 0:
        raise ValueError

    return value_int


def get_parser(with_examples: bool = False) -> ArgumentParser:
    """Provides an ArgumentParser for the gql-cli script.

    This function is also used by sphinx to generate the script documentation.

    :param with_examples: set to False by default so that the examples are not
                          present in the sphinx docs (they are put there with
                          a different layout)
    """

    parser = ArgumentParser(
        description=description,
        epilog=examples if with_examples else None,
        formatter_class=RawTextHelpFormatter,
    )
    parser.add_argument(
        "server", help="the server url starting with http://, https://, ws:// or wss://"
    )
    parser.add_argument(
        "-V",
        "--variables",
        nargs="*",
        help="query variables in the form key:json_value",
    )
    parser.add_argument(
        "-H", "--headers", nargs="*", help="http headers in the form key:value"
    )
    parser.add_argument("--version", action="version", version=f"v{__version__}")
    group = parser.add_mutually_exclusive_group()
    group.add_argument(
        "-d",
        "--debug",
        help="print lots of debugging statements (loglevel==DEBUG)",
        action="store_const",
        dest="loglevel",
        const=logging.DEBUG,
    )
    group.add_argument(
        "-v",
        "--verbose",
        help="show low level messages (loglevel==INFO)",
        action="store_const",
        dest="loglevel",
        const=logging.INFO,
    )
    parser.add_argument(
        "-o",
        "--operation-name",
        help="set the operation_name value",
        dest="operation_name",
    )
    parser.add_argument(
        "--print-schema",
        help="get the schema from instrospection and print it",
        action="store_true",
        dest="print_schema",
    )
    parser.add_argument(
        "--schema-download",
        nargs="*",
        help=textwrap.dedent(
            """select the introspection query arguments to download the schema.
            Only useful if --print-schema is used.
            By default, it will:

             - request field descriptions
             - not request deprecated input fields

            Possible options:

             - descriptions:false             for a compact schema without comments
             - input_value_deprecation:true   to download deprecated input fields
             - specified_by_url:true
             - schema_description:true
             - directive_is_repeatable:true"""
        ),
        dest="schema_download",
    )
    parser.add_argument(
        "--execute-timeout",
        help="set the execute_timeout argument of the Client (default: 10)",
        type=positive_int_or_none,
        default=10,
        dest="execute_timeout",
    )
    parser.add_argument(
        "--transport",
        default="auto",
        choices=[
            "auto",
            "aiohttp",
            "httpx",
            "phoenix",
            "websockets",
            "aiohttp_websockets",
            "appsync_http",
            "appsync_websockets",
        ],
        help=(
            "select the transport. 'auto' by default: "
            "aiohttp or websockets depending on url scheme"
        ),
        dest="transport",
    )

    appsync_description = """
By default, for an AppSync backend, the IAM authentication is chosen.

If you want API key or JWT authentication, you can provide one of the
following arguments:"""

    appsync_group = parser.add_argument_group(
        "AWS AppSync options", description=appsync_description
    )

    appsync_auth_group = appsync_group.add_mutually_exclusive_group()

    appsync_auth_group.add_argument(
        "--api-key",
        help="Provide an API key for authentication",
        dest="api_key",
    )

    appsync_auth_group.add_argument(
        "--jwt",
        help="Provide an JSON Web token for authentication",
        dest="jwt",
    )

    return parser


def get_transport_args(args: Namespace) -> Dict[str, Any]:
    """Extract extra arguments necessary for the transport
    from the parsed command line args

    Will create a headers dict by splitting the colon
    in the --headers arguments

    :param args: parsed command line arguments
    """

    transport_args: Dict[str, Any] = {}

    # Parse the headers argument
    headers = {}
    if args.headers is not None:
        for header in args.headers:

            try:
                # Split only the first colon (throw a ValueError if no colon is present)
                header_key, header_value = header.split(":", 1)

                headers[header_key] = header_value

            except ValueError:
                raise ValueError(f"Invalid header: {header}")

    if args.headers is not None:
        transport_args["headers"] = headers

    return transport_args


def get_execute_args(args: Namespace) -> Dict[str, Any]:
    """Extract extra arguments necessary for the execute or subscribe
    methods from the parsed command line args

    Extract the operation_name

    Extract the variable_values from the --variables argument
    by splitting the first colon, then loads the json value,
    We try to add double quotes around the value if it does not work first
    in order to simplify the passing of simple string values
    (we allow --variables KEY:VALUE instead of KEY:\"VALUE\")

    :param args: parsed command line arguments
    """

    execute_args: Dict[str, Any] = {}

    # Parse the operation_name argument
    if args.operation_name is not None:
        execute_args["operation_name"] = args.operation_name

    # Parse the variables argument
    if args.variables is not None:

        variables = {}

        for var in args.variables:

            try:
                # Split only the first colon
                # (throw a ValueError if no colon is present)
                variable_key, variable_json_value = var.split(":", 1)

                # Extract the json value,
                # trying with double quotes if it does not work
                try:
                    variable_value = json.loads(variable_json_value)
                except json.JSONDecodeError:
                    try:
                        variable_value = json.loads(f'"{variable_json_value}"')
                    except json.JSONDecodeError:
                        raise ValueError

                # Save the value in the variables dict
                variables[variable_key] = variable_value

            except ValueError:
                raise ValueError(f"Invalid variable: {var}")

        execute_args["variable_values"] = variables

    return execute_args


def autodetect_transport(url: URL) -> str:
    """Detects which transport should be used depending on url."""

    if url.scheme in ["ws", "wss"]:
        try:
            import websockets  # noqa: F401

            transport_name = "websockets"
        except ImportError:  # pragma: no cover
            transport_name = "aiohttp_websockets"

    else:
        assert url.scheme in ["http", "https"]
        transport_name = "aiohttp"

    return transport_name


def get_transport(args: Namespace) -> Optional[AsyncTransport]:
    """Instantiate a transport from the parsed command line arguments

    :param args: parsed command line arguments
    """

    # Get the url scheme from server parameter
    url = URL(args.server)

    # Validate scheme
    if url.scheme not in ["http", "https", "ws", "wss"]:
        raise ValueError("URL protocol should be one of: http, https, ws, wss")

    # Get extra transport parameters from command line arguments
    # (headers)
    transport_args = get_transport_args(args)

    # Either use the requested transport or autodetect it
    if args.transport == "auto":
        transport_name = autodetect_transport(url)
    else:
        transport_name = args.transport

    # Import the correct transport class depending on the transport name
    if transport_name == "aiohttp":
        from gql.transport.aiohttp import AIOHTTPTransport

        return AIOHTTPTransport(url=args.server, **transport_args)

    elif transport_name == "httpx":
        from gql.transport.httpx import HTTPXAsyncTransport

        return HTTPXAsyncTransport(url=args.server, **transport_args)

    elif transport_name == "phoenix":
        from gql.transport.phoenix_channel_websockets import (
            PhoenixChannelWebsocketsTransport,
        )

        return PhoenixChannelWebsocketsTransport(url=args.server, **transport_args)

    elif transport_name == "websockets":
        from gql.transport.websockets import WebsocketsTransport

        transport_args["ssl"] = url.scheme == "wss"

        return WebsocketsTransport(url=args.server, **transport_args)

    elif transport_name == "aiohttp_websockets":
        from gql.transport.aiohttp_websockets import AIOHTTPWebsocketsTransport

        return AIOHTTPWebsocketsTransport(url=args.server, **transport_args)

    else:

        from gql.transport.appsync_auth import AppSyncAuthentication

        assert transport_name in ["appsync_http", "appsync_websockets"]
        assert url.host is not None

        auth: AppSyncAuthentication

        if args.api_key:
            from gql.transport.appsync_auth import AppSyncApiKeyAuthentication

            auth = AppSyncApiKeyAuthentication(host=url.host, api_key=args.api_key)

        elif args.jwt:
            from gql.transport.appsync_auth import AppSyncJWTAuthentication

            auth = AppSyncJWTAuthentication(host=url.host, jwt=args.jwt)

        else:
            from gql.transport.appsync_auth import AppSyncIAMAuthentication
            from botocore.exceptions import NoRegionError

            try:
                auth = AppSyncIAMAuthentication(host=url.host)
            except NoRegionError:
                # A warning message has been printed in the console
                return None

        transport_args["auth"] = auth

        if transport_name == "appsync_http":
            from gql.transport.aiohttp import AIOHTTPTransport

            return AIOHTTPTransport(url=args.server, **transport_args)

        else:
            from gql.transport.appsync_websockets import AppSyncWebsocketsTransport

            try:
                return AppSyncWebsocketsTransport(url=args.server, **transport_args)
            except Exception:
                # This is for the NoCredentialsError but we cannot import it here
                return None


def get_introspection_args(args: Namespace) -> Dict:
    """Get the introspection args depending on the schema_download argument"""

    # Parse the headers argument
    introspection_args = {}

    possible_args = [
        "descriptions",
        "specified_by_url",
        "directive_is_repeatable",
        "schema_description",
        "input_value_deprecation",
    ]

    if args.schema_download is not None:
        for arg in args.schema_download:

            try:
                # Split only the first colon (throw a ValueError if no colon is present)
                arg_key, arg_value = arg.split(":", 1)

                if arg_key not in possible_args:
                    raise ValueError(f"Invalid schema_download: {args.schema_download}")

                arg_value = arg_value.lower()
                if arg_value not in ["true", "false"]:
                    raise ValueError(f"Invalid schema_download: {args.schema_download}")

                introspection_args[arg_key] = arg_value == "true"

            except ValueError:
                raise ValueError(f"Invalid schema_download: {args.schema_download}")

    return introspection_args


async def main(args: Namespace) -> int:
    """Main entrypoint of the gql-cli script

    :param args: The parsed command line arguments
    :return: The script exit code (0 = ok, 1 = error)
    """

    # Set requested log level
    if args.loglevel is not None:
        logging.basicConfig(level=args.loglevel)

    try:
        # Instantiate transport from command line arguments
        transport = get_transport(args)

        if transport is None:
            return 1

        # Get extra execute parameters from command line arguments
        # (variables, operation_name)
        execute_args = get_execute_args(args)

    except ValueError as e:
        print(f"Error: {e}", file=sys.stderr)
        return 1

    # By default, the exit_code is 0 (everything is ok)
    exit_code = 0

    # Connect to the backend and provide a session
    async with Client(
        transport=transport,
        fetch_schema_from_transport=args.print_schema,
        introspection_args=get_introspection_args(args),
        execute_timeout=args.execute_timeout,
    ) as session:

        if args.print_schema:
            schema_str = print_schema(session.client.schema)
            print(schema_str)

            return exit_code

        while True:

            # Read multiple lines from input and trim whitespaces
            # Will read until EOF character is received (Ctrl-D)
            query_str = sys.stdin.read().strip()

            # Exit if query is empty
            if len(query_str) == 0:
                break

            # Parse query, continue on error
            try:
                query = gql(query_str)
            except GraphQLError as e:
                print(e, file=sys.stderr)
                exit_code = 1
                continue

            # Execute or Subscribe the query depending on transport
            try:
                try:
                    async for result in session.subscribe(query, **execute_args):
                        print(json.dumps(result))
                except KeyboardInterrupt:  # pragma: no cover
                    pass
                except NotImplementedError:
                    result = await session.execute(query, **execute_args)
                    print(json.dumps(result))
            except (GraphQLError, TransportQueryError) as e:
                print(e, file=sys.stderr)
                exit_code = 1

    return exit_code


def gql_cli() -> None:
    """Synchronously invoke ``main`` with the parsed command line arguments.

    Formerly ``scripts/gql-cli``, now registered as an ``entry_point``
    """
    # Get arguments from command line
    parser = get_parser(with_examples=True)
    args = parser.parse_args()

    try:
        # Create a new asyncio event loop
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        # Create a gql-cli task with the supplied arguments
        main_task = asyncio.ensure_future(main(args), loop=loop)

        # Add signal handlers to close gql-cli cleanly on Control-C
        for signal_name in ["SIGINT", "SIGTERM", "CTRL_C_EVENT", "CTRL_BREAK_EVENT"]:
            signal = getattr(signal_module, signal_name, None)

            if signal is None:
                continue

            try:
                loop.add_signal_handler(signal, main_task.cancel)
            except NotImplementedError:  # pragma: no cover
                # not all signals supported on all platforms
                pass

        # Run the asyncio loop to execute the task
        exit_code = 0
        try:
            exit_code = loop.run_until_complete(main_task)
        finally:
            loop.close()

        # Return with the correct exit code
        sys.exit(exit_code)
    except KeyboardInterrupt:  # pragma: no cover
        pass


================================================
File: gql/dsl.py
================================================
"""
.. image:: http://www.plantuml.com/plantuml/png/ZLAzJWCn3Dxz51vXw1im50ag8L4XwC1OkLTJ8gMvAd4GwEYxGuC8pTbKtUxy_TZEvsaIYfAt7e1MII9rWfsdbF1cSRzWpvtq4GT0JENduX8GXr_g7brQlf5tw-MBOx_-HlS0LV_Kzp8xr1kZav9PfCsMWvolEA_1VylHoZCExKwKv4Tg2s_VkSkca2kof2JDb0yxZYIk3qMZYUe1B1uUZOROXn96pQMugEMUdRnUUqUf6DBXQyIz2zu5RlgUQAFVNYaeRfBI79_JrUTaeg9JZFQj5MmUc69PDmNGE2iU61fDgfri3x36gxHw3gDHD6xqqQ7P4vjKqz2-602xtkO7uo17SCLhVSv25VjRjUAFcUE73Sspb8ADBl8gTT7j2cFAOPst_Wi0  # noqa
    :alt: UML diagram
"""
import logging
import re
from abc import ABC, abstractmethod
from math import isfinite
from typing import Any, Dict, Iterable, Mapping, Optional, Tuple, Union, cast

from graphql import (
    ArgumentNode,
    BooleanValueNode,
    DocumentNode,
    EnumValueNode,
    FieldNode,
    FloatValueNode,
    FragmentDefinitionNode,
    FragmentSpreadNode,
    GraphQLArgument,
    GraphQLEnumType,
    GraphQLError,
    GraphQLField,
    GraphQLID,
    GraphQLInputObjectType,
    GraphQLInputType,
    GraphQLInterfaceType,
    GraphQLList,
    GraphQLNamedType,
    GraphQLNonNull,
    GraphQLObjectType,
    GraphQLScalarType,
    GraphQLSchema,
    GraphQLString,
    InlineFragmentNode,
    IntValueNode,
    ListTypeNode,
    ListValueNode,
    NamedTypeNode,
    NameNode,
    NonNullTypeNode,
    NullValueNode,
    ObjectFieldNode,
    ObjectValueNode,
    OperationDefinitionNode,
    OperationType,
    SelectionSetNode,
    StringValueNode,
    TypeNode,
    Undefined,
    ValueNode,
    VariableDefinitionNode,
    VariableNode,
    get_named_type,
    introspection_types,
    is_enum_type,
    is_input_object_type,
    is_leaf_type,
    is_list_type,
    is_non_null_type,
    is_wrapping_type,
    print_ast,
)
from graphql.pyutils import inspect

from .utils import to_camel_case

log = logging.getLogger(__name__)

_re_integer_string = re.compile("^-?(?:0|[1-9][0-9]*)$")


def ast_from_serialized_value_untyped(serialized: Any) -> Optional[ValueNode]:
    """Given a serialized value, try our best to produce an AST.

    Anything ressembling an array (instance of Mapping) will be converted
    to an ObjectFieldNode.

    Anything ressembling a list (instance of Iterable - except str)
    will be converted to a ListNode.

    In some cases, a custom scalar can be serialized differently in the query
    than in the variables. In that case, this function will not work."""

    if serialized is None or serialized is Undefined:
        return NullValueNode()

    if isinstance(serialized, Mapping):
        field_items = (
            (key, ast_from_serialized_value_untyped(value))
            for key, value in serialized.items()
        )
        field_nodes = tuple(
            ObjectFieldNode(name=NameNode(value=field_name), value=field_value)
            for field_name, field_value in field_items
            if field_value
        )
        return ObjectValueNode(fields=field_nodes)

    if isinstance(serialized, Iterable) and not isinstance(serialized, str):
        maybe_nodes = (ast_from_serialized_value_untyped(item) for item in serialized)
        nodes = tuple(node for node in maybe_nodes if node)
        return ListValueNode(values=nodes)

    if isinstance(serialized, bool):
        return BooleanValueNode(value=serialized)

    if isinstance(serialized, int):
        return IntValueNode(value=str(serialized))

    if isinstance(serialized, float) and isfinite(serialized):
        value = str(serialized)
        if value.endswith(".0"):
            value = value[:-2]
        return FloatValueNode(value=value)

    if isinstance(serialized, str):
        return StringValueNode(value=serialized)

    raise TypeError(f"Cannot convert value to AST: {inspect(serialized)}.")


def ast_from_value(value: Any, type_: GraphQLInputType) -> Optional[ValueNode]:
    """
    This is a partial copy paste of the ast_from_value function in
    graphql-core utilities/ast_from_value.py

    Overwrite the if blocks that use recursion and add a new case to return a
    VariableNode when value is a DSLVariable

    Produce a GraphQL Value AST given a Python object.

    Raises a GraphQLError instead of returning None if we receive an Undefined
    of if we receive a Null value for a Non-Null type.
    """
    if isinstance(value, DSLVariable):
        return value.set_type(type_).ast_variable_name

    if is_non_null_type(type_):
        type_ = cast(GraphQLNonNull, type_)
        inner_type = type_.of_type
        ast_value = ast_from_value(value, inner_type)
        if isinstance(ast_value, NullValueNode):
            raise GraphQLError(
                "Received Null value for a Non-Null type " f"{inspect(inner_type)}."
            )
        return ast_value

    # only explicit None, not Undefined or NaN
    if value is None:
        return NullValueNode()

    # undefined
    if value is Undefined:
        raise GraphQLError(f"Received Undefined value for type {inspect(type_)}.")

    # Convert Python list to GraphQL list. If the GraphQLType is a list, but the value
    # is not a list, convert the value using the list's item type.
    if is_list_type(type_):
        type_ = cast(GraphQLList, type_)
        item_type = type_.of_type
        if isinstance(value, Iterable) and not isinstance(value, str):
            maybe_value_nodes = (ast_from_value(item, item_type) for item in value)
            value_nodes = tuple(node for node in maybe_value_nodes if node)
            return ListValueNode(values=value_nodes)
        return ast_from_value(value, item_type)

    # Populate the fields of the input object by creating ASTs from each value in the
    # Python dict according to the fields in the input type.
    if is_input_object_type(type_):
        if value is None or not isinstance(value, Mapping):
            return None
        type_ = cast(GraphQLInputObjectType, type_)
        field_items = (
            (field_name, ast_from_value(value[field_name], field.type))
            for field_name, field in type_.fields.items()
            if field_name in value
        )
        field_nodes = tuple(
            ObjectFieldNode(name=NameNode(value=field_name), value=field_value)
            for field_name, field_value in field_items
            if field_value
        )
        return ObjectValueNode(fields=field_nodes)

    if is_leaf_type(type_):
        # Since value is an internally represented value, it must be serialized to an
        # externally represented value before converting into an AST.
        serialized = type_.serialize(value)  # type: ignore

        # if the serialized value is a string, then we should use the
        # type to determine if it is an enum, an ID or a normal string
        if isinstance(serialized, str):
            # Enum types use Enum literals.
            if is_enum_type(type_):
                return EnumValueNode(value=serialized)

            # ID types can use Int literals.
            if type_ is GraphQLID and _re_integer_string.match(serialized):
                return IntValueNode(value=serialized)

            return StringValueNode(value=serialized)

        # Some custom scalars will serialize to dicts or lists
        # Providing here a default conversion to AST using our best judgment
        # until graphql-js issue #1817 is solved
        # https://github.com/graphql/graphql-js/issues/1817
        return ast_from_serialized_value_untyped(serialized)

    # Not reachable. All possible input types have been considered.
    raise TypeError(f"Unexpected input type: {inspect(type_)}.")


def dsl_gql(
    *operations: "DSLExecutable", **operations_with_name: "DSLExecutable"
) -> DocumentNode:
    r"""Given arguments instances of :class:`DSLExecutable`
    containing GraphQL operations or fragments,
    generate a Document which can be executed later in a
    gql client or a gql session.

    Similar to the :func:`gql.gql` function but instead of parsing a python
    string to describe the request, we are using operations which have been generated
    dynamically using instances of :class:`DSLField`, generated
    by instances of :class:`DSLType` which themselves originated from
    a :class:`DSLSchema` class.

    :param \*operations: the GraphQL operations and fragments
    :type \*operations: DSLQuery, DSLMutation, DSLSubscription, DSLFragment
    :param \**operations_with_name: the GraphQL operations with an operation name
    :type \**operations_with_name: DSLQuery, DSLMutation, DSLSubscription

    :return: a Document which can be later executed or subscribed by a
        :class:`Client <gql.client.Client>`, by an
        :class:`async session <gql.client.AsyncClientSession>` or by a
        :class:`sync session <gql.client.SyncClientSession>`

    :raises TypeError: if an argument is not an instance of :class:`DSLExecutable`
    :raises AttributeError: if a type has not been provided in a :class:`DSLFragment`
    """

    # Concatenate operations without and with name
    all_operations: Tuple["DSLExecutable", ...] = (
        *operations,
        *(operation for operation in operations_with_name.values()),
    )

    # Set the operation name
    for name, operation in operations_with_name.items():
        operation.name = name

    # Check the type
    for operation in all_operations:
        if not isinstance(operation, DSLExecutable):
            raise TypeError(
                "Operations should be instances of DSLExecutable "
                "(DSLQuery, DSLMutation, DSLSubscription or DSLFragment).\n"
                f"Received: {type(operation)}."
            )

    return DocumentNode(
        definitions=[operation.executable_ast for operation in all_operations]
    )


class DSLSchema:
    """The DSLSchema is the root of the DSL code.

    Attributes of the DSLSchema class are generated automatically
    with the `__getattr__` dunder method in order to generate
    instances of :class:`DSLType`
    """

    def __init__(self, schema: GraphQLSchema):
        """Initialize the DSLSchema with the given schema.

        :param schema: a GraphQL Schema provided locally or fetched using
                       an introspection query. Usually `client.schema`
        :type schema: GraphQLSchema

        :raises TypeError: if the argument is not an instance of :class:`GraphQLSchema`
        """

        if not isinstance(schema, GraphQLSchema):
            raise TypeError(
                f"DSLSchema needs a schema as parameter. Received: {type(schema)}"
            )

        self._schema: GraphQLSchema = schema

    def __getattr__(self, name: str) -> "DSLType":

        type_def: Optional[GraphQLNamedType] = self._schema.get_type(name)

        if type_def is None:
            raise AttributeError(f"Type '{name}' not found in the schema!")

        if not isinstance(type_def, (GraphQLObjectType, GraphQLInterfaceType)):
            raise AttributeError(
                f'Type "{name} ({type_def!r})" is not valid as an attribute of'
                " DSLSchema. Only Object types or Interface types are accepted."
            )

        return DSLType(type_def, self)


class DSLSelector(ABC):
    """DSLSelector is an abstract class which defines the
    :meth:`select <gql.dsl.DSLSelector.select>` method to select
    children fields in the query.

    Inherited by
    :class:`DSLRootFieldSelector <gql.dsl.DSLRootFieldSelector>`,
    :class:`DSLFieldSelector <gql.dsl.DSLFieldSelector>`
    :class:`DSLFragmentSelector <gql.dsl.DSLFragmentSelector>`
    """

    selection_set: SelectionSetNode

    def __init__(
        self,
        *fields: "DSLSelectable",
        **fields_with_alias: "DSLSelectableWithAlias",
    ):
        """:meta private:"""
        self.selection_set = SelectionSetNode(selections=())

        if fields or fields_with_alias:
            self.select(*fields, **fields_with_alias)

    @abstractmethod
    def is_valid_field(self, field: "DSLSelectable") -> bool:
        raise NotImplementedError(
            "Any DSLSelector subclass must have a is_valid_field method"
        )  # pragma: no cover

    def select(
        self,
        *fields: "DSLSelectable",
        **fields_with_alias: "DSLSelectableWithAlias",
    ):
        r"""Select the fields which should be added.

        :param \*fields: fields or fragments
        :type \*fields: DSLSelectable
        :param \**fields_with_alias: fields or fragments with alias as key
        :type \**fields_with_alias: DSLSelectable

        :raises TypeError: if an argument is not an instance of :class:`DSLSelectable`
        :raises graphql.error.GraphQLError: if an argument is not a valid field
        """
        # Concatenate fields without and with alias
        added_fields: Tuple["DSLSelectable", ...] = DSLField.get_aliased_fields(
            fields, fields_with_alias
        )

        # Check that each field is valid
        for field in added_fields:
            if not isinstance(field, DSLSelectable):
                raise TypeError(
                    "Fields should be instances of DSLSelectable. "
                    f"Received: {type(field)}"
                )

            if not self.is_valid_field(field):
                raise GraphQLError(f"Invalid field for {self!r}: {field!r}")

        # Get a list of AST Nodes for each added field
        added_selections: Tuple[
            Union[FieldNode, InlineFragmentNode, FragmentSpreadNode], ...
        ] = tuple(field.ast_field for field in added_fields)

        # Update the current selection list with new selections
        self.selection_set.selections = self.selection_set.selections + added_selections

        log.debug(f"Added fields: {added_fields} in {self!r}")


class DSLExecutable(DSLSelector):
    """Interface for the root elements which can be executed
    in the :func:`dsl_gql <gql.dsl.dsl_gql>` function

    Inherited by
    :class:`DSLOperation <gql.dsl.DSLOperation>` and
    :class:`DSLFragment <gql.dsl.DSLFragment>`
    """

    variable_definitions: "DSLVariableDefinitions"
    name: Optional[str]
    selection_set: SelectionSetNode

    @property
    @abstractmethod
    def executable_ast(self):
        """Generates the ast for :func:`dsl_gql <gql.dsl.dsl_gql>`."""
        raise NotImplementedError(
            "Any DSLExecutable subclass must have executable_ast property"
        )  # pragma: no cover

    def __init__(
        self,
        *fields: "DSLSelectable",
        **fields_with_alias: "DSLSelectableWithAlias",
    ):
        r"""Given arguments of type :class:`DSLSelectable` containing GraphQL requests,
        generate an operation which can be converted to a Document
        using the :func:`dsl_gql <gql.dsl.dsl_gql>`.

        The fields arguments should be either be fragments or
        fields of root GraphQL types
        (Query, Mutation or Subscription) and correspond to the
        operation_type of this operation.

        :param \*fields: root fields or fragments
        :type \*fields: DSLSelectable
        :param \**fields_with_alias: root fields or fragments with alias as key
        :type \**fields_with_alias: DSLSelectable

        :raises TypeError: if an argument is not an instance of :class:`DSLSelectable`
        :raises AssertionError: if an argument is not a field which correspond
                                to the operation type
        """

        self.name = None
        self.variable_definitions = DSLVariableDefinitions()

        DSLSelector.__init__(self, *fields, **fields_with_alias)


class DSLRootFieldSelector(DSLSelector):
    """Class used to define the
    :meth:`is_valid_field <gql.dsl.DSLRootFieldSelector.is_valid_field>` method
    for root fields for the :meth:`select <gql.dsl.DSLSelector.select>` method.

    Inherited by
    :class:`DSLOperation <gql.dsl.DSLOperation>`
    """

    def is_valid_field(self, field: "DSLSelectable") -> bool:
        """Check that a field is valid for a root field.

        For operations, the fields arguments should be fields of root GraphQL types
        (Query, Mutation or Subscription) and correspond to the
        operation_type of this operation.

        the :code:`__typename` field can only be added to Query or Mutation.
        the :code:`__schema` and :code:`__type` field can only be added to Query.
        """

        assert isinstance(self, DSLOperation)

        operation_name = self.operation_type.name

        if isinstance(field, DSLMetaField):
            if field.name in ["__schema", "__type"]:
                return operation_name == "QUERY"
            if field.name == "__typename":
                return operation_name != "SUBSCRIPTION"

        elif isinstance(field, DSLField):

            assert field.dsl_type is not None

            schema = field.dsl_type._dsl_schema._schema

            root_type = None

            if operation_name == "QUERY":
                root_type = schema.query_type
            elif operation_name == "MUTATION":
                root_type = schema.mutation_type
            elif operation_name == "SUBSCRIPTION":
                root_type = schema.subscription_type

            if root_type is None:
                log.error(
                    f"Root type of type {operation_name} not found in the schema!"
                )
                return False

            return field.parent_type.name == root_type.name

        return False


class DSLOperation(DSLExecutable, DSLRootFieldSelector):
    """Interface for GraphQL operations.

    Inherited by
    :class:`DSLQuery <gql.dsl.DSLQuery>`,
    :class:`DSLMutation <gql.dsl.DSLMutation>` and
    :class:`DSLSubscription <gql.dsl.DSLSubscription>`
    """

    operation_type: OperationType

    @property
    def executable_ast(self) -> OperationDefinitionNode:
        """Generates the ast for :func:`dsl_gql <gql.dsl.dsl_gql>`."""

        return OperationDefinitionNode(
            operation=OperationType(self.operation_type),
            selection_set=self.selection_set,
            variable_definitions=self.variable_definitions.get_ast_definitions(),
            **({"name": NameNode(value=self.name)} if self.name else {}),
            directives=(),
        )

    def __repr__(self) -> str:
        return f"<{self.__class__.__name__}>"


class DSLQuery(DSLOperation):
    operation_type = OperationType.QUERY


class DSLMutation(DSLOperation):
    operation_type = OperationType.MUTATION


class DSLSubscription(DSLOperation):
    operation_type = OperationType.SUBSCRIPTION


class DSLVariable:
    """The DSLVariable represents a single variable defined in a GraphQL operation

    Instances of this class are generated for you automatically as attributes
    of the :class:`DSLVariableDefinitions`

    The type of the variable is set by the :class:`DSLField` instance that receives it
    in the :meth:`args <gql.dsl.DSLField.args>` method.
    """

    def __init__(self, name: str):
        """:meta private:"""
        self.name = name
        self.ast_variable_type: Optional[TypeNode] = None
        self.ast_variable_name = VariableNode(name=NameNode(value=self.name))
        self.default_value = None
        self.type: Optional[GraphQLInputType] = None

    def to_ast_type(self, type_: GraphQLInputType) -> TypeNode:
        if is_wrapping_type(type_):
            if isinstance(type_, GraphQLList):
                return ListTypeNode(type=self.to_ast_type(type_.of_type))

            elif isinstance(type_, GraphQLNonNull):
                return NonNullTypeNode(type=self.to_ast_type(type_.of_type))

        assert isinstance(
            type_, (GraphQLScalarType, GraphQLEnumType, GraphQLInputObjectType)
        )

        return NamedTypeNode(name=NameNode(value=type_.name))

    def set_type(self, type_: GraphQLInputType) -> "DSLVariable":
        self.type = type_
        self.ast_variable_type = self.to_ast_type(type_)
        return self

    def default(self, default_value: Any) -> "DSLVariable":
        self.default_value = default_value
        return self


class DSLVariableDefinitions:
    """The DSLVariableDefinitions represents variable definitions in a GraphQL operation

    Instances of this class have to be created and set as the `variable_definitions`
    attribute of a DSLOperation instance

    Attributes of the DSLVariableDefinitions class are generated automatically
    with the `__getattr__` dunder method in order to generate
    instances of :class:`DSLVariable`, that can then be used as values
    in the :meth:`args <gql.dsl.DSLField.args>` method.
    """

    def __init__(self):
        """:meta private:"""
        self.variables: Dict[str, DSLVariable] = {}

    def __getattr__(self, name: str) -> "DSLVariable":
        if name not in self.variables:
            self.variables[name] = DSLVariable(name)
        return self.variables[name]

    def get_ast_definitions(self) -> Tuple[VariableDefinitionNode, ...]:
        """
        :meta private:

        Return a list of VariableDefinitionNodes for each variable with a type
        """
        return tuple(
            VariableDefinitionNode(
                type=var.ast_variable_type,
                variable=var.ast_variable_name,
                default_value=None
                if var.default_value is None
                else ast_from_value(var.default_value, var.type),
                directives=(),
            )
            for var in self.variables.values()
            if var.type is not None  # only variables used
        )


class DSLType:
    """The DSLType represents a GraphQL type for the DSL code.

    It can be a root type (Query, Mutation or Subscription).
    Or it can be any other object type (Human in the StarWars schema).
    Or it can be an interface type (Character in the StarWars schema).

    Instances of this class are generated for you automatically as attributes
    of the :class:`DSLSchema`

    Attributes of the DSLType class are generated automatically
    with the `__getattr__` dunder method in order to generate
    instances of :class:`DSLField`
    """

    def __init__(
        self,
        graphql_type: Union[GraphQLObjectType, GraphQLInterfaceType],
        dsl_schema: DSLSchema,
    ):
        """Initialize the DSLType with the GraphQL type.

        .. warning::
            Don't instantiate this class yourself.
            Use attributes of the :class:`DSLSchema` instead.

        :param graphql_type: the GraphQL type definition from the schema
        :param dsl_schema: reference to the DSLSchema which created this type
        """
        self._type: Union[GraphQLObjectType, GraphQLInterfaceType] = graphql_type
        self._dsl_schema = dsl_schema
        log.debug(f"Creating {self!r})")

    def __getattr__(self, name: str) -> "DSLField":
        camel_cased_name = to_camel_case(name)

        if name in self._type.fields:
            formatted_name = name
            field = self._type.fields[name]
        elif camel_cased_name in self._type.fields:
            formatted_name = camel_cased_name
            field = self._type.fields[camel_cased_name]
        else:
            raise AttributeError(
                f"Field {name} does not exist in type {self._type.name}."
            )

        return DSLField(formatted_name, self._type, field, self)

    def __repr__(self) -> str:
        return f"<{self.__class__.__name__} {self._type!r}>"


class DSLSelectable(ABC):
    """DSLSelectable is an abstract class which indicates that
    the subclasses can be used as arguments of the
    :meth:`select <gql.dsl.DSLSelector.select>` method.

    Inherited by
    :class:`DSLField <gql.dsl.DSLField>`,
    :class:`DSLFragment <gql.dsl.DSLFragment>`
    :class:`DSLInlineFragment <gql.dsl.DSLInlineFragment>`
    """

    ast_field: Union[FieldNode, InlineFragmentNode, FragmentSpreadNode]

    @staticmethod
    def get_aliased_fields(
        fields: Iterable["DSLSelectable"],
        fields_with_alias: Dict[str, "DSLSelectableWithAlias"],
    ) -> Tuple["DSLSelectable", ...]:
        """
        :meta private:

        Concatenate all the fields (with or without alias) in a Tuple.

        Set the requested alias for the fields with alias.
        """

        return (
            *fields,
            *(field.alias(alias) for alias, field in fields_with_alias.items()),
        )

    def __str__(self) -> str:
        return print_ast(self.ast_field)


class DSLFragmentSelector(DSLSelector):
    """Class used to define the
    :meth:`is_valid_field <gql.dsl.DSLFragmentSelector.is_valid_field>` method
    for fragments for the :meth:`select <gql.dsl.DSLSelector.select>` method.

    Inherited by
    :class:`DSLFragment <gql.dsl.DSLFragment>`,
    :class:`DSLInlineFragment <gql.dsl.DSLInlineFragment>`
    """

    def is_valid_field(self, field: DSLSelectable) -> bool:
        """Check that a field is valid."""

        assert isinstance(self, (DSLFragment, DSLInlineFragment))

        if isinstance(field, (DSLFragment, DSLInlineFragment)):
            return True

        assert isinstance(field, DSLField)

        if isinstance(field, DSLMetaField):
            return field.name == "__typename"

        fragment_type = self._type

        assert fragment_type is not None

        if field.name in fragment_type.fields.keys():
            return fragment_type.fields[field.name].type == field.field.type

        return False


class DSLFieldSelector(DSLSelector):
    """Class used to define the
    :meth:`is_valid_field <gql.dsl.DSLFieldSelector.is_valid_field>` method
    for fields for the :meth:`select <gql.dsl.DSLSelector.select>` method.

    Inherited by
    :class:`DSLField <gql.dsl.DSLField>`,
    """

    def is_valid_field(self, field: DSLSelectable) -> bool:
        """Check that a field is valid."""

        assert isinstance(self, DSLField)

        if isinstance(field, (DSLFragment, DSLInlineFragment)):
            return True

        assert isinstance(field, DSLField)

        if isinstance(field, DSLMetaField):
            return field.name == "__typename"

        parent_type = get_named_type(self.field.type)

        if not isinstance(parent_type, (GraphQLInterfaceType, GraphQLObjectType)):
            return False

        if field.name in parent_type.fields.keys():
            return parent_type.fields[field.name].type == field.field.type

        return False


class DSLSelectableWithAlias(DSLSelectable):
    """DSLSelectableWithAlias is an abstract class which indicates that
    the subclasses can be selected with an alias.
    """

    ast_field: FieldNode

    def alias(self, alias: str) -> "DSLSelectableWithAlias":
        """Set an alias

        .. note::
            You can also pass the alias directly at the
            :meth:`select <gql.dsl.DSLSelector.select>` method.
            :code:`ds.Query.human.select(my_name=ds.Character.name)` is equivalent to:
            :code:`ds.Query.human.select(ds.Character.name.alias("my_name"))`

        :param alias: the alias
        :type alias: str
        :return: itself
        """

        self.ast_field.alias = NameNode(value=alias)
        return self


class DSLField(DSLSelectableWithAlias, DSLFieldSelector):
    """The DSLField represents a GraphQL field for the DSL code.

    Instances of this class are generated for you automatically as attributes
    of the :class:`DSLType`

    If this field contains children fields, then you need to select which ones
    you want in the request using the :meth:`select <gql.dsl.DSLField.select>`
    method.
    """

    _type: Union[GraphQLObjectType, GraphQLInterfaceType]
    ast_field: FieldNode
    field: GraphQLField

    def __init__(
        self,
        name: str,
        parent_type: Union[GraphQLObjectType, GraphQLInterfaceType],
        field: GraphQLField,
        dsl_type: Optional[DSLType] = None,
    ):
        """Initialize the DSLField.

        .. warning::
            Don't instantiate this class yourself.
            Use attributes of the :class:`DSLType` instead.

        :param name: the name of the field
        :param parent_type: the GraphQL type definition from the schema of the
                            parent type of the field
        :param field: the GraphQL field definition from the schema
        :param dsl_type: reference of the DSLType instance which created this field
        """
        self.parent_type = parent_type
        self.field = field
        self.ast_field = FieldNode(
            name=NameNode(value=name),
            arguments=(),
            directives=(),
        )
        self.dsl_type = dsl_type

        log.debug(f"Creating {self!r}")

        DSLSelector.__init__(self)

    @property
    def name(self):
        """:meta private:"""
        return self.ast_field.name.value

    def __call__(self, **kwargs) -> "DSLField":
        return self.args(**kwargs)

    def args(self, **kwargs) -> "DSLField":
        r"""Set the arguments of a field

        The arguments are parsed to be stored in the AST of this field.

        .. note::
            You can also call the field directly with your arguments.
            :code:`ds.Query.human(id=1000)` is equivalent to:
            :code:`ds.Query.human.args(id=1000)`

        :param \**kwargs: the arguments (keyword=value)
        :return: itself

        :raises KeyError: if any of the provided arguments does not exist
                          for this field.
        """

        assert self.ast_field.arguments is not None

        self.ast_field.arguments = self.ast_field.arguments + tuple(
            ArgumentNode(
                name=NameNode(value=name),
                value=ast_from_value(value, self._get_argument(name).type),
            )
            for name, value in kwargs.items()
        )

        log.debug(f"Added arguments {kwargs} in field {self!r})")

        return self

    def _get_argument(self, name: str) -> GraphQLArgument:
        """Method used to return the GraphQLArgument definition
        of an argument from its name.

        :raises KeyError: if the provided argument does not exist
                          for this field.
        """
        arg = self.field.args.get(name)

        if arg is None:
            raise KeyError(f"Argument {name} does not exist in {self.field}.")

        return arg

    def select(
        self, *fields: "DSLSelectable", **fields_with_alias: "DSLSelectableWithAlias"
    ) -> "DSLField":
        """Calling :meth:`select <gql.dsl.DSLSelector.select>` method with
        corrected typing hints
        """

        super().select(*fields, **fields_with_alias)
        self.ast_field.selection_set = self.selection_set

        return self

    def __repr__(self) -> str:
        return f"<{self.__class__.__name__} {self.parent_type.name}" f"::{self.name}>"


class DSLMetaField(DSLField):
    """DSLMetaField represents a GraphQL meta-field for the DSL code.

    meta-fields are reserved field in the GraphQL type system prefixed with
    "__" two underscores and used for introspection.
    """

    meta_type = GraphQLObjectType(
        "meta_field",
        fields={
            "__typename": GraphQLField(GraphQLString),
            "__schema": GraphQLField(
                cast(GraphQLObjectType, introspection_types["__Schema"])
            ),
            "__type": GraphQLField(
                cast(GraphQLObjectType, introspection_types["__Type"]),
                args={"name": GraphQLArgument(type_=GraphQLNonNull(GraphQLString))},
            ),
        },
    )

    def __init__(self, name: str):
        """Initialize the meta-field.

        :param name: the name between __typename, __schema or __type
        """

        try:
            field = self.meta_type.fields[name]
        except KeyError:
            raise GraphQLError(f'Invalid meta-field "{name}"')

        super().__init__(name, self.meta_type, field)


class DSLInlineFragment(DSLSelectable, DSLFragmentSelector):
    """DSLInlineFragment represents an inline fragment for the DSL code."""

    _type: Union[GraphQLObjectType, GraphQLInterfaceType]
    ast_field: InlineFragmentNode

    def __init__(
        self,
        *fields: "DSLSelectable",
        **fields_with_alias: "DSLSelectableWithAlias",
    ):
        r"""Initialize the DSLInlineFragment.

        :param \*fields: new children fields
        :type \*fields: DSLSelectable (DSLField, DSLFragment or DSLInlineFragment)
        :param \**fields_with_alias: new children fields with alias as key
        :type \**fields_with_alias: DSLField
        """

        log.debug(f"Creating {self!r}")

        self.ast_field = InlineFragmentNode(directives=())

        DSLSelector.__init__(self, *fields, **fields_with_alias)

    def select(
        self, *fields: "DSLSelectable", **fields_with_alias: "DSLSelectableWithAlias"
    ) -> "DSLInlineFragment":
        """Calling :meth:`select <gql.dsl.DSLSelector.select>` method with
        corrected typing hints
        """
        super().select(*fields, **fields_with_alias)
        self.ast_field.selection_set = self.selection_set

        return self

    def on(self, type_condition: DSLType) -> "DSLInlineFragment":
        """Provides the GraphQL type of this inline fragment."""

        self._type = type_condition._type
        self.ast_field.type_condition = NamedTypeNode(
            name=NameNode(value=self._type.name)
        )
        return self

    def __repr__(self) -> str:
        type_info = ""

        try:
            type_info += f" on {self._type.name}"
        except AttributeError:
            pass

        return f"<{self.__class__.__name__}{type_info}>"


class DSLFragment(DSLSelectable, DSLFragmentSelector, DSLExecutable):
    """DSLFragment represents a named GraphQL fragment for the DSL code."""

    _type: Optional[Union[GraphQLObjectType, GraphQLInterfaceType]]
    ast_field: FragmentSpreadNode
    name: str

    def __init__(
        self,
        name: str,
    ):
        r"""Initialize the DSLFragment.

        :param name: the name of the fragment
        :type name: str
        """

        DSLExecutable.__init__(self)

        self.name = name
        self._type = None

        log.debug(f"Creating {self!r}")

    @property  # type: ignore
    def ast_field(self) -> FragmentSpreadNode:  # type: ignore
        """ast_field property will generate a FragmentSpreadNode with the
        provided name.

        Note: We need to ignore the type because of
        `issue #4125 of mypy <https://github.com/python/mypy/issues/4125>`_.
        """

        spread_node = FragmentSpreadNode(directives=())
        spread_node.name = NameNode(value=self.name)

        return spread_node

    def select(
        self, *fields: "DSLSelectable", **fields_with_alias: "DSLSelectableWithAlias"
    ) -> "DSLFragment":
        """Calling :meth:`select <gql.dsl.DSLSelector.select>` method with
        corrected typing hints
        """
        if self._type is None:
            raise AttributeError(
                "Missing type condition. Please use .on(type_condition) method"
            )

        super().select(*fields, **fields_with_alias)

        return self

    def on(self, type_condition: DSLType) -> "DSLFragment":
        """Provides the GraphQL type of this fragment.

        :param type_condition: the provided type
        :type type_condition: DSLType
        """

        self._type = type_condition._type

        return self

    @property
    def executable_ast(self) -> FragmentDefinitionNode:
        """Generates the ast for :func:`dsl_gql <gql.dsl.dsl_gql>`.

        :raises AttributeError: if a type has not been provided
        """
        assert self.name is not None

        if self._type is None:
            raise AttributeError(
                "Missing type condition. Please use .on(type_condition) method"
            )

        fragment_variable_definitions = self.variable_definitions.get_ast_definitions()

        if len(fragment_variable_definitions) == 0:
            """Fragment variable definitions are obsolete and only supported on
            graphql-core if the Parser is initialized with:
            allow_legacy_fragment_variables=True.

            We will not provide variable_definitions instead of providing an empty
            tuple to be coherent with how it works by default on graphql-core.
            """
            variable_definition_kwargs = {}
        else:
            variable_definition_kwargs = {
                "variable_definitions": fragment_variable_definitions
            }

        return FragmentDefinitionNode(
            type_condition=NamedTypeNode(name=NameNode(value=self._type.name)),
            selection_set=self.selection_set,
            **variable_definition_kwargs,
            name=NameNode(value=self.name),
            directives=(),
        )

    def __repr__(self) -> str:
        return f"<{self.__class__.__name__} {self.name!s}>"


================================================
File: gql/gql.py
================================================
from __future__ import annotations

from graphql import DocumentNode, Source, parse


def gql(request_string: str | Source) -> DocumentNode:
    """Given a string containing a GraphQL request, parse it into a Document.

    :param request_string: the GraphQL request as a String
    :type request_string: str | Source
    :return: a Document which can be later executed or subscribed by a
        :class:`Client <gql.client.Client>`, by an
        :class:`async session <gql.client.AsyncClientSession>` or by a
        :class:`sync session <gql.client.SyncClientSession>`

    :raises graphql.error.GraphQLError: if a syntax error is encountered.
    """
    if isinstance(request_string, Source):
        source = request_string
    elif isinstance(request_string, str):
        source = Source(request_string, "GraphQL request")
    else:
        raise TypeError("Request must be passed as a string or Source object.")
    return parse(source)


================================================
File: gql/graphql_request.py
================================================
from dataclasses import dataclass
from typing import Any, Dict, Optional

from graphql import DocumentNode, GraphQLSchema

from .utilities import serialize_variable_values


@dataclass(frozen=True)
class GraphQLRequest:
    """GraphQL Request to be executed."""

    document: DocumentNode
    """GraphQL query as AST Node object."""

    variable_values: Optional[Dict[str, Any]] = None
    """Dictionary of input parameters (Default: None)."""

    operation_name: Optional[str] = None
    """
    Name of the operation that shall be executed.
    Only required in multi-operation documents (Default: None).
    """

    def serialize_variable_values(self, schema: GraphQLSchema) -> "GraphQLRequest":
        assert self.variable_values

        return GraphQLRequest(
            document=self.document,
            variable_values=serialize_variable_values(
                schema=schema,
                document=self.document,
                variable_values=self.variable_values,
                operation_name=self.operation_name,
            ),
            operation_name=self.operation_name,
        )


================================================
File: gql/py.typed
================================================
# Marker file for PEP 561. The gql package uses inline types.


================================================
File: gql/utils.py
================================================
"""Utilities to manipulate several python objects."""

from typing import Any, Dict, List, Tuple, Type


# From this response in Stackoverflow
# http://stackoverflow.com/a/19053800/1072990
def to_camel_case(snake_str):
    components = snake_str.split("_")
    # We capitalize the first letter of each component except the first one
    # with the 'title' method and join them together.
    return components[0] + "".join(x.title() if x else "_" for x in components[1:])


def extract_files(
    variables: Dict, file_classes: Tuple[Type[Any], ...]
) -> Tuple[Dict, Dict]:
    files = {}

    def recurse_extract(path, obj):
        """
        recursively traverse obj, doing a deepcopy, but
        replacing any file-like objects with nulls and
        shunting the originals off to the side.
        """
        nonlocal files
        if isinstance(obj, list):
            nulled_obj = []
            for key, value in enumerate(obj):
                value = recurse_extract(f"{path}.{key}", value)
                nulled_obj.append(value)
            return nulled_obj
        elif isinstance(obj, dict):
            nulled_obj = {}
            for key, value in obj.items():
                value = recurse_extract(f"{path}.{key}", value)
                nulled_obj[key] = value
            return nulled_obj
        elif isinstance(obj, file_classes):
            # extract obj from its parent and put it into files instead.
            files[path] = obj
            return None
        else:
            # base case: pass through unchanged
            return obj

    nulled_variables = recurse_extract("variables", variables)

    return nulled_variables, files


def str_first_element(errors: List) -> str:
    try:
        first_error = errors[0]
    except (KeyError, TypeError):
        first_error = errors

    return str(first_error)


================================================
File: gql/transport/__init__.py
================================================
from .async_transport import AsyncTransport
from .transport import Transport

__all__ = ["AsyncTransport", "Transport"]


================================================
File: gql/transport/aiohttp.py
================================================
import asyncio
import functools
import io
import json
import logging
from ssl import SSLContext
from typing import Any, AsyncGenerator, Callable, Dict, Optional, Tuple, Type, Union

import aiohttp
from aiohttp.client_exceptions import ClientResponseError
from aiohttp.client_reqrep import Fingerprint
from aiohttp.helpers import BasicAuth
from aiohttp.typedefs import LooseCookies, LooseHeaders
from graphql import DocumentNode, ExecutionResult, print_ast
from multidict import CIMultiDictProxy

from ..utils import extract_files
from .appsync_auth import AppSyncAuthentication
from .async_transport import AsyncTransport
from .exceptions import (
    TransportAlreadyConnected,
    TransportClosed,
    TransportProtocolError,
    TransportServerError,
)

log = logging.getLogger(__name__)


class AIOHTTPTransport(AsyncTransport):
    """:ref:`Async Transport <async_transports>` to execute GraphQL queries
    on remote servers with an HTTP connection.

    This transport use the aiohttp library with asyncio.
    """

    file_classes: Tuple[Type[Any], ...] = (
        io.IOBase,
        aiohttp.StreamReader,
        AsyncGenerator,
    )

    def __init__(
        self,
        url: str,
        headers: Optional[LooseHeaders] = None,
        cookies: Optional[LooseCookies] = None,
        auth: Optional[Union[BasicAuth, "AppSyncAuthentication"]] = None,
        ssl: Union[SSLContext, bool, Fingerprint] = False,
        timeout: Optional[int] = None,
        ssl_close_timeout: Optional[Union[int, float]] = 10,
        json_serialize: Callable = json.dumps,
        json_deserialize: Callable = json.loads,
        client_session_args: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Initialize the transport with the given aiohttp parameters.

        :param url: The GraphQL server URL. Example: 'https://server.com:PORT/path'.
        :param headers: Dict of HTTP Headers.
        :param cookies: Dict of HTTP cookies.
        :param auth: BasicAuth object to enable Basic HTTP auth if needed
                     Or Appsync Authentication class
        :param ssl: ssl_context of the connection. Use ssl=False to disable encryption
        :param ssl_close_timeout: Timeout in seconds to wait for the ssl connection
                                  to close properly
        :param json_serialize: Json serializer callable.
                By default json.dumps() function
        :param json_deserialize: Json deserializer callable.
                By default json.loads() function
        :param client_session_args: Dict of extra args passed to
                `aiohttp.ClientSession`_

        .. _aiohttp.ClientSession:
          https://docs.aiohttp.org/en/stable/client_reference.html#aiohttp.ClientSession
        """
        self.url: str = url
        self.headers: Optional[LooseHeaders] = headers
        self.cookies: Optional[LooseCookies] = cookies
        self.auth: Optional[Union[BasicAuth, "AppSyncAuthentication"]] = auth
        self.ssl: Union[SSLContext, bool, Fingerprint] = ssl
        self.timeout: Optional[int] = timeout
        self.ssl_close_timeout: Optional[Union[int, float]] = ssl_close_timeout
        self.client_session_args = client_session_args
        self.session: Optional[aiohttp.ClientSession] = None
        self.response_headers: Optional[CIMultiDictProxy[str]]
        self.json_serialize: Callable = json_serialize
        self.json_deserialize: Callable = json_deserialize

    async def connect(self) -> None:
        """Coroutine which will create an aiohttp ClientSession() as self.session.

        Don't call this coroutine directly on the transport, instead use
        :code:`async with` on the client and this coroutine will be executed
        to create the session.

        Should be cleaned with a call to the close coroutine.
        """

        if self.session is None:

            client_session_args: Dict[str, Any] = {
                "cookies": self.cookies,
                "headers": self.headers,
                "auth": None
                if isinstance(self.auth, AppSyncAuthentication)
                else self.auth,
                "json_serialize": self.json_serialize,
            }

            if self.timeout is not None:
                client_session_args["timeout"] = aiohttp.ClientTimeout(
                    total=self.timeout
                )

            # Adding custom parameters passed from init
            if self.client_session_args:
                client_session_args.update(self.client_session_args)  # type: ignore

            log.debug("Connecting transport")

            self.session = aiohttp.ClientSession(**client_session_args)

        else:
            raise TransportAlreadyConnected("Transport is already connected")

    @staticmethod
    def create_aiohttp_closed_event(session) -> asyncio.Event:
        """Work around aiohttp issue that doesn't properly close transports on exit.

        See https://github.com/aio-libs/aiohttp/issues/1925#issuecomment-639080209

        Returns:
           An event that will be set once all transports have been properly closed.
        """

        ssl_transports = 0
        all_is_lost = asyncio.Event()

        def connection_lost(exc, orig_lost):
            nonlocal ssl_transports

            try:
                orig_lost(exc)
            finally:
                ssl_transports -= 1
                if ssl_transports == 0:
                    all_is_lost.set()

        def eof_received(orig_eof_received):
            try:  # pragma: no cover
                orig_eof_received()
            except AttributeError:  # pragma: no cover
                # It may happen that eof_received() is called after
                # _app_protocol and _transport are set to None.
                pass

        for conn in session.connector._conns.values():
            for handler, _ in conn:
                proto = getattr(handler.transport, "_ssl_protocol", None)
                if proto is None:
                    continue

                ssl_transports += 1
                orig_lost = proto.connection_lost
                orig_eof_received = proto.eof_received

                proto.connection_lost = functools.partial(
                    connection_lost, orig_lost=orig_lost
                )
                proto.eof_received = functools.partial(
                    eof_received, orig_eof_received=orig_eof_received
                )

        if ssl_transports == 0:
            all_is_lost.set()

        return all_is_lost

    async def close(self) -> None:
        """Coroutine which will close the aiohttp session.

        Don't call this coroutine directly on the transport, instead use
        :code:`async with` on the client and this coroutine will be executed
        when you exit the async context manager.
        """
        if self.session is not None:

            log.debug("Closing transport")

            if (
                self.client_session_args
                and self.client_session_args.get("connector_owner") is False
            ):

                log.debug("connector_owner is False -> not closing connector")

            else:
                closed_event = self.create_aiohttp_closed_event(self.session)
                await self.session.close()
                try:
                    await asyncio.wait_for(closed_event.wait(), self.ssl_close_timeout)
                except asyncio.TimeoutError:
                    pass

        self.session = None

    async def execute(
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
        extra_args: Optional[Dict[str, Any]] = None,
        upload_files: bool = False,
    ) -> ExecutionResult:
        """Execute the provided document AST against the configured remote server
        using the current session.
        This uses the aiohttp library to perform a HTTP POST request asynchronously
        to the remote server.

        Don't call this coroutine directly on the transport, instead use
        :code:`execute` on a client or a session.

        :param document: the parsed GraphQL request
        :param variable_values: An optional Dict of variable values
        :param operation_name: An optional Operation name for the request
        :param extra_args: additional arguments to send to the aiohttp post method
        :param upload_files: Set to True if you want to put files in the variable values
        :returns: an ExecutionResult object.
        """

        query_str = print_ast(document)

        payload: Dict[str, Any] = {
            "query": query_str,
        }

        if operation_name:
            payload["operationName"] = operation_name

        if upload_files:

            # If the upload_files flag is set, then we need variable_values
            assert variable_values is not None

            # If we upload files, we will extract the files present in the
            # variable_values dict and replace them by null values
            nulled_variable_values, files = extract_files(
                variables=variable_values,
                file_classes=self.file_classes,
            )

            # Save the nulled variable values in the payload
            payload["variables"] = nulled_variable_values

            # Prepare aiohttp to send multipart-encoded data
            data = aiohttp.FormData()

            # Generate the file map
            # path is nested in a list because the spec allows multiple pointers
            # to the same file. But we don't support that.
            # Will generate something like {"0": ["variables.file"]}
            file_map = {str(i): [path] for i, path in enumerate(files)}

            # Enumerate the file streams
            # Will generate something like {'0': <_io.BufferedReader ...>}
            file_streams = {str(i): files[path] for i, path in enumerate(files)}

            # Add the payload to the operations field
            operations_str = self.json_serialize(payload)
            log.debug("operations %s", operations_str)
            data.add_field(
                "operations", operations_str, content_type="application/json"
            )

            # Add the file map field
            file_map_str = self.json_serialize(file_map)
            log.debug("file_map %s", file_map_str)
            data.add_field("map", file_map_str, content_type="application/json")

            # Add the extracted files as remaining fields
            for k, f in file_streams.items():
                name = getattr(f, "name", k)
                content_type = getattr(f, "content_type", None)

                data.add_field(k, f, filename=name, content_type=content_type)

            post_args: Dict[str, Any] = {"data": data}

        else:
            if variable_values:
                payload["variables"] = variable_values

            if log.isEnabledFor(logging.INFO):
                log.info(">>> %s", self.json_serialize(payload))

            post_args = {"json": payload}

        # Pass post_args to aiohttp post method
        if extra_args:
            post_args.update(extra_args)

        # Add headers for AppSync if requested
        if isinstance(self.auth, AppSyncAuthentication):
            post_args["headers"] = self.auth.get_headers(
                self.json_serialize(payload),
                {"content-type": "application/json"},
            )

        if self.session is None:
            raise TransportClosed("Transport is not connected")

        async with self.session.post(self.url, ssl=self.ssl, **post_args) as resp:

            # Saving latest response headers in the transport
            self.response_headers = resp.headers

            async def raise_response_error(resp: aiohttp.ClientResponse, reason: str):
                # We raise a TransportServerError if the status code is 400 or higher
                # We raise a TransportProtocolError in the other cases

                try:
                    # Raise a ClientResponseError if response status is 400 or higher
                    resp.raise_for_status()
                except ClientResponseError as e:
                    raise TransportServerError(str(e), e.status) from e

                result_text = await resp.text()
                raise TransportProtocolError(
                    f"Server did not return a GraphQL result: "
                    f"{reason}: "
                    f"{result_text}"
                )

            try:
                result = await resp.json(loads=self.json_deserialize, content_type=None)

                if log.isEnabledFor(logging.INFO):
                    result_text = await resp.text()
                    log.info("<<< %s", result_text)

            except Exception:
                await raise_response_error(resp, "Not a JSON answer")

            if result is None:
                await raise_response_error(resp, "Not a JSON answer")

            if "errors" not in result and "data" not in result:
                await raise_response_error(resp, 'No "data" or "errors" keys in answer')

            return ExecutionResult(
                errors=result.get("errors"),
                data=result.get("data"),
                extensions=result.get("extensions"),
            )

    def subscribe(
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
    ) -> AsyncGenerator[ExecutionResult, None]:
        """Subscribe is not supported on HTTP.

        :meta private:
        """
        raise NotImplementedError(" The HTTP transport does not support subscriptions")


================================================
File: gql/transport/aiohttp_websockets.py
================================================
import asyncio
import json
import logging
import warnings
from contextlib import suppress
from ssl import SSLContext
from typing import (
    Any,
    AsyncGenerator,
    Collection,
    Dict,
    Literal,
    Mapping,
    Optional,
    Tuple,
    Union,
)

import aiohttp
from aiohttp import BasicAuth, Fingerprint, WSMsgType
from aiohttp.typedefs import LooseHeaders, StrOrURL
from graphql import DocumentNode, ExecutionResult, print_ast
from multidict import CIMultiDictProxy

from gql.transport.aiohttp import AIOHTTPTransport
from gql.transport.async_transport import AsyncTransport
from gql.transport.exceptions import (
    TransportAlreadyConnected,
    TransportClosed,
    TransportProtocolError,
    TransportQueryError,
    TransportServerError,
)

log = logging.getLogger("gql.transport.aiohttp_websockets")

ParsedAnswer = Tuple[str, Optional[ExecutionResult]]


class ListenerQueue:
    """Special queue used for each query waiting for server answers

    If the server is stopped while the listener is still waiting,
    Then we send an exception to the queue and this exception will be raised
    to the consumer once all the previous messages have been consumed from the queue
    """

    def __init__(self, query_id: int, send_stop: bool) -> None:
        self.query_id: int = query_id
        self.send_stop: bool = send_stop
        self._queue: asyncio.Queue = asyncio.Queue()
        self._closed: bool = False

    async def get(self) -> ParsedAnswer:

        try:
            item = self._queue.get_nowait()
        except asyncio.QueueEmpty:
            item = await self._queue.get()

        self._queue.task_done()

        # If we receive an exception when reading the queue, we raise it
        if isinstance(item, Exception):
            self._closed = True
            raise item

        # Don't need to save new answers or
        # send the stop message if we already received the complete message
        answer_type, execution_result = item
        if answer_type == "complete":
            self.send_stop = False
            self._closed = True

        return item

    async def put(self, item: ParsedAnswer) -> None:

        if not self._closed:
            await self._queue.put(item)

    async def set_exception(self, exception: Exception) -> None:

        # Put the exception in the queue
        await self._queue.put(exception)

        # Don't need to send stop messages in case of error
        self.send_stop = False
        self._closed = True


class AIOHTTPWebsocketsTransport(AsyncTransport):

    # This transport supports two subprotocols and will autodetect the
    # subprotocol supported on the server
    APOLLO_SUBPROTOCOL: str = "graphql-ws"
    GRAPHQLWS_SUBPROTOCOL: str = "graphql-transport-ws"

    def __init__(
        self,
        url: StrOrURL,
        *,
        subprotocols: Optional[Collection[str]] = None,
        heartbeat: Optional[float] = None,
        auth: Optional[BasicAuth] = None,
        origin: Optional[str] = None,
        params: Optional[Mapping[str, str]] = None,
        headers: Optional[LooseHeaders] = None,
        proxy: Optional[StrOrURL] = None,
        proxy_auth: Optional[BasicAuth] = None,
        proxy_headers: Optional[LooseHeaders] = None,
        ssl: Optional[Union[SSLContext, Literal[False], Fingerprint]] = None,
        websocket_close_timeout: float = 10.0,
        receive_timeout: Optional[float] = None,
        ssl_close_timeout: Optional[Union[int, float]] = 10,
        connect_timeout: Optional[Union[int, float]] = 10,
        close_timeout: Optional[Union[int, float]] = 10,
        ack_timeout: Optional[Union[int, float]] = 10,
        keep_alive_timeout: Optional[Union[int, float]] = None,
        init_payload: Dict[str, Any] = {},
        ping_interval: Optional[Union[int, float]] = None,
        pong_timeout: Optional[Union[int, float]] = None,
        answer_pings: bool = True,
        client_session_args: Optional[Dict[str, Any]] = None,
        connect_args: Dict[str, Any] = {},
    ) -> None:
        """Initialize the transport with the given parameters.

        :param url: The GraphQL server URL. Example: 'wss://server.com:PORT/graphql'.
        :param subprotocols: list of subprotocols sent to the
            backend in the 'subprotocols' http header.
            By default: both apollo and graphql-ws subprotocols.
        :param float heartbeat: Send low level `ping` message every `heartbeat`
                                seconds and wait `pong` response, close
                                connection if `pong` response is not
                                received. The timer is reset on any data reception.
        :param auth: An object that represents HTTP Basic Authorization.
                     :class:`~aiohttp.BasicAuth` (optional)
        :param str origin: Origin header to send to server(optional)
        :param params: Mapping, iterable of tuple of *key*/*value* pairs or
                       string to be sent as parameters in the query
                       string of the new request. Ignored for subsequent
                       redirected requests (optional)

                       Allowed values are:

                       - :class:`collections.abc.Mapping` e.g. :class:`dict`,
                         :class:`multidict.MultiDict` or
                         :class:`multidict.MultiDictProxy`
                       - :class:`collections.abc.Iterable` e.g. :class:`tuple` or
                         :class:`list`
                       - :class:`str` with preferably url-encoded content
                         (**Warning:** content will not be encoded by *aiohttp*)
        :param headers: HTTP Headers that sent with every request
                        May be either *iterable of key-value pairs* or
                        :class:`~collections.abc.Mapping`
                        (e.g. :class:`dict`,
                        :class:`~multidict.CIMultiDict`).
        :param proxy: Proxy URL, :class:`str` or :class:`~yarl.URL` (optional)
        :param aiohttp.BasicAuth proxy_auth: an object that represents proxy HTTP
                                             Basic Authorization (optional)
        :param ssl: SSL validation mode. ``True`` for default SSL check
                      (:func:`ssl.create_default_context` is used),
                      ``False`` for skip SSL certificate validation,
                      :class:`aiohttp.Fingerprint` for fingerprint
                      validation, :class:`ssl.SSLContext` for custom SSL
                      certificate validation.
        :param float websocket_close_timeout: Timeout for websocket to close.
                                              ``10`` seconds by default
        :param float receive_timeout: Timeout for websocket to receive
                                      complete message.  ``None`` (unlimited)
                                      seconds by default
        :param ssl_close_timeout: Timeout in seconds to wait for the ssl connection
                                  to close properly
        :param connect_timeout: Timeout in seconds for the establishment
            of the websocket connection. If None is provided this will wait forever.
        :param close_timeout: Timeout in seconds for the close. If None is provided
            this will wait forever.
        :param ack_timeout: Timeout in seconds to wait for the connection_ack message
            from the server. If None is provided this will wait forever.
        :param keep_alive_timeout: Optional Timeout in seconds to receive
            a sign of liveness from the server.
        :param init_payload: Dict of the payload sent in the connection_init message.
        :param ping_interval: Delay in seconds between pings sent by the client to
            the backend for the graphql-ws protocol. None (by default) means that
            we don't send pings. Note: there are also pings sent by the underlying
            websockets protocol. See the
            :ref:`keepalive documentation <websockets_transport_keepalives>`
            for more information about this.
        :param pong_timeout: Delay in seconds to receive a pong from the backend
            after we sent a ping (only for the graphql-ws protocol).
            By default equal to half of the ping_interval.
        :param answer_pings: Whether the client answers the pings from the backend
            (for the graphql-ws protocol).
            By default: True
        :param client_session_args: Dict of extra args passed to
                `aiohttp.ClientSession`_
        :param connect_args: Dict of extra args passed to
                `aiohttp.ClientSession.ws_connect`_

        .. _aiohttp.ClientSession.ws_connect:
          https://docs.aiohttp.org/en/stable/client_reference.html#aiohttp.ClientSession.ws_connect
        .. _aiohttp.ClientSession:
          https://docs.aiohttp.org/en/stable/client_reference.html#aiohttp.ClientSession
        """
        self.url: StrOrURL = url
        self.heartbeat: Optional[float] = heartbeat
        self.auth: Optional[BasicAuth] = auth
        self.origin: Optional[str] = origin
        self.params: Optional[Mapping[str, str]] = params
        self.headers: Optional[LooseHeaders] = headers

        self.proxy: Optional[StrOrURL] = proxy
        self.proxy_auth: Optional[BasicAuth] = proxy_auth
        self.proxy_headers: Optional[LooseHeaders] = proxy_headers

        self.ssl: Optional[Union[SSLContext, Literal[False], Fingerprint]] = ssl

        self.websocket_close_timeout: float = websocket_close_timeout
        self.receive_timeout: Optional[float] = receive_timeout

        self.ssl_close_timeout: Optional[Union[int, float]] = ssl_close_timeout
        self.connect_timeout: Optional[Union[int, float]] = connect_timeout
        self.close_timeout: Optional[Union[int, float]] = close_timeout
        self.ack_timeout: Optional[Union[int, float]] = ack_timeout
        self.keep_alive_timeout: Optional[Union[int, float]] = keep_alive_timeout

        self.init_payload: Dict[str, Any] = init_payload

        # We need to set an event loop here if there is none
        # Or else we will not be able to create an asyncio.Event()
        try:
            with warnings.catch_warnings():
                warnings.filterwarnings(
                    "ignore", message="There is no current event loop"
                )
                self._loop = asyncio.get_event_loop()
        except RuntimeError:
            self._loop = asyncio.new_event_loop()
            asyncio.set_event_loop(self._loop)

        self._next_keep_alive_message: asyncio.Event = asyncio.Event()
        self._next_keep_alive_message.set()

        self.session: Optional[aiohttp.ClientSession] = None
        self.websocket: Optional[aiohttp.ClientWebSocketResponse] = None
        self.next_query_id: int = 1
        self.listeners: Dict[int, ListenerQueue] = {}
        self._connecting: bool = False
        self.response_headers: Optional[CIMultiDictProxy[str]] = None

        self.receive_data_task: Optional[asyncio.Future] = None
        self.check_keep_alive_task: Optional[asyncio.Future] = None
        self.close_task: Optional[asyncio.Future] = None

        self._wait_closed: asyncio.Event = asyncio.Event()
        self._wait_closed.set()

        self._no_more_listeners: asyncio.Event = asyncio.Event()
        self._no_more_listeners.set()

        self.payloads: Dict[str, Any] = {}

        self.ping_interval: Optional[Union[int, float]] = ping_interval
        self.pong_timeout: Optional[Union[int, float]]
        self.answer_pings: bool = answer_pings

        if ping_interval is not None:
            if pong_timeout is None:
                self.pong_timeout = ping_interval / 2
            else:
                self.pong_timeout = pong_timeout

        self.send_ping_task: Optional[asyncio.Future] = None

        self.ping_received: asyncio.Event = asyncio.Event()
        """ping_received is an asyncio Event which will fire  each time
        a ping is received with the graphql-ws protocol"""

        self.pong_received: asyncio.Event = asyncio.Event()
        """pong_received is an asyncio Event which will fire  each time
        a pong is received with the graphql-ws protocol"""

        self.supported_subprotocols: Collection[str] = subprotocols or (
            self.APOLLO_SUBPROTOCOL,
            self.GRAPHQLWS_SUBPROTOCOL,
        )

        self.close_exception: Optional[Exception] = None

        self.client_session_args = client_session_args
        self.connect_args = connect_args

    def _parse_answer_graphqlws(
        self, answer: Dict[str, Any]
    ) -> Tuple[str, Optional[int], Optional[ExecutionResult]]:
        """Parse the answer received from the server if the server supports the
        graphql-ws protocol.

        Returns a list consisting of:
            - the answer_type (between:
              'connection_ack', 'ping', 'pong', 'data', 'error', 'complete')
            - the answer id (Integer) if received or None
            - an execution Result if the answer_type is 'data' or None

        Differences with the apollo websockets protocol (superclass):
            - the "data" message is now called "next"
            - the "stop" message is now called "complete"
            - there is no connection_terminate or connection_error messages
            - instead of a unidirectional keep-alive (ka) message from server to client,
              there is now the possibility to send bidirectional ping/pong messages
            - connection_ack has an optional payload
            - the 'error' answer type returns a list of errors instead of a single error
        """

        answer_type: str = ""
        answer_id: Optional[int] = None
        execution_result: Optional[ExecutionResult] = None

        try:
            answer_type = str(answer.get("type"))

            if answer_type in ["next", "error", "complete"]:
                answer_id = int(str(answer.get("id")))

                if answer_type == "next" or answer_type == "error":

                    payload = answer.get("payload")

                    if answer_type == "next":

                        if not isinstance(payload, dict):
                            raise ValueError("payload is not a dict")

                        if "errors" not in payload and "data" not in payload:
                            raise ValueError(
                                "payload does not contain 'data' or 'errors' fields"
                            )

                        execution_result = ExecutionResult(
                            errors=payload.get("errors"),
                            data=payload.get("data"),
                            extensions=payload.get("extensions"),
                        )

                        # Saving answer_type as 'data' to be understood with superclass
                        answer_type = "data"

                    elif answer_type == "error":

                        if not isinstance(payload, list):
                            raise ValueError("payload is not a list")

                        raise TransportQueryError(
                            str(payload[0]), query_id=answer_id, errors=payload
                        )

            elif answer_type in ["ping", "pong", "connection_ack"]:
                self.payloads[answer_type] = answer.get("payload", None)

            else:
                raise ValueError

            if self.check_keep_alive_task is not None:
                self._next_keep_alive_message.set()

        except ValueError as e:
            raise TransportProtocolError(
                f"Server did not return a GraphQL result: {answer}"
            ) from e

        return answer_type, answer_id, execution_result

    def _parse_answer_apollo(
        self, answer: Dict[str, Any]
    ) -> Tuple[str, Optional[int], Optional[ExecutionResult]]:
        """Parse the answer received from the server if the server supports the
        apollo websockets protocol.

        Returns a list consisting of:
            - the answer_type (between:
              'connection_ack', 'ka', 'connection_error', 'data', 'error', 'complete')
            - the answer id (Integer) if received or None
            - an execution Result if the answer_type is 'data' or None
        """

        answer_type: str = ""
        answer_id: Optional[int] = None
        execution_result: Optional[ExecutionResult] = None

        try:
            answer_type = str(answer.get("type"))

            if answer_type in ["data", "error", "complete"]:
                answer_id = int(str(answer.get("id")))

                if answer_type == "data" or answer_type == "error":

                    payload = answer.get("payload")

                    if not isinstance(payload, dict):
                        raise ValueError("payload is not a dict")

                    if answer_type == "data":

                        if "errors" not in payload and "data" not in payload:
                            raise ValueError(
                                "payload does not contain 'data' or 'errors' fields"
                            )

                        execution_result = ExecutionResult(
                            errors=payload.get("errors"),
                            data=payload.get("data"),
                            extensions=payload.get("extensions"),
                        )

                    elif answer_type == "error":

                        raise TransportQueryError(
                            str(payload), query_id=answer_id, errors=[payload]
                        )

            elif answer_type == "ka":
                # Keep-alive message
                if self.check_keep_alive_task is not None:
                    self._next_keep_alive_message.set()
            elif answer_type == "connection_ack":
                pass
            elif answer_type == "connection_error":
                error_payload = answer.get("payload")
                raise TransportServerError(f"Server error: '{repr(error_payload)}'")
            else:
                raise ValueError

        except ValueError as e:
            raise TransportProtocolError(
                f"Server did not return a GraphQL result: {answer}"
            ) from e

        return answer_type, answer_id, execution_result

    def _parse_answer(
        self, answer: str
    ) -> Tuple[str, Optional[int], Optional[ExecutionResult]]:
        """Parse the answer received from the server depending on
        the detected subprotocol.
        """
        try:
            json_answer = json.loads(answer)
        except ValueError:
            raise TransportProtocolError(
                f"Server did not return a GraphQL result: {answer}"
            )

        if self.subprotocol == self.GRAPHQLWS_SUBPROTOCOL:
            return self._parse_answer_graphqlws(json_answer)

        return self._parse_answer_apollo(json_answer)

    async def _wait_ack(self) -> None:
        """Wait for the connection_ack message. Keep alive messages are ignored"""

        while True:
            init_answer = await self._receive()

            answer_type, _, _ = self._parse_answer(init_answer)

            if answer_type == "connection_ack":
                return

            if answer_type != "ka":
                raise TransportProtocolError(
                    "Websocket server did not return a connection ack"
                )

    async def _send_init_message_and_wait_ack(self) -> None:
        """Send init message to the provided websocket and wait for the connection ACK.

        If the answer is not a connection_ack message, we will return an Exception.
        """

        init_message = {"type": "connection_init", "payload": self.init_payload}

        await self._send(init_message)

        # Wait for the connection_ack message or raise a TimeoutError
        await asyncio.wait_for(self._wait_ack(), self.ack_timeout)

    async def _initialize(self):
        """Hook to send the initialization messages after the connection
        and potentially wait for the backend ack.
        """
        await self._send_init_message_and_wait_ack()

    async def _stop_listener(self, query_id: int):
        """Hook to stop to listen to a specific query.
        Will send a stop message in some subclasses.
        """
        log.debug(f"stop listener {query_id}")

        if self.subprotocol == self.GRAPHQLWS_SUBPROTOCOL:
            await self._send_complete_message(query_id)
            await self.listeners[query_id].put(("complete", None))
        else:
            await self._send_stop_message(query_id)

    async def _after_connect(self):
        """Hook to add custom code for subclasses after the connection
        has been established.
        """
        # Find the backend subprotocol returned in the response headers
        response_headers = self.websocket._response.headers
        log.debug(f"Response headers: {response_headers!r}")
        try:
            self.subprotocol = response_headers["Sec-WebSocket-Protocol"]
        except KeyError:
            self.subprotocol = self.APOLLO_SUBPROTOCOL

        log.debug(f"backend subprotocol returned: {self.subprotocol!r}")

    async def send_ping(self, payload: Optional[Any] = None) -> None:
        """Send a ping message for the graphql-ws protocol"""

        ping_message = {"type": "ping"}

        if payload is not None:
            ping_message["payload"] = payload

        await self._send(ping_message)

    async def send_pong(self, payload: Optional[Any] = None) -> None:
        """Send a pong message for the graphql-ws protocol"""

        pong_message = {"type": "pong"}

        if payload is not None:
            pong_message["payload"] = payload

        await self._send(pong_message)

    async def _send_stop_message(self, query_id: int) -> None:
        """Send stop message to the provided websocket connection and query_id.

        The server should afterwards return a 'complete' message.
        """

        stop_message = {"id": str(query_id), "type": "stop"}

        await self._send(stop_message)

    async def _send_complete_message(self, query_id: int) -> None:
        """Send a complete message for the provided query_id.

        This is only for the graphql-ws protocol.
        """

        complete_message = {"id": str(query_id), "type": "complete"}

        await self._send(complete_message)

    async def _send_ping_coro(self) -> None:
        """Coroutine to periodically send a ping from the client to the backend.

        Only used for the graphql-ws protocol.

        Send a ping every ping_interval seconds.
        Close the connection if a pong is not received within pong_timeout seconds.
        """

        assert self.ping_interval is not None

        try:
            while True:
                await asyncio.sleep(self.ping_interval)

                await self.send_ping()

                await asyncio.wait_for(self.pong_received.wait(), self.pong_timeout)

                # Reset for the next iteration
                self.pong_received.clear()

        except asyncio.TimeoutError:
            # No pong received in the appriopriate time, close with error
            # If the timeout happens during a close already in progress, do nothing
            if self.close_task is None:
                await self._fail(
                    TransportServerError(
                        f"No pong received after {self.pong_timeout!r} seconds"
                    ),
                    clean_close=False,
                )

    async def _after_initialize(self):
        """Hook to add custom code for subclasses after the initialization
        has been done.
        """

        # If requested, create a task to send periodic pings to the backend
        if (
            self.subprotocol == self.GRAPHQLWS_SUBPROTOCOL
            and self.ping_interval is not None
        ):

            self.send_ping_task = asyncio.ensure_future(self._send_ping_coro())

    async def _close_hook(self):
        """Hook to add custom code for subclasses for the connection close"""
        # Properly shut down the send ping task if enabled
        if self.send_ping_task is not None:
            self.send_ping_task.cancel()
            with suppress(asyncio.CancelledError):
                await self.send_ping_task
            self.send_ping_task = None

    async def _connection_terminate(self):
        """Hook to add custom code for subclasses after the initialization
        has been done.
        """
        if self.subprotocol == self.APOLLO_SUBPROTOCOL:
            await self._send_connection_terminate_message()

    async def _send_connection_terminate_message(self) -> None:
        """Send a connection_terminate message to the provided websocket connection.

        This message indicates that the connection will disconnect.
        """

        connection_terminate_message = {"type": "connection_terminate"}

        await self._send(connection_terminate_message)

    async def _send_query(
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
    ) -> int:
        """Send a query to the provided websocket connection.

        We use an incremented id to reference the query.

        Returns the used id for this query.
        """

        query_id = self.next_query_id
        self.next_query_id += 1

        payload: Dict[str, Any] = {"query": print_ast(document)}
        if variable_values:
            payload["variables"] = variable_values
        if operation_name:
            payload["operationName"] = operation_name

        query_type = "start"

        if self.subprotocol == self.GRAPHQLWS_SUBPROTOCOL:
            query_type = "subscribe"

        query = {"id": str(query_id), "type": query_type, "payload": payload}

        await self._send(query)

        return query_id

    async def _send(self, message: Dict[str, Any]) -> None:
        """Send the provided message to the websocket connection and log the message"""

        if self.websocket is None:
            raise TransportClosed("WebSocket connection is closed")

        try:
            await self.websocket.send_json(message)
            log.info(">>> %s", message)
        except ConnectionResetError as e:
            await self._fail(e, clean_close=False)
            raise e

    async def _receive(self) -> str:
        """Wait the next message from the websocket connection and log the answer"""

        # It is possible that the websocket has been already closed in another task
        if self.websocket is None:
            raise TransportClosed("Transport is already closed")

        while True:
            ws_message = await self.websocket.receive()

            # Ignore low-level ping and pong received
            if ws_message.type not in (WSMsgType.PING, WSMsgType.PONG):
                break

        if ws_message.type in (
            WSMsgType.CLOSE,
            WSMsgType.CLOSED,
            WSMsgType.CLOSING,
            WSMsgType.ERROR,
        ):
            raise ConnectionResetError
        elif ws_message.type is WSMsgType.BINARY:
            raise TransportProtocolError("Binary data received in the websocket")

        assert ws_message.type is WSMsgType.TEXT

        answer: str = ws_message.data

        log.info("<<< %s", answer)

        return answer

    def _remove_listener(self, query_id) -> None:
        """After exiting from a subscription, remove the listener and
        signal an event if this was the last listener for the client.
        """
        if query_id in self.listeners:
            del self.listeners[query_id]

        remaining = len(self.listeners)
        log.debug(f"listener {query_id} deleted, {remaining} remaining")

        if remaining == 0:
            self._no_more_listeners.set()

    async def _check_ws_liveness(self) -> None:
        """Coroutine which will periodically check the liveness of the connection
        through keep-alive messages
        """

        try:
            while True:
                await asyncio.wait_for(
                    self._next_keep_alive_message.wait(), self.keep_alive_timeout
                )

                # Reset for the next iteration
                self._next_keep_alive_message.clear()

        except asyncio.TimeoutError:
            # No keep-alive message in the appriopriate interval, close with error
            # while trying to notify the server of a proper close (in case
            # the keep-alive interval of the client or server was not aligned
            # the connection still remains)

            # If the timeout happens during a close already in progress, do nothing
            if self.close_task is None:
                await self._fail(
                    TransportServerError(
                        "No keep-alive message has been received within "
                        "the expected interval ('keep_alive_timeout' parameter)"
                    ),
                    clean_close=False,
                )

        except asyncio.CancelledError:
            # The client is probably closing, handle it properly
            pass

    async def _handle_answer(
        self,
        answer_type: str,
        answer_id: Optional[int],
        execution_result: Optional[ExecutionResult],
    ) -> None:

        try:
            # Put the answer in the queue
            if answer_id is not None:
                await self.listeners[answer_id].put((answer_type, execution_result))
        except KeyError:
            # Do nothing if no one is listening to this query_id.
            pass

        # Answer pong to ping for graphql-ws protocol
        if answer_type == "ping":
            self.ping_received.set()
            if self.answer_pings:
                await self.send_pong()

        elif answer_type == "pong":
            self.pong_received.set()

    async def _receive_data_loop(self) -> None:
        """Main asyncio task which will listen to the incoming messages and will
        call the parse_answer and handle_answer methods of the subclass."""
        log.debug("Entering _receive_data_loop()")

        try:
            while True:

                # Wait the next answer from the websocket server
                try:
                    answer = await self._receive()
                except (ConnectionResetError, TransportProtocolError) as e:
                    await self._fail(e, clean_close=False)
                    break
                except TransportClosed as e:
                    await self._fail(e, clean_close=False)
                    raise e

                # Parse the answer
                try:
                    answer_type, answer_id, execution_result = self._parse_answer(
                        answer
                    )
                except TransportQueryError as e:
                    # Received an exception for a specific query
                    # ==> Add an exception to this query queue
                    # The exception is raised for this specific query,
                    # but the transport is not closed.
                    assert isinstance(
                        e.query_id, int
                    ), "TransportQueryError should have a query_id defined here"
                    try:
                        await self.listeners[e.query_id].set_exception(e)
                    except KeyError:
                        # Do nothing if no one is listening to this query_id
                        pass

                    continue

                except (TransportServerError, TransportProtocolError) as e:
                    # Received a global exception for this transport
                    # ==> close the transport
                    # The exception will be raised for all current queries.
                    await self._fail(e, clean_close=False)
                    break

                await self._handle_answer(answer_type, answer_id, execution_result)

        finally:
            log.debug("Exiting _receive_data_loop()")

    async def connect(self) -> None:
        log.debug("connect: starting")

        if self.session is None:
            client_session_args: Dict[str, Any] = {}

            # Adding custom parameters passed from init
            if self.client_session_args:
                client_session_args.update(self.client_session_args)  # type: ignore

            self.session = aiohttp.ClientSession(**client_session_args)

        if self.websocket is None and not self._connecting:
            self._connecting = True

            connect_args: Dict[str, Any] = {
                "url": self.url,
                "headers": self.headers,
                "auth": self.auth,
                "heartbeat": self.heartbeat,
                "origin": self.origin,
                "params": self.params,
                "protocols": self.supported_subprotocols,
                "proxy": self.proxy,
                "proxy_auth": self.proxy_auth,
                "proxy_headers": self.proxy_headers,
                "timeout": self.websocket_close_timeout,
                "receive_timeout": self.receive_timeout,
            }

            if self.ssl is not None:
                connect_args.update(
                    {
                        "ssl": self.ssl,
                    }
                )

            # Adding custom parameters passed from init
            if self.connect_args:
                connect_args.update(self.connect_args)

            try:
                # Connection to the specified url
                # Generate a TimeoutError if taking more than connect_timeout seconds
                # Set the _connecting flag to False after in all cases
                self.websocket = await asyncio.wait_for(
                    self.session.ws_connect(
                        **connect_args,
                    ),
                    self.connect_timeout,
                )
            finally:
                self._connecting = False

            self.response_headers = self.websocket._response.headers

            await self._after_connect()

            self.next_query_id = 1
            self.close_exception = None
            self._wait_closed.clear()

            # Send the init message and wait for the ack from the server
            # Note: This should generate a TimeoutError
            # if no ACKs are received within the ack_timeout
            try:
                await self._initialize()
            except ConnectionResetError as e:
                raise e
            except (
                TransportProtocolError,
                TransportServerError,
                asyncio.TimeoutError,
            ) as e:
                await self._fail(e, clean_close=False)
                raise e

            # Run the after_init hook of the subclass
            await self._after_initialize()

            # If specified, create a task to check liveness of the connection
            # through keep-alive messages
            if self.keep_alive_timeout is not None:
                self.check_keep_alive_task = asyncio.ensure_future(
                    self._check_ws_liveness()
                )

            # Create a task to listen to the incoming websocket messages
            self.receive_data_task = asyncio.ensure_future(self._receive_data_loop())

        else:
            raise TransportAlreadyConnected("Transport is already connected")

        log.debug("connect: done")

    async def _clean_close(self) -> None:
        """Coroutine which will:

        - send stop messages for each active subscription to the server
        - send the connection terminate message
        """
        log.debug(f"Listeners: {self.listeners}")

        # Send 'stop' message for all current queries
        for query_id, listener in self.listeners.items():
            print(f"Listener {query_id} send_stop: {listener.send_stop}")

            if listener.send_stop:
                await self._stop_listener(query_id)
                listener.send_stop = False

        # Wait that there is no more listeners (we received 'complete' for all queries)
        try:
            await asyncio.wait_for(self._no_more_listeners.wait(), self.close_timeout)
        except asyncio.TimeoutError:  # pragma: no cover
            log.debug("Timer close_timeout fired")

        # Calling the subclass hook
        await self._connection_terminate()

    async def _close_coro(self, e: Exception, clean_close: bool = True) -> None:
        """Coroutine which will:

        - do a clean_close if possible:
            - send stop messages for each active query to the server
            - send the connection terminate message
        - close the websocket connection
        - send the exception to all the remaining listeners
        """

        log.debug("_close_coro: starting")

        try:

            try:
                # Properly shut down liveness checker if enabled
                if self.check_keep_alive_task is not None:
                    # More info: https://stackoverflow.com/a/43810272/1113207
                    self.check_keep_alive_task.cancel()
                    with suppress(asyncio.CancelledError):
                        await self.check_keep_alive_task
            except Exception as exc:  # pragma: no cover
                log.warning(
                    "_close_coro cancel keep alive task exception: " + repr(exc)
                )

            try:
                # Calling the subclass close hook
                await self._close_hook()
            except Exception as exc:  # pragma: no cover
                log.warning("_close_coro close_hook exception: " + repr(exc))

            # Saving exception to raise it later if trying to use the transport
            # after it has already closed.
            self.close_exception = e

            if clean_close:
                log.debug("_close_coro: starting clean_close")
                try:
                    await self._clean_close()
                except Exception as exc:  # pragma: no cover
                    log.warning("Ignoring exception in _clean_close: " + repr(exc))

            log.debug("_close_coro: sending exception to listeners")

            # Send an exception to all remaining listeners
            for query_id, listener in self.listeners.items():
                await listener.set_exception(e)

            log.debug("_close_coro: close websocket connection")

            try:
                assert self.websocket is not None

                await self.websocket.close()
                self.websocket = None
            except Exception as exc:
                log.warning("_close_coro websocket close exception: " + repr(exc))

            log.debug("_close_coro: close aiohttp session")

            if (
                self.client_session_args
                and self.client_session_args.get("connector_owner") is False
            ):

                log.debug("connector_owner is False -> not closing connector")

            else:
                try:
                    assert self.session is not None

                    closed_event = AIOHTTPTransport.create_aiohttp_closed_event(
                        self.session
                    )
                    await self.session.close()
                    try:
                        await asyncio.wait_for(
                            closed_event.wait(), self.ssl_close_timeout
                        )
                    except asyncio.TimeoutError:
                        pass
                except Exception as exc:  # pragma: no cover
                    log.warning("_close_coro session close exception: " + repr(exc))

            self.session = None

            log.debug("_close_coro: aiohttp session closed")

            try:
                assert self.receive_data_task is not None

                self.receive_data_task.cancel()
                with suppress(asyncio.CancelledError):
                    await self.receive_data_task
            except Exception as exc:  # pragma: no cover
                log.warning(
                    "_close_coro cancel receive data task exception: " + repr(exc)
                )

        except Exception as exc:  # pragma: no cover
            log.warning("Exception catched in _close_coro: " + repr(exc))

        finally:

            log.debug("_close_coro: final cleanup")

            self.websocket = None
            self.close_task = None
            self.check_keep_alive_task = None
            self.receive_data_task = None
            self._wait_closed.set()

        log.debug("_close_coro: exiting")

    async def _fail(self, e: Exception, clean_close: bool = True) -> None:
        log.debug("_fail: starting with exception: " + repr(e))

        if self.close_task is None:

            if self._wait_closed.is_set():
                log.debug("_fail started but transport is already closed")
            else:
                self.close_task = asyncio.shield(
                    asyncio.ensure_future(self._close_coro(e, clean_close=clean_close))
                )
        else:
            log.debug(
                "close_task is not None in _fail. Previous exception is: "
                + repr(self.close_exception)
                + " New exception is: "
                + repr(e)
            )

    async def close(self) -> None:
        log.debug("close: starting")

        await self._fail(TransportClosed("Websocket GraphQL transport closed by user"))
        await self.wait_closed()

        log.debug("close: done")

    async def wait_closed(self) -> None:
        log.debug("wait_close: starting")

        if not self._wait_closed.is_set():
            await self._wait_closed.wait()

        log.debug("wait_close: done")

    async def execute(
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
    ) -> ExecutionResult:
        """Execute the provided document AST against the configured remote server
        using the current session.

        Send a query but close the async generator as soon as we have the first answer.

        The result is sent as an ExecutionResult object.
        """
        first_result = None

        generator = self.subscribe(
            document, variable_values, operation_name, send_stop=False
        )

        async for result in generator:
            first_result = result
            break

        if first_result is None:
            raise TransportQueryError(
                "Query completed without any answer received from the server"
            )

        return first_result

    async def subscribe(
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
        send_stop: Optional[bool] = True,
    ) -> AsyncGenerator[ExecutionResult, None]:
        """Send a query and receive the results using a python async generator.

        The query can be a graphql query, mutation or subscription.

        The results are sent as an ExecutionResult object.
        """

        # Send the query and receive the id
        query_id: int = await self._send_query(
            document, variable_values, operation_name
        )

        # Create a queue to receive the answers for this query_id
        listener = ListenerQueue(query_id, send_stop=(send_stop is True))
        self.listeners[query_id] = listener

        # We will need to wait at close for this query to clean properly
        self._no_more_listeners.clear()

        try:
            # Loop over the received answers
            while True:

                # Wait for the answer from the queue of this query_id
                # This can raise a TransportError or ConnectionClosed exception.
                answer_type, execution_result = await listener.get()

                # If the received answer contains data,
                # Then we will yield the results back as an ExecutionResult object
                if execution_result is not None:
                    yield execution_result

                # If we receive a 'complete' answer from the server,
                # Then we will end this async generator output without errors
                elif answer_type == "complete":
                    log.debug(
                        f"Complete received for query {query_id} --> exit without error"
                    )
                    break

        except (asyncio.CancelledError, GeneratorExit) as e:
            log.debug(f"Exception in subscribe: {e!r}")
            if listener.send_stop:
                await self._stop_listener(query_id)
                listener.send_stop = False

        finally:
            log.debug(f"In subscribe finally for query_id {query_id}")
            self._remove_listener(query_id)


================================================
File: gql/transport/appsync_auth.py
================================================
import json
import logging
import re
from abc import ABC, abstractmethod
from base64 import b64encode
from typing import Any, Callable, Dict, Optional

try:
    import botocore
except ImportError:  # pragma: no cover
    # botocore is only needed for the IAM AppSync authentication method
    pass

log = logging.getLogger("gql.transport.appsync")


class AppSyncAuthentication(ABC):
    """AWS authentication abstract base class

    All AWS authentication class should have a
    :meth:`get_headers <gql.transport.appsync_auth.AppSyncAuthentication.get_headers>`
    method which defines the headers used in the authentication process."""

    def get_auth_url(self, url: str) -> str:
        """
        :return: a url with base64 encoded headers used to establish
                 a websocket connection to the appsync-realtime-api.
        """
        headers = self.get_headers()

        encoded_headers = b64encode(
            json.dumps(headers, separators=(",", ":")).encode()
        ).decode()

        url_base = url.replace("https://", "wss://").replace(
            "appsync-api", "appsync-realtime-api"
        )

        return f"{url_base}?header={encoded_headers}&payload=e30="

    @abstractmethod
    def get_headers(
        self, data: Optional[str] = None, headers: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        raise NotImplementedError()  # pragma: no cover


class AppSyncApiKeyAuthentication(AppSyncAuthentication):
    """AWS authentication class using an API key"""

    def __init__(self, host: str, api_key: str) -> None:
        """
        :param host: the host, something like:
                     XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com
        :param api_key: the API key
        """
        self._host = host.replace("appsync-realtime-api", "appsync-api")
        self.api_key = api_key

    def get_headers(
        self, data: Optional[str] = None, headers: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        return {"host": self._host, "x-api-key": self.api_key}


class AppSyncJWTAuthentication(AppSyncAuthentication):
    """AWS authentication class using a JWT access token.

    It can be used either for:
     - Amazon Cognito user pools
     - OpenID Connect (OIDC)
    """

    def __init__(self, host: str, jwt: str) -> None:
        """
        :param host: the host, something like:
                     XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com
        :param jwt: the JWT Access Token
        """
        self._host = host.replace("appsync-realtime-api", "appsync-api")
        self.jwt = jwt

    def get_headers(
        self, data: Optional[str] = None, headers: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        return {"host": self._host, "Authorization": self.jwt}


class AppSyncIAMAuthentication(AppSyncAuthentication):
    """AWS authentication class using IAM.

    .. note::
        There is no need for you to use this class directly, you could instead
        intantiate :class:`gql.transport.appsync_websockets.AppSyncWebsocketsTransport`
        without an auth argument.

    During initialization, this class will use botocore to attempt to
    find your IAM credentials, either from environment variables or
    from your AWS credentials file.
    """

    def __init__(
        self,
        host: str,
        region_name: Optional[str] = None,
        signer: Optional["botocore.auth.BaseSigner"] = None,
        request_creator: Optional[
            Callable[[Dict[str, Any]], "botocore.awsrequest.AWSRequest"]
        ] = None,
        credentials: Optional["botocore.credentials.Credentials"] = None,
        session: Optional["botocore.session.Session"] = None,
    ) -> None:
        """Initialize itself, saving the found credentials used
        to sign the headers later.

        if no credentials are found, then a NoCredentialsError is raised.
        """

        from botocore.auth import SigV4Auth
        from botocore.awsrequest import create_request_object
        from botocore.session import get_session

        self._host = host.replace("appsync-realtime-api", "appsync-api")
        self._session = session if session else get_session()
        self._credentials = (
            credentials if credentials else self._session.get_credentials()
        )
        self._service_name = "appsync"
        self._region_name = region_name or self._detect_region_name()
        self._signer = (
            signer
            if signer
            else SigV4Auth(self._credentials, self._service_name, self._region_name)
        )
        self._request_creator = (
            request_creator if request_creator else create_request_object
        )

    def _detect_region_name(self):
        """Try to detect the correct region_name.

        First try to extract the region_name from the host.

        If that does not work, then try to get the region_name from
        the aws configuration (~/.aws/config file) or the AWS_DEFAULT_REGION
        environment variable.

        If no region_name was found, then raise a NoRegionError exception."""

        from botocore.exceptions import NoRegionError

        # Regular expression from botocore.utils.validate_region
        m = re.search(
            r"appsync-api\.((?![0-9]+$)(?!-)[a-zA-Z0-9-]{,63}(?<!-))\.", self._host
        )

        if m:
            region_name = m.groups()[0]
            log.debug(f"Region name extracted from host: {region_name}")

        else:
            log.debug("Region name not found in host, trying default region name")
            region_name = self._session._resolve_region_name(
                None, self._session.get_default_client_config()
            )

        if region_name is None:
            log.warning(
                "Region name not found. "
                "It was not possible to detect your region either from the host "
                "or from your default AWS configuration."
            )
            raise NoRegionError

        return region_name

    def get_headers(
        self, data: Optional[str] = None, headers: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:

        from botocore.exceptions import NoCredentialsError

        # Default headers for a websocket connection
        headers = headers or {
            "accept": "application/json, text/javascript",
            "content-encoding": "amz-1.0",
            "content-type": "application/json; charset=UTF-8",
        }

        request: "botocore.awsrequest.AWSRequest" = self._request_creator(
            {
                "method": "POST",
                "url": f"https://{self._host}/graphql{'' if data else '/connect'}",
                "headers": headers,
                "context": {},
                "body": data or "{}",
            }
        )

        try:
            self._signer.add_auth(request)
        except NoCredentialsError:
            log.warning(
                "Credentials not found for the IAM auth. "
                "Do you have default AWS credentials configured?",
            )
            raise

        headers = dict(request.headers)

        headers["host"] = self._host

        if log.isEnabledFor(logging.DEBUG):
            headers_log = []
            headers_log.append("\n\nSigned headers:")
            for key, value in headers.items():
                headers_log.append(f"    {key}: {value}")
            headers_log.append("\n")
            log.debug("\n".join(headers_log))

        return headers


================================================
File: gql/transport/appsync_websockets.py
================================================
import json
import logging
from ssl import SSLContext
from typing import Any, Dict, Optional, Tuple, Union, cast
from urllib.parse import urlparse

from graphql import DocumentNode, ExecutionResult, print_ast

from .appsync_auth import AppSyncAuthentication, AppSyncIAMAuthentication
from .exceptions import TransportProtocolError, TransportServerError
from .websockets import WebsocketsTransport, WebsocketsTransportBase

log = logging.getLogger("gql.transport.appsync")

try:
    import botocore
except ImportError:  # pragma: no cover
    # botocore is only needed for the IAM AppSync authentication method
    pass


class AppSyncWebsocketsTransport(WebsocketsTransportBase):
    """:ref:`Async Transport <async_transports>` used to execute GraphQL subscription on
    AWS appsync realtime endpoint.

    This transport uses asyncio and the websockets library in order to send requests
    on a websocket connection.
    """

    auth: Optional[AppSyncAuthentication]

    def __init__(
        self,
        url: str,
        auth: Optional[AppSyncAuthentication] = None,
        session: Optional["botocore.session.Session"] = None,
        ssl: Union[SSLContext, bool] = False,
        connect_timeout: int = 10,
        close_timeout: int = 10,
        ack_timeout: int = 10,
        keep_alive_timeout: Optional[Union[int, float]] = None,
        connect_args: Dict[str, Any] = {},
    ) -> None:
        """Initialize the transport with the given parameters.

        :param url: The GraphQL endpoint URL. Example:
            https://XXXXXXXXXXXXXXXXXXXXXXXXXX.appsync-api.REGION.amazonaws.com/graphql
        :param auth: Optional AWS authentication class which will provide the
                     necessary headers to be correctly authenticated. If this
                     argument is not provided, then we will try to authenticate
                     using IAM.
        :param ssl: ssl_context of the connection.
        :param connect_timeout: Timeout in seconds for the establishment
            of the websocket connection. If None is provided this will wait forever.
        :param close_timeout: Timeout in seconds for the close. If None is provided
            this will wait forever.
        :param ack_timeout: Timeout in seconds to wait for the connection_ack message
            from the server. If None is provided this will wait forever.
        :param keep_alive_timeout: Optional Timeout in seconds to receive
            a sign of liveness from the server.
        :param connect_args: Other parameters forwarded to websockets.connect
        """

        if not auth:

            # Extract host from url
            host = str(urlparse(url).netloc)

            # May raise NoRegionError or NoCredentialsError or ImportError
            auth = AppSyncIAMAuthentication(host=host, session=session)

        self.auth = auth

        url = self.auth.get_auth_url(url)

        super().__init__(
            url,
            ssl=ssl,
            connect_timeout=connect_timeout,
            close_timeout=close_timeout,
            ack_timeout=ack_timeout,
            keep_alive_timeout=keep_alive_timeout,
            connect_args=connect_args,
        )

        # Using the same 'graphql-ws' protocol as the apollo protocol
        self.supported_subprotocols = [
            WebsocketsTransport.APOLLO_SUBPROTOCOL,
        ]
        self.subprotocol = WebsocketsTransport.APOLLO_SUBPROTOCOL

    def _parse_answer(
        self, answer: str
    ) -> Tuple[str, Optional[int], Optional[ExecutionResult]]:
        """Parse the answer received from the server.

        Difference between apollo protocol and aws protocol:

        - aws protocol can return an error without an id
        - aws protocol will send start_ack messages

        Returns a list consisting of:
            - the answer_type:
              - 'connection_ack',
              - 'connection_error',
              - 'start_ack',
              - 'ka',
              - 'data',
              - 'error',
              - 'complete'
            - the answer id (Integer) if received or None
            - an execution Result if the answer_type is 'data' or None
        """

        answer_type: str = ""

        try:
            json_answer = json.loads(answer)

            answer_type = str(json_answer.get("type"))

            if answer_type == "start_ack":
                return ("start_ack", None, None)

            elif answer_type == "error" and "id" not in json_answer:
                error_payload = json_answer.get("payload")
                raise TransportServerError(f"Server error: '{error_payload!r}'")

            else:

                return WebsocketsTransport._parse_answer_apollo(
                    cast(WebsocketsTransport, self), json_answer
                )

        except ValueError:
            raise TransportProtocolError(
                f"Server did not return a GraphQL result: {answer}"
            )

    async def _send_query(
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
    ) -> int:

        query_id = self.next_query_id

        self.next_query_id += 1

        data: Dict = {"query": print_ast(document)}

        if variable_values:
            data["variables"] = variable_values

        if operation_name:
            data["operationName"] = operation_name

        serialized_data = json.dumps(data, separators=(",", ":"))

        payload = {"data": serialized_data}

        message: Dict = {
            "id": str(query_id),
            "type": "start",
            "payload": payload,
        }

        assert self.auth is not None

        message["payload"]["extensions"] = {
            "authorization": self.auth.get_headers(serialized_data)
        }

        await self._send(
            json.dumps(
                message,
                separators=(",", ":"),
            )
        )

        return query_id

    subscribe = WebsocketsTransportBase.subscribe
    """Send a subscription query and receive the results using
    a python async generator.

    Only subscriptions are supported, queries and mutations are forbidden.

    The results are sent as an ExecutionResult object.
    """

    async def execute(
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
    ) -> ExecutionResult:
        """This method is not available.

        Only subscriptions are supported on the AWS realtime endpoint.

        :raise: AssertionError"""
        raise AssertionError(
            "execute method is not allowed for AppSyncWebsocketsTransport "
            "because only subscriptions are allowed on the realtime endpoint."
        )

    _initialize = WebsocketsTransport._initialize
    _stop_listener = WebsocketsTransport._send_stop_message  # type: ignore
    _send_init_message_and_wait_ack = (
        WebsocketsTransport._send_init_message_and_wait_ack
    )
    _wait_ack = WebsocketsTransport._wait_ack


================================================
File: gql/transport/async_transport.py
================================================
import abc
from typing import Any, AsyncGenerator, Dict, Optional

from graphql import DocumentNode, ExecutionResult


class AsyncTransport(abc.ABC):
    @abc.abstractmethod
    async def connect(self):
        """Coroutine used to create a connection to the specified address"""
        raise NotImplementedError(
            "Any AsyncTransport subclass must implement connect method"
        )  # pragma: no cover

    @abc.abstractmethod
    async def close(self):
        """Coroutine used to Close an established connection"""
        raise NotImplementedError(
            "Any AsyncTransport subclass must implement close method"
        )  # pragma: no cover

    @abc.abstractmethod
    async def execute(
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
    ) -> ExecutionResult:
        """Execute the provided document AST for either a remote or local GraphQL
        Schema."""
        raise NotImplementedError(
            "Any AsyncTransport subclass must implement execute method"
        )  # pragma: no cover

    @abc.abstractmethod
    def subscribe(
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
    ) -> AsyncGenerator[ExecutionResult, None]:
        """Send a query and receive the results using an async generator

        The query can be a graphql query, mutation or subscription

        The results are sent as an ExecutionResult object
        """
        raise NotImplementedError(
            "Any AsyncTransport subclass must implement subscribe method"
        )  # pragma: no cover


================================================
File: gql/transport/exceptions.py
================================================
from typing import Any, List, Optional


class TransportError(Exception):
    """Base class for all the Transport exceptions"""

    pass


class TransportProtocolError(TransportError):
    """Transport protocol error.

    The answer received from the server does not correspond to the transport protocol.
    """


class TransportServerError(TransportError):
    """The server returned a global error.

    This exception will close the transport connection.
    """

    code: Optional[int]

    def __init__(self, message: str, code: Optional[int] = None):
        super().__init__(message)
        self.code = code


class TransportQueryError(TransportError):
    """The server returned an error for a specific query.

    This exception should not close the transport connection.
    """

    query_id: Optional[int]
    errors: Optional[List[Any]]
    data: Optional[Any]
    extensions: Optional[Any]

    def __init__(
        self,
        msg: str,
        query_id: Optional[int] = None,
        errors: Optional[List[Any]] = None,
        data: Optional[Any] = None,
        extensions: Optional[Any] = None,
    ):
        super().__init__(msg)
        self.query_id = query_id
        self.errors = errors
        self.data = data
        self.extensions = extensions


class TransportClosed(TransportError):
    """Transport is already closed.

    This exception is generated when the client is trying to use the transport
    while the transport was previously closed.
    """


class TransportAlreadyConnected(TransportError):
    """Transport is already connected.

    Exception generated when the client is trying to connect to the transport
    while the transport is already connected.
    """


================================================
File: gql/transport/httpx.py
================================================
import io
import json
import logging
from typing import (
    Any,
    AsyncGenerator,
    Callable,
    Dict,
    List,
    Optional,
    Tuple,
    Type,
    Union,
    cast,
)

import httpx
from graphql import DocumentNode, ExecutionResult, print_ast

from ..utils import extract_files
from . import AsyncTransport, Transport
from .exceptions import (
    TransportAlreadyConnected,
    TransportClosed,
    TransportProtocolError,
    TransportServerError,
)

log = logging.getLogger(__name__)


class _HTTPXTransport:
    file_classes: Tuple[Type[Any], ...] = (io.IOBase,)

    response_headers: Optional[httpx.Headers] = None

    def __init__(
        self,
        url: Union[str, httpx.URL],
        json_serialize: Callable = json.dumps,
        json_deserialize: Callable = json.loads,
        **kwargs,
    ):
        """Initialize the transport with the given httpx parameters.

        :param url: The GraphQL server URL. Example: 'https://server.com:PORT/path'.
        :param json_serialize: Json serializer callable.
                By default json.dumps() function.
        :param json_deserialize: Json deserializer callable.
                By default json.loads() function.
        :param kwargs: Extra args passed to the `httpx` client.
        """
        self.url = url
        self.json_serialize = json_serialize
        self.json_deserialize = json_deserialize
        self.kwargs = kwargs

    def _prepare_request(
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
        extra_args: Optional[Dict[str, Any]] = None,
        upload_files: bool = False,
    ) -> Dict[str, Any]:
        query_str = print_ast(document)

        payload: Dict[str, Any] = {
            "query": query_str,
        }

        if operation_name:
            payload["operationName"] = operation_name

        if upload_files:
            # If the upload_files flag is set, then we need variable_values
            assert variable_values is not None

            post_args = self._prepare_file_uploads(variable_values, payload)
        else:
            if variable_values:
                payload["variables"] = variable_values

            post_args = {"json": payload}

        # Log the payload
        if log.isEnabledFor(logging.DEBUG):
            log.debug(">>> %s", self.json_serialize(payload))

        # Pass post_args to httpx post method
        if extra_args:
            post_args.update(extra_args)

        return post_args

    def _prepare_file_uploads(self, variable_values, payload) -> Dict[str, Any]:
        # If we upload files, we will extract the files present in the
        # variable_values dict and replace them by null values
        nulled_variable_values, files = extract_files(
            variables=variable_values,
            file_classes=self.file_classes,
        )

        # Save the nulled variable values in the payload
        payload["variables"] = nulled_variable_values

        # Prepare to send multipart-encoded data
        data: Dict[str, Any] = {}
        file_map: Dict[str, List[str]] = {}
        file_streams: Dict[str, Tuple[str, ...]] = {}

        for i, (path, f) in enumerate(files.items()):
            key = str(i)

            # Generate the file map
            # path is nested in a list because the spec allows multiple pointers
            # to the same file. But we don't support that.
            # Will generate something like {"0": ["variables.file"]}
            file_map[key] = [path]

            # Generate the file streams
            # Will generate something like
            # {"0": ("variables.file", <_io.BufferedReader ...>)}
            name = cast(str, getattr(f, "name", key))
            content_type = getattr(f, "content_type", None)

            if content_type is None:
                file_streams[key] = (name, f)
            else:
                file_streams[key] = (name, f, content_type)

        # Add the payload to the operations field
        operations_str = self.json_serialize(payload)
        log.debug("operations %s", operations_str)
        data["operations"] = operations_str

        # Add the file map field
        file_map_str = self.json_serialize(file_map)
        log.debug("file_map %s", file_map_str)
        data["map"] = file_map_str

        return {"data": data, "files": file_streams}

    def _prepare_result(self, response: httpx.Response) -> ExecutionResult:
        # Save latest response headers in transport
        self.response_headers = response.headers

        if log.isEnabledFor(logging.DEBUG):
            log.debug("<<< %s", response.text)

        try:
            result: Dict[str, Any] = self.json_deserialize(response.content)

        except Exception:
            self._raise_response_error(response, "Not a JSON answer")

        if "errors" not in result and "data" not in result:
            self._raise_response_error(response, 'No "data" or "errors" keys in answer')

        return ExecutionResult(
            errors=result.get("errors"),
            data=result.get("data"),
            extensions=result.get("extensions"),
        )

    def _raise_response_error(self, response: httpx.Response, reason: str):
        # We raise a TransportServerError if the status code is 400 or higher
        # We raise a TransportProtocolError in the other cases

        try:
            # Raise a HTTPError if response status is 400 or higher
            response.raise_for_status()
        except httpx.HTTPStatusError as e:
            raise TransportServerError(str(e), e.response.status_code) from e

        raise TransportProtocolError(
            f"Server did not return a GraphQL result: " f"{reason}: " f"{response.text}"
        )


class HTTPXTransport(Transport, _HTTPXTransport):
    """:ref:`Sync Transport <sync_transports>` used to execute GraphQL queries
    on remote servers.

    The transport uses the httpx library to send HTTP POST requests.
    """

    client: Optional[httpx.Client] = None

    def connect(self):
        if self.client:
            raise TransportAlreadyConnected("Transport is already connected")

        log.debug("Connecting transport")

        self.client = httpx.Client(**self.kwargs)

    def execute(  # type: ignore
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
        extra_args: Optional[Dict[str, Any]] = None,
        upload_files: bool = False,
    ) -> ExecutionResult:
        """Execute GraphQL query.

        Execute the provided document AST against the configured remote server. This
        uses the httpx library to perform a HTTP POST request to the remote server.

        :param document: GraphQL query as AST Node object.
        :param variable_values: Dictionary of input parameters (Default: None).
        :param operation_name: Name of the operation that shall be executed.
            Only required in multi-operation documents (Default: None).
        :param extra_args: additional arguments to send to the httpx post method
        :param upload_files: Set to True if you want to put files in the variable values
        :return: The result of execution.
            `data` is the result of executing the query, `errors` is null
            if no errors occurred, and is a non-empty array if an error occurred.
        """
        if not self.client:
            raise TransportClosed("Transport is not connected")

        post_args = self._prepare_request(
            document,
            variable_values,
            operation_name,
            extra_args,
            upload_files,
        )

        response = self.client.post(self.url, **post_args)

        return self._prepare_result(response)

    def close(self):
        """Closing the transport by closing the inner session"""
        if self.client:
            self.client.close()
            self.client = None


class HTTPXAsyncTransport(AsyncTransport, _HTTPXTransport):
    """:ref:`Async Transport <async_transports>` used to execute GraphQL queries
    on remote servers.

    The transport uses the httpx library with anyio.
    """

    client: Optional[httpx.AsyncClient] = None

    async def connect(self):
        if self.client:
            raise TransportAlreadyConnected("Transport is already connected")

        log.debug("Connecting transport")

        self.client = httpx.AsyncClient(**self.kwargs)

    async def execute(
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
        extra_args: Optional[Dict[str, Any]] = None,
        upload_files: bool = False,
    ) -> ExecutionResult:
        """Execute GraphQL query.

        Execute the provided document AST against the configured remote server. This
        uses the httpx library to perform a HTTP POST request asynchronously to the
        remote server.

        :param document: GraphQL query as AST Node object.
        :param variable_values: Dictionary of input parameters (Default: None).
        :param operation_name: Name of the operation that shall be executed.
            Only required in multi-operation documents (Default: None).
        :param extra_args: additional arguments to send to the httpx post method
        :param upload_files: Set to True if you want to put files in the variable values
        :return: The result of execution.
            `data` is the result of executing the query, `errors` is null
            if no errors occurred, and is a non-empty array if an error occurred.
        """
        if not self.client:
            raise TransportClosed("Transport is not connected")

        post_args = self._prepare_request(
            document,
            variable_values,
            operation_name,
            extra_args,
            upload_files,
        )

        response = await self.client.post(self.url, **post_args)

        return self._prepare_result(response)

    async def close(self):
        """Closing the transport by closing the inner session"""
        if self.client:
            await self.client.aclose()
            self.client = None

    def subscribe(
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
    ) -> AsyncGenerator[ExecutionResult, None]:
        """Subscribe is not supported on HTTP.

        :meta private:
        """
        raise NotImplementedError("The HTTP transport does not support subscriptions")


================================================
File: gql/transport/local_schema.py
================================================
import asyncio
from inspect import isawaitable
from typing import AsyncGenerator, Awaitable, cast

from graphql import DocumentNode, ExecutionResult, GraphQLSchema, execute, subscribe

from gql.transport import AsyncTransport


class LocalSchemaTransport(AsyncTransport):
    """A transport for executing GraphQL queries against a local schema."""

    def __init__(
        self,
        schema: GraphQLSchema,
    ):
        """Initialize the transport with the given local schema.

        :param schema: Local schema as GraphQLSchema object
        """
        self.schema = schema

    async def connect(self):
        """No connection needed on local transport"""
        pass

    async def close(self):
        """No close needed on local transport"""
        pass

    async def execute(
        self,
        document: DocumentNode,
        *args,
        **kwargs,
    ) -> ExecutionResult:
        """Execute the provided document AST for on a local GraphQL Schema."""

        result_or_awaitable = execute(self.schema, document, *args, **kwargs)

        execution_result: ExecutionResult

        if isawaitable(result_or_awaitable):
            result_or_awaitable = cast(Awaitable[ExecutionResult], result_or_awaitable)
            execution_result = await result_or_awaitable
        else:
            result_or_awaitable = cast(ExecutionResult, result_or_awaitable)
            execution_result = result_or_awaitable

        return execution_result

    @staticmethod
    async def _await_if_necessary(obj):
        """This method is necessary to work with
        graphql-core versions < and >= 3.3.0a3"""
        return await obj if asyncio.iscoroutine(obj) else obj

    async def subscribe(
        self,
        document: DocumentNode,
        *args,
        **kwargs,
    ) -> AsyncGenerator[ExecutionResult, None]:
        """Send a subscription and receive the results using an async generator

        The results are sent as an ExecutionResult object
        """

        subscribe_result = await self._await_if_necessary(
            subscribe(self.schema, document, *args, **kwargs)
        )

        if isinstance(subscribe_result, ExecutionResult):
            yield subscribe_result

        else:
            async for result in subscribe_result:
                yield result


================================================
File: gql/transport/phoenix_channel_websockets.py
================================================
import asyncio
import json
import logging
from typing import Any, Dict, Optional, Tuple

from graphql import DocumentNode, ExecutionResult, print_ast
from websockets.exceptions import ConnectionClosed

from .exceptions import (
    TransportProtocolError,
    TransportQueryError,
    TransportServerError,
)
from .websockets_base import WebsocketsTransportBase

log = logging.getLogger(__name__)


class Subscription:
    """Records listener_id and unsubscribe query_id for a subscription."""

    def __init__(self, query_id: int) -> None:
        self.listener_id: int = query_id
        self.unsubscribe_id: Optional[int] = None


class PhoenixChannelWebsocketsTransport(WebsocketsTransportBase):
    """The PhoenixChannelWebsocketsTransport is an async transport
    which allows you to execute queries and subscriptions against an `Absinthe`_
    backend using the `Phoenix`_ framework `channels`_.

    .. _Absinthe: http://absinthe-graphql.org
    .. _Phoenix: https://www.phoenixframework.org
    .. _channels: https://hexdocs.pm/phoenix/Phoenix.Channel.html#content
    """

    def __init__(
        self,
        channel_name: str = "__absinthe__:control",
        heartbeat_interval: float = 30,
        *args,
        **kwargs,
    ) -> None:
        """Initialize the transport with the given parameters.

        :param channel_name: Channel on the server this transport will join.
            The default for Absinthe servers is "__absinthe__:control"
        :param heartbeat_interval: Interval in second between each heartbeat messages
            sent by the client
        """
        self.channel_name: str = channel_name
        self.heartbeat_interval: float = heartbeat_interval
        self.heartbeat_task: Optional[asyncio.Future] = None
        self.subscriptions: Dict[str, Subscription] = {}
        super().__init__(*args, **kwargs)

    async def _initialize(self) -> None:
        """Join the specified channel and wait for the connection ACK.

        If the answer is not a connection_ack message, we will return an Exception.
        """

        query_id = self.next_query_id
        self.next_query_id += 1

        init_message = json.dumps(
            {
                "topic": self.channel_name,
                "event": "phx_join",
                "payload": {},
                "ref": query_id,
            }
        )

        await self._send(init_message)

        # Wait for the connection_ack message or raise a TimeoutError
        init_answer = await asyncio.wait_for(self._receive(), self.ack_timeout)

        answer_type, answer_id, execution_result = self._parse_answer(init_answer)

        if answer_type != "reply":
            raise TransportProtocolError(
                "Websocket server did not return a connection ack"
            )

        async def heartbeat_coro():
            while True:
                await asyncio.sleep(self.heartbeat_interval)
                try:
                    query_id = self.next_query_id
                    self.next_query_id += 1

                    await self._send(
                        json.dumps(
                            {
                                "topic": "phoenix",
                                "event": "heartbeat",
                                "payload": {},
                                "ref": query_id,
                            }
                        )
                    )
                except ConnectionClosed:  # pragma: no cover
                    return

        self.heartbeat_task = asyncio.ensure_future(heartbeat_coro())

    async def _send_stop_message(self, query_id: int) -> None:
        """Send an 'unsubscribe' message to the Phoenix Channel referencing
        the listener's query_id, saving the query_id of the message.

        The server should afterwards return a 'phx_reply' message with
        the same query_id and subscription_id of the 'unsubscribe' request.
        """
        subscription_id = self._find_existing_subscription(query_id)

        unsubscribe_query_id = self.next_query_id
        self.next_query_id += 1

        # Save the ref so it can be matched in the reply
        self.subscriptions[subscription_id].unsubscribe_id = unsubscribe_query_id
        unsubscribe_message = json.dumps(
            {
                "topic": self.channel_name,
                "event": "unsubscribe",
                "payload": {"subscriptionId": subscription_id},
                "ref": unsubscribe_query_id,
            }
        )

        await self._send(unsubscribe_message)

    async def _stop_listener(self, query_id: int) -> None:
        await self._send_stop_message(query_id)

    async def _send_connection_terminate_message(self) -> None:
        """Send a phx_leave message to disconnect from the provided channel."""

        query_id = self.next_query_id
        self.next_query_id += 1

        connection_terminate_message = json.dumps(
            {
                "topic": self.channel_name,
                "event": "phx_leave",
                "payload": {},
                "ref": query_id,
            }
        )

        await self._send(connection_terminate_message)

    async def _connection_terminate(self):
        await self._send_connection_terminate_message()

    async def _send_query(
        self,
        document: DocumentNode,
        variable_values: Optional[Dict[str, Any]] = None,
        operation_name: Optional[str] = None,
    ) -> int:
        """Send a query to the provided websocket connection.

        We use an incremented id to reference the query.

        Returns the used id for this query.
        """

        query_id = self.next_query_id
        self.next_query_id += 1

        query_str = json.dumps(
            {
                "topic": self.channel_name,
                "event": "doc",
                "payload": {
                    "query": print_ast(document),
                    "variables": variable_values or {},
                },
                "ref": query_id,
            }
        )

        await self._send(query_str)

        return query_id

    def _parse_answer(
        self, answer: str
    ) -> Tuple[str, Optional[int], Optional[ExecutionResult]]:
        """Parse the answer received from the server

        Returns a list consisting of:
            - the answer_type (between:
              'data', 'reply', 'complete', 'close')
            - the answer id (Integer) if received or None
            - an execution Result if the answer_type is 'data' or None
        """

        event: str = ""
        answer_id: Optional[int] = None
        answer_type: str = ""
        execution_result: Optional[ExecutionResult] = None
        subscription_id: Optional[str] = None

        def _get_value(d: Any, key: str, label: str) -> Any:
            if not isinstance(d, dict):
                raise ValueError(f"{label} is not a dict")

            return d.get(key)

        def _required_value(d: Any, key: str, label: str) -> Any:
            value = _get_value(d, key, label)
            if value is None:
                raise ValueError(f"null {key} in {label}")

            return value

        def _required_subscription_id(
            d: Any, label: str, must_exist: bool = False, must_not_exist=False
        ) -> str:
            subscription_id = str(_required_value(d, "subscriptionId", label))
            if must_exist and (subscription_id not in self.subscriptions):
                raise ValueError("unregistered subscriptionId")
            if must_not_exist and (subscription_id in self.subscriptions):
                raise ValueError("previously registered subscriptionId")

            return subscription_id

        def _validate_data_response(d: Any, label: str) -> dict:
            """Make sure query, mutation or subscription answer conforms.
            The GraphQL spec says only three keys are permitted.
            """
            if not isinstance(d, dict):
                raise ValueError(f"{label} is not a dict")

            keys = set(d.keys())
            invalid = keys - {"data", "errors", "extensions"}
            if len(invalid) > 0:
                raise ValueError(
                    f"{label} contains invalid items: " + ", ".join(invalid)
                )
            return d

        try:
            json_answer = json.loads(answer)

            event = str(_required_value(json_answer, "event", "answer"))

            if event == "subscription:data":
                payload = _required_value(json_answer, "payload", "answer")

                subscription_id = _required_subscription_id(
                    payload, "payload", must_exist=True
                )

                result = _validate_data_response(payload.get("result"), "result")

                answer_type = "data"

                subscription = self.subscriptions[subscription_id]
                answer_id = subscription.listener_id

                execution_result = ExecutionResult(
                    data=result.get("data"),
                    errors=result.get("errors"),
                    extensions=result.get("extensions"),
                )

            elif event == "phx_reply":

                # Will generate a ValueError if 'ref' is not there
                # or if it is not an integer
                answer_id = int(_required_value(json_answer, "ref", "answer"))

                payload = _required_value(json_answer, "payload", "answer")

                status = _get_value(payload, "status", "payload")

                if status == "ok":
                    answer_type = "reply"

                    if answer_id in self.listeners:
                        response = _required_value(payload, "response", "payload")

                        if isinstance(response, dict) and "subscriptionId" in response:

                            # Subscription answer
                            subscription_id = _required_subscription_id(
                                response, "response", must_not_exist=True
                            )

                            self.subscriptions[subscription_id] = Subscription(
                                answer_id
                            )

                        else:
                            # Query or mutation answer
                            # GraphQL spec says only three keys are permitted
                            response = _validate_data_response(response, "response")

                            answer_type = "data"

                            execution_result = ExecutionResult(
                                data=response.get("data"),
                                errors=response.get("errors"),
                                extensions=response.get("extensions"),
                            )
                    else:
                        (
                            registered_subscription_id,
                            listener_id,
                        ) = self._find_subscription(answer_id)
                        if registered_subscription_id is not None:
                            # Unsubscription answer
                            response = _required_value(payload, "response", "payload")
                            subscription_id = _required_subscription_id(
                                response, "response"
                            )

                            if subscription_id != registered_subscription_id:
                                raise ValueError("subscription id does not match")

                            answer_type = "complete"

                            answer_id = listener_id

                elif status == "error":
                    response = payload.get("response")

                    if isinstance(response, dict):
                        if "errors" in response:
                            raise TransportQueryError(
                                str(response.get("errors")), query_id=answer_id
                            )
                        elif "reason" in response:
                            raise TransportQueryError(
                                str(response.get("reason")), query_id=answer_id
                            )
                    raise TransportQueryError("reply error", query_id=answer_id)

                elif status == "timeout":
                    raise TransportQueryError("reply timeout", query_id=answer_id)

                # In case of missing or unrecognized status, just continue

            elif event == "phx_error":
                # Sent if the channel has crashed
                # answer_id will be the "join_ref" for the channel
                # answer_id = int(json_answer.get("ref"))
                raise TransportServerError("Server error")
            elif event == "phx_close":
                answer_type = "close"
            else:
                raise ValueError("unrecognized event")

        except ValueError as e:
            log.error(f"Error parsing answer '{answer}': {e!r}")
            raise TransportProtocolError(
                f"Server did not return a GraphQL result: {e!s}"
            ) from e

        return answer_type, answer_id, execution_result

    async def _handle_answer(
        self,
        answer_type: str,
        answer_id: Optional[int],
        execution_result: Optional[ExecutionResult],
    ) -> None:
        if answer_type == "close":
            await self.close()
        else:
            await super()._handle_answer(answer_type, answer_id, execution_result)

    def _remove_listener(self, query_id: int) -> None:
        """If the listener was a subscription, remove that information."""
        try:
            subscription_id = self._find_existing_subscription(query_id)
            del self.subscriptions[subscription_id]
        except Exception:
            pass
        super()._remove_listener(query_id)

    def _find_subscription(self, query_id: int) -> Tuple[Optional[str], int]:
        """Perform a reverse lookup to find the subscription id matching
        a listener's query_id.
        """
        for subscription_id, subscription in self.subscriptions.items():
            if query_id == subscription.listener_id:
                return subscription_id, query_id
            if query_id == subscription.unsubscribe_id:
                return subscription_id, subscription.listener_id
        return None, query_id

    def _find_existing_subscription(self, query_id: int) -> str:
        """Perform a reverse lookup to find the subscription id matching
        a listener's query_id.
        """
        subscription_id, _listener_id = self._find_subscription(query_id)

        if subscription_id is None:
            raise TransportProtocolError(
                f"No subscription registered for listener {query_id}"
            )
        return subscription_id

    async def _close_coro(self, e: Exception, clean_close: bool = True) -> None:
        if self.heartbeat_task is not None:
            self.heartbeat_task.cancel()

        await super()._close_coro(e, clean_close)


================================================
File: gql/transport/requests.py
================================================
import io
import json
import logging
from typing import Any, Callable, Collection, Dict, List, Optional, Tuple, Type, Union

import requests
from graphql import DocumentNode, ExecutionResult, print_ast
from requests.adapters import HTTPAdapter, Retry
from requests.auth import AuthBase
from requests.cookies import RequestsCookieJar
from requests_toolbelt.multipart.encoder import MultipartEncoder

from gql.transport import Transport

from ..graphql_request import GraphQLRequest
from ..utils import extract_files
from .exceptions import (
    TransportAlreadyConnected,
    TransportClosed,
    TransportProtocolError,
    TransportServerError,
)

log = logging.getLogger(__name__)


class RequestsHTTPTransport(Transport):
    """:ref:`Sync Transport <sync_transports>` used to execute GraphQL queries
    on remote servers.

    The transport uses the requests library to send HTTP POST requests.
    """

    file_classes: Tuple[Type[Any], ...] = (io.IOBase,)
    _default_retry_codes = (429, 500, 502, 503, 504)

    def __init__(
        self,
        url: str,
        headers: Optional[Dict[str, Any]] = None,
        cookies: Optional[Union[Dict[str, Any], RequestsCookieJar]] = None,
        auth: Optional[AuthBase] = None,
        use_json: bool = True,
        timeout: Optional[int] = None,
        verify: Union[bool, str] = True,
        retries: int = 0,
        method: str = "POST",
        retry_backoff_factor: float = 0.1,
        retry_status_forcelist: Collection[int] = _default_retry_codes,
        json_serialize: Callable = json.dumps,
        json_deserialize: Callable = json.loads,
        **kwargs: Any,
    ):
        """Initialize the transport with the given request parameters.

        :param url: The GraphQL server URL.
        :param headers: Dictionary of HTTP Headers to send with
            :meth:`requests.Session.request` (Default: None).
        :param cookies: Dict or CookieJar object to send with
            :meth:`requests.Session.request` (Default: None).
        :param auth: Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth
            (Default: None).
        :param use_json: Send request body as JSON instead of form-urlencoded
            (Default: True).
        :param timeout: Specifies a default timeout for requests (Default: None).
        :param verify: Either a boolean, in which case it controls whether we verify
            the server's TLS certificate, or a string, in which case it must be a path
            to a CA bundle to use. (Default: True).
        :param retries: Pre-setup of the requests' Session for performing retries
        :param method: HTTP method used for requests. (Default: POST).
        :param retry_backoff_factor: A backoff factor to apply between attempts after
            the second try. urllib3 will sleep for:
            {backoff factor} * (2 ** ({number of previous retries}))
        :param retry_status_forcelist: A set of integer HTTP status codes that we
            should force a retry on. A retry is initiated if the request method is
            in allowed_methods and the response status code is in status_forcelist.
            (Default: [429, 500, 502, 503, 504])
        :param json_serialize: Json serializer callable.
                By default json.dumps() function
        :param json_deserialize: Json deserializer callable.
                By default json.loads() function
        :param kwargs: Optional arguments that ``request`` takes.
            These can be seen at the `requests`_ source code or the official `docs`_

        .. _requests: https://github.com/psf/requests/blob/master/requests/api.py
        .. _docs: https://requests.readthedocs.io/en/master/
        """
        self.url = url
        self.headers = headers
        self.cookies = cookies
        self.auth = auth
        self.use_json = use_json
        self.default_timeout = timeout
        self.verify = verify
        self.retries = retries
        self.method = method
        self.retry_backoff_factor = retry_backoff_factor
        self.retry_status_forcelist = retry_status_forcelist
        self.json_serialize: Callable = json_serialize
        self.json_deserialize: Callable = json_deserialize
        self.kwargs = kwargs

        self.session = None

        self.response_headers = Non